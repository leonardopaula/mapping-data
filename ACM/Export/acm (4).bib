@inproceedings{10.1145/1978942.1979395,
author = {Choe, Eun Kyoung and Consolvo, Sunny and Watson, Nathaniel F. and Kientz, Julie A.},
title = {Opportunities for Computing Technologies to Support Healthy Sleep Behaviors},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979395},
doi = {10.1145/1978942.1979395},
abstract = {Getting the right amount of quality sleep is a key aspect of good health, along with a healthy diet and regular exercise. Human-computer interaction (HCI) researchers have recently designed systems to support diet and exercise, but sleep has been relatively under-studied in the HCI community. We conducted a literature review and formative study aimed at uncovering opportunities for computing to support the important area of promoting healthy sleep. We present results from interviews with sleep experts, as well as a survey (N = 230) and interviews with potential users (N = 16) to indicate what people would find practical and useful for sleep. Based on these results, we identify a number of design considerations, challenges, and opportunities for using computing to support healthy sleep behaviors, as well as a design framework for mapping the design space of technologies for sleep.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3053–3062},
numpages = {10},
keywords = {design, health, health informatics, persuasive technology, qualitative study, wellness, sleep},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979070,
author = {Balaam, Madeline and Fitzpatrick, Geraldine and Good, Judith and Harris, Eric},
title = {Enhancing Interactional Synchrony with an Ambient Display},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979070},
doi = {10.1145/1978942.1979070},
abstract = {Nonverbal communication is an essential part of face-to-face social interaction, conveying information about emotion and interpersonal relationships. The rigorous sensing capabilities of pervasive technologies and the subtle nature of ambient technologies make them ideal to support the production of nonverbal communication in social interactions. In this paper we present a study using an ambient technology that supports nonverbal communication, and specifically nonverbal behaviours associated with rapport. We show that an ambient display can influence a participant's nonverbal behaviour, and that participants are not aware of this change in their behaviour. We discuss these findings in terms of the design and ethical issues that it raises, and define an agenda for future work.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {867–876},
numpages = {10},
keywords = {interactional synchrony, face-to-face interaction, ambient display, biofeedback monitor for social interaction, rapport, social interaction},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979288,
author = {Medler, Ben and John, Michael and Lane, Jeff},
title = {Data Cracker: Developing a Visual Game Analytic Tool for Analyzing Online Gameplay},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979288},
doi = {10.1145/1978942.1979288},
abstract = {Game analytics is a domain that focuses on the systems and methods used to analyze game-related data. In this paper we present how a visual game analytic tool can be developed to analyze player gameplay behavior. Our tool, Data Cracker, was built for monitoring gameplay in Dead Space 2, the latest game in the Dead Space franchise. We use Data Cracker as a case study to inform a larger discussion of designing a visual game analytic tool while working with a game team. Our design approach focuses on increasing the data literacy of a game team. This means getting an entire team interested and involved with game analytics. We found that building our tool during the early game development cycle, creating multiple early visual prototypes and branding the tool to the Dead Space team caused more team members to become interested in our tool. Increasing interest in analytics is also a means, we argue, for changing the common occurrence within the game industry to disband teams after a game is released. Instead, we promote the creation of "live" teams which stay attached to a game long after it is release in order to continue the analysis process. Additionally, we discuss the barriers one might face when developing game analytic tools, such as prejudice against analytics or the technical issues involved when collecting large data sets. All of these examples are presented as insights we gained while coupling analytic tool design to game development.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2365–2374},
numpages = {10},
keywords = {information visualization, team communication, game analytics, visual analytics, game design, player behavior},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979003,
author = {Purpura, Stephen and Schwanda, Victoria and Williams, Kaiton and Stubler, William and Sengers, Phoebe},
title = {Fit4life: The Design of a Persuasive Technology Promoting Healthy Behavior and Ideal Weight},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979003},
doi = {10.1145/1978942.1979003},
abstract = {This is a critical design paper offering a possible scenario of use intended to provoke reflection about values and politics of design in persuasive computing. We describe the design of a system - Fit4Life - that encourages individuals to address the larger goal of reducing obesity in society by promoting individual healthy behaviors. Using the Persuasive Systems Design Model [26], this paper outlines the Fit4Life persuasion context, the technology, its use of persuasive messages, and an experimental design to test the system's efficacy. We also contribute a novel discussion of the ethical and sociocultural considerations involved in our design, an issue that has remained largely unaddressed in the existing persuasive technologies literature [29].},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {423–432},
numpages = {10},
keywords = {persuasive technology, social implications, critical design, weight loss},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979446,
author = {Gjerlufsen, Tony and Klokmose, Clemens Nylandsted and Eagan, James and Pillias, Cl\'{e}ment and Beaudouin-Lafon, Michel},
title = {Shared Substance: Developing Flexible Multi-Surface Applications},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979446},
doi = {10.1145/1978942.1979446},
abstract = {This paper presents a novel middleware for developing flexible interactive multi-surface applications. Using a scenario-based approach, we identify the requirements for this type of applications. We then introduce Substance, a data-oriented framework that decouples functionality from data, and Shared Substance, a middleware implemented in Substance that provides powerful sharing abstractions. We describe our implementation of two applications with Shared Substance and discuss the insights gained from these experiments. Our finding is that the combination of a data-oriented programming model with middleware support for sharing data and functionality provides a flexible, robust solution with low viscosity at both design-time and run-time.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3383–3392},
numpages = {10},
keywords = {middleware, data-oriented model, multi-surface interaction},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979245,
author = {Mazurek, Michelle L. and Klemperer, Peter F. and Shay, Richard and Takabi, Hassan and Bauer, Lujo and Cranor, Lorrie Faith},
title = {Exploring Reactive Access Control},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979245},
doi = {10.1145/1978942.1979245},
abstract = {As users store and share more digital content at home, access control becomes increasingly important. One promising approach for helping non-expert users create accurate access policies is reactive policy creation, in which users can update their policy dynamically in response to access requests that would not otherwise succeed. An earlier study suggested reactive policy creation might be a good fit for file access control at home. To test this, we conducted an experience-sampling study in which participants used a simulated reactive access-control system for a week. Our results bolster the case for reactive policy creation as one mode by which home users specify access-control policy. We found both quantitative and qualitative evidence of dynamic, situational policies that are hard to implement using traditional models but that reactive policy creation can facilitate. While we found some clear disadvantages to the reactive model, they do not seem insurmountable.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2085–2094},
numpages = {10},
keywords = {access control, home computing, human factors, privacy},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979249,
author = {Brush, A.J. Bernheim and Lee, Bongshin and Mahajan, Ratul and Agarwal, Sharad and Saroiu, Stefan and Dixon, Colin},
title = {Home Automation in the Wild: Challenges and Opportunities},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979249},
doi = {10.1145/1978942.1979249},
abstract = {Visions of smart homes have long caught the attention of researchers and considerable effort has been put toward enabling home automation. However, these technologies have not been widely adopted despite being available for over three decades. To gain insight into this state of affairs, we conducted semi-structured home visits to 14 households with home automation. The long term experience, both positive and negative, of the households we interviewed illustrates four barriers that need to be addressed before home automation becomes amenable to broader adoption. These barriers are high cost of ownership, inflexibility, poor manageability, and difficulty achieving security. Our findings also provide several directions for further research, which include eliminating the need for structural changes for installing home automation, providing users with simple security primitives that they can confidently configure, and enabling composition of home devices.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2115–2124},
numpages = {10},
keywords = {smart home, home automation, domestic technology},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979155,
author = {Romero, Mario and Vialard, Alice and Peponis, John and Stasko, John and Abowd, Gregory},
title = {Evaluating Video Visualizations of Human Behavior},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979155},
doi = {10.1145/1978942.1979155},
abstract = {Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1441–1450},
numpages = {10},
keywords = {video, information visualization, user studies, behavior},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978997,
author = {Solovey, Erin Treacy and Lalooses, Francine and Chauncey, Krysta and Weaver, Douglas and Parasi, Margarita and Scheutz, Matthias and Sassaroli, Angelo and Fantini, Sergio and Schermerhorn, Paul and Girouard, Audrey and Jacob, Robert J.K.},
title = {Sensing Cognitive Multitasking for a Brain-Based Adaptive User Interface},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978997},
doi = {10.1145/1978942.1978997},
abstract = {Multitasking has become an integral part of work environments, even though people are not well-equipped cognitively to handle numerous concurrent tasks effectively. Systems that support such multitasking may produce better performance and less frustration. However, without understanding the user's internal processes, it is difficult to determine optimal strategies for adapting interfaces, since all multitasking activity is not identical. We describe two experiments leading toward a system that detects cognitive multitasking processes and uses this information as input to an adaptive interface. Using functional near-infrared spectroscopy sensors, we differentiate four cognitive multitasking processes. These states cannot readily be distinguished using behavioral measures such as response time, accuracy, keystrokes or screen contents. We then present our human-robot system as a proof-of-concept that uses real-time cognitive state information as input and adapts in response. This prototype system serves as a platform to study interfaces that enable better task switching, interruption management, and multitasking.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {383–392},
numpages = {10},
keywords = {fnirs, near-infrared spectroscopy, interruption, brain computer interface, human-robot interaction, multitasking},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979046,
author = {Epp, Clayton and Lippold, Michael and Mandryk, Regan L.},
title = {Identifying Emotional States Using Keystroke Dynamics},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979046},
doi = {10.1145/1978942.1979046},
abstract = {The ability to recognize emotions is an important part of building intelligent computers. Emotionally-aware systems would have a rich context from which to make appropriate decisions about how to interact with the user or adapt their system response. There are two main problems with current system approaches for identifying emotions that limit their applicability: they can be invasive and can require costly equipment. Our solution is to determine user emotion by analyzing the rhythm of their typing patterns on a standard keyboard. We conducted a field study where we collected participants' keystrokes and their emotional states via self-reports. From this data, we extracted keystroke features, and created classifiers for 15 emotional states. Our top results include 2-level classifiers for confidence, hesitance, nervousness, relaxation, sadness, and tiredness with accuracies ranging from 77 to 88%. In addition, we show promise for anger and excitement, with accuracies of 84%.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {715–724},
numpages = {10},
keywords = {keystroke dynamics, affective computing, emotion sensing},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979108,
author = {Shklovski, Irina and Kotamraju, Nalini},
title = {Online Contribution Practices in Countries That Engage in Internet Blocking and Censorship},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979108},
doi = {10.1145/1978942.1979108},
abstract = {In this article we describe people's online contribution practices in contexts in which the government actively blocks access to or censors the Internet. We argue that people experience blocking as confusing, as a motivation for self-censorship online, as a cause of impoverishment of available content and as a real threat of personal persecution. Challenging ideas of blocking as a monolithic, abstract policy, we discuss five strategies with which Internet users navigate blocking: self-censorship, cultivating technical savvy, reliance on social ties to relay blocked content, use of already blocked sites for content production as a form of protection and practiced transparency. We also discuss strategies that forum owners and blogging platform providers employ to deal with and to avoid blocking. We conclude by advocating for more research that acknowledges the complexity of the contexts in which all Internet users contribute to the Internet and social media.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1109–1118},
numpages = {10},
keywords = {internet use, internet censorship, contribution, online communities, lurkers, internet non-use, ethnography, social media, blocking, motivation, government},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979438,
author = {Chen, Yunan and Ngo, Victor and Harrison, Sidney and Duong, Victoria},
title = {Unpacking Exam-Room Computing: Negotiating Computer-Use in Patient-Physician Interactions},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979438},
doi = {10.1145/1978942.1979438},
abstract = {The presence of computers - especially desktops - takes significant time and attention away from patients during medical visits. As a result, patients may feel disengaged and disregarded. In this study, we examined the impact of using "Computer-on-Wheels" (COWs) in exam-rooms. We found physicians constantly reorienting and resituating exam-room computers to different positions during the three stages of a medical visit: communication-intensive phase, lecturing phase and ordering phase. We refer to this behavior as micro-negotiation of computer-use. Analysis of its usage patterns, as well as physician and patient perceptions, show that micro-negotiations facilitate eye contact expression and encourage patient participation in medical visits. In addition, we identify two tensions and two unintended benefits resulting from micro-negotiations. These findings lead us to consider new modes of negotiation in the exam-room that could alleviate the tensions identified while enabling physicians to continue enjoying micro-negotiation benefits in their work practice.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3343–3352},
numpages = {10},
keywords = {eye contact, exam-room computing, patient-physician interactions, electronic medical record (emr), micro-negotiation},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979353,
author = {Sato, Daisuke and Zhu, Shaojian and Kobayashi, Masatomo and Takagi, Hironobu and Asakawa, Chieko},
title = {Sasayaki: Augmented Voice Web Browsing Experience},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979353},
doi = {10.1145/1978942.1979353},
abstract = {Auditory user interfaces have great Web-access potential for billions of people with visual impairments, with limited literacy, who are driving, or who are otherwise unable to use a visual interface. However a sequential speech-based representation can only convey a limited amount of information. In addition, typical auditory user interfaces lose the visual cues such as text styles and page structures, and lack effective feedback about the current focus. To address these limitations, we created Sasayaki (from whisper in Japanese), which augments the primary voice output with a secondary whisper of contextually relevant information, automatically or in response to user requests. It also offers new ways to jump to semantically meaningful locations. A prototype was implemented as a plug-in for an auditory Web browser. Our experimental results show that the Sasayaki can reduce the task completion times for finding elements in webpages and increase satisfaction and confidence.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2769–2778},
numpages = {10},
keywords = {web accessibility, voice augmented browsing, multiple voices, auditory interface, Sasayaki},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978944,
author = {Cramer, Meg and Hirano, Sen H. and Tentori, Monica and Yeganyan, Michael T. and Hayes, Gillian R.},
title = {Classroom-Based Assistive Technology: Collective Use of Interactive Visual Schedules by Students with Autism},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978944},
doi = {10.1145/1978942.1978944},
abstract = {vSked is an interactive and collaborative assistive technology for students with autism, combining visual schedules, choice boards, and a token-based reward system into an integrated classroom system. In this paper, we present the results of a study of three deployments of vSked over the course of a year in two autism classrooms. The results of our study demonstrate that vSked can promote student independence, reduce the quantity of educator-initiated prompts, encourage consistency and predictability, reduce the time required to transition from one activity to another. The findings from this study reveal practices surrounding the use of assistive technologies in classrooms and highlight important considerations for both the design and the evaluation of assistive technologies in the future, especially those destined for classroom use.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {autism, visual schedules, assistive technology},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979187,
author = {Zimmerman, John and Tomasic, Anthony and Garrod, Charles and Yoo, Daisy and Hiruncharoenvate, Chaya and Aziz, Rafae and Thiruvengadam, Nikhil Ravi and Huang, Yun and Steinfeld, Aaron},
title = {Field Trial of Tiramisu: Crowd-Sourcing Bus Arrival Times to Spur Co-Design},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979187},
doi = {10.1145/1978942.1979187},
abstract = {Crowd-sourcing social computing systems represent a new material for HCI designers. However, these systems are difficult to work with and to prototype, because they require a critical mass of participants to investigate social behavior. Service design is an emerging research area that focuses on how customers co-produce the services that they use, and thus it appears to be a great domain to apply this new material. To investigate this relationship, we developed Tiramisu, a transit information system where commuters share GPS traces and submit problem reports. Tiramisu processes incoming traces and generates real-time arrival time predictions for buses. We conducted a field trial with 28 participants. In this paper we report on the results and reflect on the use of field trials to evaluate crowd-sourcing prototypes and on how crowd sourcing can generate co-production between citizens and public services.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1677–1686},
numpages = {10},
keywords = {service design, field trial, crowd-sourcing, transit},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979368,
author = {Shroff, Geeta and Kam, Matthew},
title = {Towards a Design Model for Women's Empowerment in the Developing World},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979368},
doi = {10.1145/1978942.1979368},
abstract = {Pulitzer Prize-winning journalist Nicholas Kristof argues that in this century the paramount moral challenge will be the struggle for gender equality around the world. In this paper, we present a design model for empowering low-income women in the developing world, in ways that cut across individual application areas. Specifically, this model characterizes a possible trajectory for NGOs and women to engage with each other and among themselves potentially augmented by technology to help women escape from poverty. The fieldwork components in this study took place over 15 weeks in three phases, with a total of 47 NGO staff members and 35 socio-economically challenged women in rural and urban India. Interviews and co-design sessions with seven proof-of-concept prototypes showed that women appeared to belong to five distinct stages of growth in striving towards independence. We report the technology design lessons from our co-design sessions to illustrate how user readiness, relationship building at the community and family levels, and integration with state, national and international level programs, should be taken into account in the broader context of intervention design.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2867–2876},
numpages = {10},
keywords = {digital divide, HCI4D, gender, ICTD, development, feminist interaction, ICT4D, women empowerment, developing countries, design for inclusion},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978979,
author = {Bardzell, Jeffrey and Bardzell, Shaowen},
title = {Pleasure is Your Birthright: Digitally Enabled Designer Sex Toys as a Case of Third-Wave HCI},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978979},
doi = {10.1145/1978942.1978979},
abstract = {In the past decade, HCI has become increasingly preoccupied with the deeply subjective qualities of interaction: experience, embodiment, pleasure, intimacy, and so on, an agenda sometimes grouped under the heading of "third-wave HCI"."Analytically understanding and designing for such qualities has been an ongoing challenge to the field, in part because its established theories and methodologies are comparatively weak at understanding and being responsive to human subjectivity. In this paper, we present a case study of a group of designers who have, in the past few years, revolutionized their domain - sex toys - by combining embodied pleasure, intimate experience, health and wellness, emerging technologies, high-quality design processes, and social activism. We consider the implications this case could have for researchers innovating on especially third-wave HCI design theories, methodologies, and processes.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {257–266},
numpages = {10},
keywords = {human sexuality, activism, criticism, HCI},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979190,
author = {Woelfer, Jill Palzkill and Hendry, David G.},
title = {Homeless Young People and Living with Personal Digital Artifacts},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979190},
doi = {10.1145/1978942.1979190},
abstract = {This paper reports on an investigation of how homeless young people hold themselves in relation to personal digital artifacts. Twelve participants, aged 19-29, took part in semi-structured interviews. Participants answered questions about the acquisition and disposition of personal artifacts, digital and non-digital, including mobile phones, music players, and wallets. The analysis of the interview transcripts reveals that young people often part with their digital artifacts in order to meet immediate needs, including the need to create and reciprocate goodwill. This contingent holding of personal artifacts illuminates both the ordinary and extraordinary circumstances of homelessness. The paper concludes with a discussion of constraints and implications for the design of information systems for improving the welfare of homeless young people.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1697–1706},
numpages = {10},
keywords = {access, homeless young people, personal digital artifacts, infrastructure, attachment, iPods, mobile phones},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979204,
author = {Sharma, Nikhil},
title = {Role of Available and Provided Resources in Sensemaking},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979204},
doi = {10.1145/1978942.1979204},
abstract = {Making sense of a topic often involves appropriating information and organizing themes from various existing resources. We studied how sensemakers appropriated from available online resources as well as artifacts provided by another person directly. We found that both available and provided resources affect sensemaking activities. Sensemakers added more structure in their work when online resources were easily available, but added less structure and information when they were provided relevant sensemaking artifacts from another person. We also studied how early and mature artifacts provided by another person were appropriated differently and found that mature artifacts were rated better and used more but resulted in lesser structure and information being added by the recipient. These findings have implications for the support of sensemaking activities using resources available online as well as artifacts provided by others including co-workers and friends.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1807–1816},
numpages = {10},
keywords = {resources, sensemaking, structure, collaboration, handoffs, representations},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979445,
author = {Gerken, Jens and Jetter, Hans-Christian and Z\"{o}llner, Michael and Mader, Martin and Reiterer, Harald},
title = {The Concept Maps Method as a Tool to Evaluate the Usability of APIs},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979445},
doi = {10.1145/1978942.1979445},
abstract = {Application programming interfaces (APIs) are the interfaces to existing code structures, such as widgets, frameworks, or toolkits. Therefore, they very much do have an impact on the quality of the resulting system. So, ensuring that developers can make the most out of them is an important challenge. However standard usability evaluation methods as known from HCI have limitations in grasping the interaction between developer and API as most IDEs (essentially the GUI) capture only part of it. In this paper we present the Concept Map method to study the usability of an API over time. This allows us to elicit the mental model of a programmer when using an API and thereby identify usability issues and learning barriers and their development over time.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3373–3382},
numpages = {10},
keywords = {API usability, longitudinal, evaluation method, concept maps},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978983,
author = {Vihavainen, Sami and Mate, Sujeet and Sepp\"{a}l\"{a}, Lassi and Cricri, Francesco and Curcio, Igor D.D.},
title = {We Want More: Human-Computer Collaboration in Mobile Social Video Remixing of Music Concerts},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978983},
doi = {10.1145/1978942.1978983},
abstract = {Recording and publishing mobile video clips from music concerts is popular. There is a high potential to increase the concert's perceived value when producing video remixes from individual video clips and using them socially. A digital production of a video remix is an interactive process between human and computer. However, it is not clear what the collaboration implications between human and computer are.We present a case study where we compare the processes and products of manual and automatic mobile video remixing. We provide results from the first systematic real world study of the subject. We draw our observations from a user trial where fans recorded mobile video clips during a rock concert.The results reveal issues on heterogeneous interests of the stakeholders, unexpected uses of the raw material, the burden of editing, diverse quality requirements, motivations for remixing, the effect of understanding the logic of automation, and the collaborative use of manual and automatic remixing.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {287–296},
numpages = {10},
keywords = {human factors, mobile, video, music, social, automation},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979104,
author = {Kwak, Haewoon and Chun, Hyunwoo and Moon, Sue},
title = {Fragile Online Relationship: A First Look at Unfollow Dynamics in Twitter},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979104},
doi = {10.1145/1978942.1979104},
abstract = {We analyze the dynamics of the behavior known as 'unfollow' in Twitter. We collected daily snapshots of the online relationships of 1.2 million Korean-speaking users for 51 days as well as all of their tweets. We found that Twitter users frequently unfollow. We then discover the major factors, including the reciprocity of the relationships, the duration of a relationship, the followees' informativeness, and the overlap of the relationships, which affect the decision to unfollow. We conduct interview with 22 Korean respondents to supplement the quantitative results.They unfollowed those who left many tweets within a short time, created tweets about uninteresting topics, or tweeted about the mundane details of their lives. To the best of our knowledge, this work is the first systematic study of the unfollow behavior in Twitter.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1091–1100},
numpages = {10},
keywords = {twitter, computer-mediated communication, unfollow, online relationship},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1982143.1982149,
author = {Serva, Mark A. and Benamati, John and Blue, Jon and Baroudi, Jack},
title = {"In Times of Stress, Be Bold and Valiant": A Preliminary Exploration of the Psychosocial and Physiological Measures of Stress and Suggestions for Future MIS Research},
year = {2011},
isbn = {9781450306669},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982143.1982149},
doi = {10.1145/1982143.1982149},
abstract = {Stress is a fact of life. Previous MIS research has examined stress, but has focused on the psychosocial aspects of stress, which are usually measures using self-reported metrics. This preliminary study examines the possible contrast between psychosocial and physiological stress that is, the body's autonomic reaction to a potential threat. The study also examines the effectiveness of the Trier Social Stress Test (TSST), a protocol designed to induce stress in social situations. Using a sample of thirty-two students, the results indicate that the TSST protocol is an effectiveness mechanism for inducing both psychosocial and physiological stress. The results also indicate that psychosocial and physiological stress are indeed different metrics. The study concludes with recommendations for future MIS researchers to build on these findings.},
booktitle = {Proceedings of the 49th SIGMIS Annual Conference on Computer Personnel Research},
pages = {14–19},
numpages = {6},
keywords = {psychosocial stress, stress, biometrics, physiological stress},
location = {San Antonio, Texas, USA},
series = {SIGMIS-CPR '11}
}

@inproceedings{10.1145/1987875.1987911,
author = {Tsay, Jason and Wright, Hyrum K. and Perry, Dewayne E.},
title = {Experiences Mining Open Source Release Histories},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987911},
doi = {10.1145/1987875.1987911},
abstract = {Software releases form a critical part of the life cycle of a software project. Typically, each project produces releases in its own way, using various methods of versioning, archiving, announcing and publishing the release. Understanding the release history of a software project can shed light on the project history, as well as the release process used by that project, and how those processes change. However, many factors make automating the retrieval of release history information difficult, such as the many sources of data, a lack of relevant standards and a disparity of tools used to create releases.In spite of the large amount of raw data available, no attempt has been made to create a release history database of a large number of projects in the open source ecosystem. This paper presents our experiences, including the tools, techniques and pitfalls, in our early work to create a software release history database which will be of use to future researchers who want to study and model the release engineering process in greater depth.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {208–212},
numpages = {5},
keywords = {data mining, release engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@inproceedings{10.1145/1985441.1985461,
author = {Pagano, Dennis and Maalej, Walid},
title = {How Do Developers Blog? An Exploratory Study},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985461},
doi = {10.1145/1985441.1985461},
abstract = {We report on an exploratory study, which aims at understanding how software developers use social media compared to conventional development infrastructures. We analyzed the blogging and the committing behavior of 1,100 developers in four large open source communities. We observed that these communities intensively use blogs with one new entry about every 8 hours. A blog entry includes 14 times more words than a commit message. When analyzing the content of the blogs, we found that most popular topics represent high-level concepts such as functional requirements and domain concepts. Source code related topics are covered in less than 15% of the posts. Our results also show that developers are more likely to blog after corrective engineering and management activities than after forward engineering and re-engineering activities. Our findings call for a hypothesis-driven research to further understand the role of social media in software engineering and integrate it into development processes and tools.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {123–132},
numpages = {10},
keywords = {open source, blogs, data mining, social software},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1145/1985793.1985883,
author = {Schmerl, Bradley and Garlan, David and Dwivedi, Vishal and Bigrigg, Michael W. and Carley, Kathleen M.},
title = {SORASCS: A Case Study in Soa-Based Platform Design for Socio-Cultural Analysis},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985883},
doi = {10.1145/1985793.1985883},
abstract = {An increasingly important class of software-based systems is platforms that permit integration of third-party components, services, and tools. Service-Oriented Architecture (SOA) is one such platform that has been successful in providing integration and distribution in the business domain, and could be effective in other domains (e.g., scientific computing, healthcare, and complex decision making). In this paper, we discuss our application of SOA to provide an integration platform for socio-cultural analysis, a domain that, through models, tries to understand, analyze and predict relationships in large complex social systems. In developing this platform, called SORASCS, we had to overcome issues we believe are generally applicable to any application of SOA within a domain that involves technically na\"{\i}ve users and seeks to establish a sustainable software ecosystem based on a common integration platform. We discuss these issues, the lessons learned about the kinds of problems that occur, and pathways toward a solution.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {643–652},
numpages = {10},
keywords = {service oriented architectures, platform design, socio-cultural analysis},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/1987875.1987893,
author = {Paasivaara, Maria and Lassenius, Casper},
title = {How Does an Agile Coaching Team Work? A Case Study},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987893},
doi = {10.1145/1987875.1987893},
abstract = {This paper presents a case study on building a successful agile coaching team focusing on distributed software development projects in a global software company. We describe how the team of eight coaches was built, how the coaches work as a team, how the coaches work with their customer projects, what the main benefits of coaching have been for the customer projects, and the main challenges on building the coaching activities.The data was gathered by 13 semi-structured interviews of the coaching team members, as well as the interviews with personnel from four coached customer projects.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {101–109},
numpages = {9},
keywords = {global software development, agile software development, agile coaching},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@inproceedings{10.1145/1987993.1987995,
author = {Winbladh, Kristina and Ziv, Hadar and Richardson, Debra J.},
title = {Evolving Requirements in Patient-Centered Software},
year = {2011},
isbn = {9781450305853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987993.1987995},
doi = {10.1145/1987993.1987995},
abstract = {The implications of an aging U.S. population indicate that a large portion of the population will receive limited access to the healthcare they need, unless clinical preventive services are provided. Patient-centered healthcare, in which patients gain more access to and control over their own health, is becoming an important part of clinical preventive services and so is software. Healthcare entails highly complex processes that require substantial communication between different healthcare professionals. A major concern for patient-centered software is that it must adapt to changing needs to support long-term wellbeing, i.e., new knowledge must be considered continuously as part of the software lifecycle. This position paper contends that research efforts should be directed toward software engineering solutions that consider evolution as a part of the software lifecycle and use a variety of feedback channels to direct evolution, and presents a research agenda integrated with an approach that addresses evolving needs through a continuous data-driven requirements engineering (RE) technique.},
booktitle = {Proceedings of the 3rd Workshop on Software Engineering in Health Care},
pages = {1–4},
numpages = {4},
keywords = {requirements evolution, patient-centered software, data-driven requirements engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {SEHC '11}
}

@inproceedings{10.1145/1984701.1984705,
author = {King, Abayomi and Lyons, Kelly},
title = {Automatic Status Updates in Distributed Software Development},
year = {2011},
isbn = {9781450305952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1984701.1984705},
doi = {10.1145/1984701.1984705},
abstract = {This study investigates how automatic, real-time, user-centered awareness information can help distributed software development teams. We created an Eclipse plugin that automatically determines a user's activity in their Eclipse IDE and publishes the activity information as the status of their instant messenger client. The status is updated in real-time every time the user changes his or her activities in their IDE. We evaluated this tool by demonstrating it to eighty-one academics and industry workers in the field of computer science and interviewing them about the perceived benefits and usefulness of the tool. The results reveal various factors that can impact a participant's desire for increased awareness information. Despite these factors there was a general desire to improve awareness of users' activities via the tool. There was also some indication that the tool might help with interruption management.},
booktitle = {Proceedings of the 2nd International Workshop on Web 2.0 for Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {status update, interruption management, distributed software development, awareness},
location = {Waikiki, Honolulu, HI, USA},
series = {Web2SE '11}
}

@inproceedings{10.1145/1988688.1988725,
author = {Elia, Annibale and Vellutino, Daniela and Marano, Federica and Langella, Alberto Maria and Napoli, Antonella},
title = {Semantic Web and Language Resources for E-Government: Linguistically Motivated Data Mining},
year = {2011},
isbn = {9781450301480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988688.1988725},
doi = {10.1145/1988688.1988725},
abstract = {Language Technologies (LT) perform well when they rely on a previous Language Resources (LR) development. Hence, in this paper we illustrate how to build an efficient data mining system based on a coherent formalization of natural language and on a lingware (in Machine-Readable Form) built on the universal concepts of "lexical unit", "meaning unit" and "morphosyntactic context". From a linguistic and semantic point of view we get more coherent results through a linguistically motivated data mining than a statistical approach. We developed LR for Natural Language Processing (NLP) applications, composed by electronic dictionaries made of terminological multiword-expressions (Machine-Readable Form) and by local grammars (in the form of finite-state automata and transducers -- FSA/FST). Both parts of this lingware were built and applied according to Lexicon-Grammar (LG) formalization principles and methods. The mentioned language resources form the basis to develop an Information Retrieval System for e-Government. This device is a Semantic Web (SW) application, which will automatically recognize a given set of frequently-asked questions (from here on, FAQs) on European Community Information, previously formalized as syntactic patterns inside local grammars.},
booktitle = {Proceedings of the International Conference on Web Intelligence, Mining and Semantics},
articleno = {31},
numpages = {9},
keywords = {multiword-expressions, IR, finite state automata, data mining, local grammars, NLP, linguistic resources, semantic web, electronic dictionaries},
location = {Sogndal, Norway},
series = {WIMS '11}
}

@inproceedings{10.1145/1988688.1988718,
author = {Orgaz, Gema Bello and Cajias, Raul and Camacho, David},
title = {A Study on the Impact of Crowd-Based Voting Schemes in the 'Eurovision' European Contest},
year = {2011},
isbn = {9781450301480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988688.1988718},
doi = {10.1145/1988688.1988718},
abstract = {The Eurovision contest has been the reference on european song contests for the past 50 years. Countries in the European Union can shows the rest of the participants their current music tendencies. This phenomena has been studied in domains like physic and social sciences to find correlations between contests and current political and socio-economy trends in EU. The inclusion of web and social technologies some years ago, have caused a disruption in the traditional voting system whereby the audience is encouraged to participate by casting votes for their favorite song. As a result, this system yields new, relevant information that may be extrapolated to social and political tendencies in Europe with a higher degree accuracy than by data collected using the previous jury-based system. This paper provides an initial data analysis in crowd behavior to assess the impact of the televote system, in the Eurovision voting dynamic, by focusing on two distinct five years periods that can successfully contrast each voting scheme. Analyzing these periods separately, we can observe results from the televoting contests and then compare to the jury to see if there is a change in voting patterns. Finally, we study the underlying community structure of the voting network using the Cluster Percolation Method and Edge Betweenness to discover stable core communities spanning a number of years in the contest. The clusters obtained using these algorithms are then used to compare how these stable communities have evolving during the considered periods.},
booktitle = {Proceedings of the International Conference on Web Intelligence, Mining and Semantics},
articleno = {25},
numpages = {9},
keywords = {CPM, data mining, social mining, web mining, eurovision, voting partnership, network, edge betweenness, televoting, graph based algorithms},
location = {Sogndal, Norway},
series = {WIMS '11}
}

@inproceedings{10.1145/2141622.2141633,
author = {Metsis, Vangelis and Galatas, Georgios and Papangelis, Alexandros and Kosmopoulos, Dimitrios and Makedon, Fillia},
title = {Recognition of Sleep Patterns Using a Bed Pressure Mat},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141633},
doi = {10.1145/2141622.2141633},
abstract = {The monitoring of sleep patterns is of major importance for various reason such as, the detection and treatment of sleep disorders, the assessment of the effect of different medical conditions or medications on the sleep quality and the assessment of mortality risks associated with sleeping patterns in adults and children. Sleep monitoring by itself is a difficult problem due to both privacy and technical considerations. The proposed system uses a bed pressure mat to assess and report sleep patterns. To evaluate our system we used real data collected in Heracleia Lab's assistive living apartment. Our method is non-invasive, as it does not disrupt the user's usual sleeping behavior and it can be used both at the clinic and at home with minimal cost.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {9},
numpages = {4},
keywords = {motion recognition, sleep patterns, machine learning, sleep disorders},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@inproceedings{10.1145/2141622.2141674,
author = {Bieber, Gerald and Luthardt, Andr\'{e} and Peter, Christian and Urban, Bodo},
title = {The Hearing Trousers Pocket: Activity Recognition by Alternative Sensors},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141674},
doi = {10.1145/2141622.2141674},
abstract = {In daily life, mobile phones accompany the user permanently and are worn often in the front pocket of the trousers. The sensors included in today's mobile phones can hence be used for ubiquitous assistance. For instance, the acceleration sensor could be used for analysis of the person's bodily activity, or the microphone can be used to analyze the environmental noise levels. A possible sensor fusion provides additional and assured environmental and context information.This work presents new methods of activity recognition by acceleration and sound sensors by means of sensors included in commercially available smart phones during everyday life. We could identify that sounds provide valuable additional information on a user's situation that allow to better asses a person's current context.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {44},
numpages = {6},
keywords = {stress detection, context, user state detection, situation, mental state, physical activity, sound sensor, microphone, mobile phone, acceleration, assistive technologies, activity monitoring},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@inproceedings{10.1145/2141622.2141678,
author = {Xefteris, Stefanos and Baboshin, Andrey and Tserpes, Konstantinos and Androulidakis, Aggelos and Glickman, Yuri and Varvarigou, Theodora and Haritou, Maria and D'Andria, Francesco},
title = {Enabling Risk Assessment and Analysis by Event Detection in Dementia Patients Using a Reconfigurable Rule Set},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141678},
doi = {10.1145/2141622.2141678},
abstract = {Chronic mental illnesses pose a great burden on the lives of citizens worldwide. In modern health-care, decentralization and enabling the self management of patients at home are crucial factors in improving the every-day lives of patients and the people close to them. People in general tend to dislike obtrusive monitoring on their daily activities, so how can we implement a platform that can provide clinicians with adequate and concise information on their patients health status and at the same time be unobtrusive and easy to use. Moreover, how can we make such an unobtrusive system capable of providing the doctor with high-impact warnings on the patient's health status only when it is needed, thus relieving him of unnecessary workload? In this paper, the authors present a reconfigurable Event Detection mechanism used in the ALADDIN platform for Risk Assessment and Analysis.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {47},
numpages = {7},
keywords = {cognitive states, risk analysis, non-obtrusive monitoring, assistive infrastructures, dementia, assisted living, quality of life, e-health},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@inproceedings{10.5555/2248467.2248480,
author = {Pizziol, Sergio and Dehais, Fr\'{e}d\'{e}ric and Tessier, Catherine},
title = {Towards Human Operator "State" Assessment},
year = {2011},
isbn = {9781450315067},
publisher = {IRIT Press},
address = {Toulouse, FRA},
abstract = {This paper focuses on an approach to estimate the symbolic "state" and detect the attentional tunneling of a human operator in the frame of a human-robot mission. The symbolic "state" results from a fuzzy aggregation of the operator's gaze position and heart rate.},
booktitle = {Proceedings of the 1st International Conference on Application and Theory of Automation in Command and Control Systems},
pages = {99–106},
numpages = {8},
keywords = {heart rate, authority conflict, human operator state, situation awareness, attentional tunneling, eye tracker},
location = {Barcelona, Spain},
series = {ATACCS '11}
}

@inproceedings{10.1145/1987816.1987828,
author = {Essary, David and Amer, Ahmed},
title = {Sustainable Predictive Storage Management: On-Line Grouping for Energy and Latency Reduction},
year = {2011},
isbn = {9781450307734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987816.1987828},
doi = {10.1145/1987816.1987828},
abstract = {The divergence of processor and storage system speeds is one of the most intensely investigated problems in computing. Yet the performance disparity remains, and further, storage energy consumption is rapidly becoming a new critical problem. While smarter caching and predictive techniques do much to alleviate this disparity, the problem persists, and data storage remains a growing contributor to latency and energy consumption. We present an online, block-level, context-based predictive engine utilizing opportunistic replication. We test this predictive engine on real-world workloads, gathering all necessary predictive metadata on the fly without any warm-up period, and show reductions of total disk activity by up to 65%. This reduction in logical movement equates to a reduction in physical mechanical movement of the drive by 80%, a reduction of perceived latency by up to 63%, and a reduction in measured energy consumed by mechanical acticvity of 52--71% on live hardware.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems and Storage},
articleno = {9},
numpages = {11},
keywords = {energy, sustainable, prediction, predictive metadata, latency, power, predictive engine, data layout, storage systems, replication},
location = {Haifa, Israel},
series = {SYSTOR '11}
}

@inproceedings{10.1145/1988796.1988807,
author = {Van Hensbergen, Eric and Shinde, Pravin and Evans, Noah},
title = {Brasil: Basic Resource Aggregation System Infrastructure Layer},
year = {2011},
isbn = {9781450307611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988796.1988807},
doi = {10.1145/1988796.1988807},
abstract = {Brasil is a self-contained service which can be deployed across a cluster to provide a dataflow workload distribution and communication aggregation mechanism. Together with our dataflow shell, named PUSH, it is intended to be used for the management of non-traditional super computing applications as well as provide a mechanism to manage in-situ analysis and vizualization of more traditional high performance computing simulations. This paper describes our experiences implementing and deploying a prototype of Brasil on a BlueGene/P supercomputer.},
booktitle = {Proceedings of the 1st International Workshop on Runtime and Operating Systems for Supercomputers},
pages = {73–80},
numpages = {8},
location = {Tucson, Arizona},
series = {ROSS '11}
}

@inproceedings{10.1145/1993498.1993551,
author = {Budi, Aditya and Lo, David and Jiang, Lingxiao and Lucia},
title = {<i>Kb</i>-Anonymity: A Model for Anonymized Behaviour-Preserving Test and Debugging Data},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993551},
doi = {10.1145/1993498.1993551},
abstract = {It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.For the purpose of releasing private data for testing and debugging, this paper proposes the kb-anonymity model, which combines the k-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, kb-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {447–457},
numpages = {11},
keywords = {k-anonymity, third-party testing and debugging, behavior preservation, symbolic execution},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993551,
author = {Budi, Aditya and Lo, David and Jiang, Lingxiao and Lucia},
title = {<i>Kb</i>-Anonymity: A Model for Anonymized Behaviour-Preserving Test and Debugging Data},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993551},
doi = {10.1145/1993316.1993551},
abstract = {It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.For the purpose of releasing private data for testing and debugging, this paper proposes the kb-anonymity model, which combines the k-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, kb-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.},
journal = {SIGPLAN Not.},
month = jun,
pages = {447–457},
numpages = {11},
keywords = {k-anonymity, symbolic execution, third-party testing and debugging, behavior preservation}
}

@inproceedings{10.1145/1993498.1993509,
author = {Jung, Changhee and Rus, Silvius and Railing, Brian P. and Clark, Nathan and Pande, Santosh},
title = {Brainy: Effective Selection of Data Structures},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993509},
doi = {10.1145/1993498.1993509},
abstract = {Data structure selection is one of the most critical aspects of developing effective applications. By analyzing data structures' behavior and their interaction with the rest of the application on the underlying architecture, tools can make suggestions for alternative data structures better suited for the program input on which the application runs. Consequently, developers can optimize their data structure usage to make the application conscious of an underlying architecture and a particular program input.This paper presents the design and evaluation of Brainy, a new program analysis tool that automatically selects the best data structure for a given program and its input on a specific microarchitecture. The data structure's interface functions are instrumented to dynamically monitor how the data structure interacts with the application for a given input. The instrumentation records traces of various runtime characteristics including underlying architecture-specific events. These generated traces are analyzed and fed into an offline model, constructed using machine learning, to select the best data structure. That is, Brainy exploits runtime feedback of data structures to model the situation an application runs on, and selects the best data structure for a given application/input/architecture combination based on the constructed model. The empirical evaluation shows that this technique is highly accurate across several real-world applications with various program input sets on two different state-of-the-art microarchitectures. Consequently, Brainy achieved an average performance improvement of 27% and 33% on both microarchitectures, respectively.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {86–97},
numpages = {12},
keywords = {application generator, data structure selection, performance counters, training framework},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993509,
author = {Jung, Changhee and Rus, Silvius and Railing, Brian P. and Clark, Nathan and Pande, Santosh},
title = {Brainy: Effective Selection of Data Structures},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993509},
doi = {10.1145/1993316.1993509},
abstract = {Data structure selection is one of the most critical aspects of developing effective applications. By analyzing data structures' behavior and their interaction with the rest of the application on the underlying architecture, tools can make suggestions for alternative data structures better suited for the program input on which the application runs. Consequently, developers can optimize their data structure usage to make the application conscious of an underlying architecture and a particular program input.This paper presents the design and evaluation of Brainy, a new program analysis tool that automatically selects the best data structure for a given program and its input on a specific microarchitecture. The data structure's interface functions are instrumented to dynamically monitor how the data structure interacts with the application for a given input. The instrumentation records traces of various runtime characteristics including underlying architecture-specific events. These generated traces are analyzed and fed into an offline model, constructed using machine learning, to select the best data structure. That is, Brainy exploits runtime feedback of data structures to model the situation an application runs on, and selects the best data structure for a given application/input/architecture combination based on the constructed model. The empirical evaluation shows that this technique is highly accurate across several real-world applications with various program input sets on two different state-of-the-art microarchitectures. Consequently, Brainy achieved an average performance improvement of 27% and 33% on both microarchitectures, respectively.},
journal = {SIGPLAN Not.},
month = jun,
pages = {86–97},
numpages = {12},
keywords = {application generator, performance counters, training framework, data structure selection}
}

@inproceedings{10.1145/1993498.1993501,
author = {Pingali, Keshav and Nguyen, Donald and Kulkarni, Milind and Burtscher, Martin and Hassaan, M. Amber and Kaleem, Rashid and Lee, Tsung-Hsien and Lenharth, Andrew and Manevich, Roman and M\'{e}ndez-Lojo, Mario and Prountzos, Dimitrios and Sui, Xin},
title = {The Tao of Parallelism in Algorithms},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993501},
doi = {10.1145/1993498.1993501},
abstract = {For more than thirty years, the parallel programming community has used the dependence graph as the main abstraction for reasoning about and exploiting parallelism in "regular" algorithms that use dense arrays, such as finite-differences and FFTs. In this paper, we argue that the dependence graph is not a suitable abstraction for algorithms in new application areas like machine learning and network analysis in which the key data structures are "irregular" data structures like graphs, trees, and sets.To address the need for better abstractions, we introduce a data-centric formulation of algorithms called the operator formulation in which an algorithm is expressed in terms of its action on data structures. This formulation is the basis for a structural analysis of algorithms that we call tao-analysis. Tao-analysis can be viewed as an abstraction of algorithms that distills out algorithmic properties important for parallelization. It reveals that a generalized form of data-parallelism called amorphous data-parallelism is ubiquitous in algorithms, and that, depending on the tao-structure of the algorithm, this parallelism may be exploited by compile-time, inspector-executor or optimistic parallelization, thereby unifying these seemingly unrelated parallelization techniques. Regular algorithms emerge as a special case of irregular algorithms, and many application-specific optimization techniques can be generalized to a broader context.These results suggest that the operator formulation and tao-analysis of algorithms can be the foundation of a systematic approach to parallel programming.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {12–25},
numpages = {14},
keywords = {tao-analysis, operator formulation, amorphous data-parallelism, galois system, irregular programs},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993501,
author = {Pingali, Keshav and Nguyen, Donald and Kulkarni, Milind and Burtscher, Martin and Hassaan, M. Amber and Kaleem, Rashid and Lee, Tsung-Hsien and Lenharth, Andrew and Manevich, Roman and M\'{e}ndez-Lojo, Mario and Prountzos, Dimitrios and Sui, Xin},
title = {The Tao of Parallelism in Algorithms},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993501},
doi = {10.1145/1993316.1993501},
abstract = {For more than thirty years, the parallel programming community has used the dependence graph as the main abstraction for reasoning about and exploiting parallelism in "regular" algorithms that use dense arrays, such as finite-differences and FFTs. In this paper, we argue that the dependence graph is not a suitable abstraction for algorithms in new application areas like machine learning and network analysis in which the key data structures are "irregular" data structures like graphs, trees, and sets.To address the need for better abstractions, we introduce a data-centric formulation of algorithms called the operator formulation in which an algorithm is expressed in terms of its action on data structures. This formulation is the basis for a structural analysis of algorithms that we call tao-analysis. Tao-analysis can be viewed as an abstraction of algorithms that distills out algorithmic properties important for parallelization. It reveals that a generalized form of data-parallelism called amorphous data-parallelism is ubiquitous in algorithms, and that, depending on the tao-structure of the algorithm, this parallelism may be exploited by compile-time, inspector-executor or optimistic parallelization, thereby unifying these seemingly unrelated parallelization techniques. Regular algorithms emerge as a special case of irregular algorithms, and many application-specific optimization techniques can be generalized to a broader context.These results suggest that the operator formulation and tao-analysis of algorithms can be the foundation of a systematic approach to parallel programming.},
journal = {SIGPLAN Not.},
month = jun,
pages = {12–25},
numpages = {14},
keywords = {galois system, irregular programs, amorphous data-parallelism, tao-analysis, operator formulation}
}

@inproceedings{10.1145/2000417.2000418,
author = {Teodoro, George and Sussman, Alan},
title = {AARTS: Low Overhead Online Adaptive Auto-Tuning},
year = {2011},
isbn = {9781450307086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000417.2000418},
doi = {10.1145/2000417.2000418},
abstract = {We present an online lightweight auto-tuning system for shared-memory parallel programs. We employ an online adaptive tuning algorithm that is based on performance measurements, to adapt to performance variability that arises during program execution. We address the impact of synchronous vs. asynchronous interactions between the application and the tuning system, and describe an adaptive approach that benefits from the improvements provided by both options. We presented a performance study of the online tuning system, and compared it to synchronous tuning systems. Finally, AARTS is evaluated under different scenarios, showing the potential benefits of using online tuning and the ability of AARTS to exploit those benefits.},
booktitle = {Proceedings of the 1st International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era},
pages = {1–11},
numpages = {11},
keywords = {online tuning, auto-tuning},
location = {San Jose, California, USA},
series = {EXADAPT '11}
}

@inproceedings{10.1145/2024724.2024735,
author = {Wang, Yanzhi and Xie, Qing and Ammari, Ahmed and Pedram, Massoud},
title = {Deriving a Near-Optimal Power Management Policy Using Model-Free Reinforcement Learning and Bayesian Classification},
year = {2011},
isbn = {9781450306362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024724.2024735},
doi = {10.1145/2024724.2024735},
abstract = {To cope with the variations and uncertainties that emanate from hardware and application characteristics, dynamic power management (DPM) frameworks must be able to learn about the system inputs and environment and adjust the power management policy on the fly. In this paper we present an online adaptive DPM technique based on model-free reinforcement learning (RL), which is commonly used to control stochastic dynamical systems. In particular, we employ temporal difference learning for semi-Markov decision process (SMDP) for the model-free RL. In addition a novel workload predictor based on an online Bayes classifier is presented to provide effective estimates of the workload states for the RL algorithm. In this DPM framework, power and latency tradeoffs can be precisely controlled based on a user-defined parameter. Experiments show that amount of average power saving (without any increase in the latency) is up to 16.7% compared to a reference expert-based approach. Alternatively, the per-request latency reduction without any power consumption increase is up to 28.6% compared to the expert-based approach.},
booktitle = {Proceedings of the 48th Design Automation Conference},
pages = {41–46},
numpages = {6},
keywords = {dynamic power management, Bayes classification, reinforcement learning},
location = {San Diego, California},
series = {DAC '11}
}

@inproceedings{10.1145/2018358.2018383,
author = {Boer, Alexander and van Engers, Tom},
title = {An Agent-Based Legal Knowledge Acquisition Methodology for Agile Public Administration},
year = {2011},
isbn = {9781450307550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018358.2018383},
doi = {10.1145/2018358.2018383},
abstract = {This paper proposes a knowledge elicitation method based on serious gaming for theory construction about the effects of the law on the behaviours of agents. These games provide input to simulations of business process and product design alternatives. For knowledge representation, we have combined agent role descriptions with a generic task framework. An important thesis of this paper is that, in the interest of quick and simple domain analysis, agent roles, not intelligent agents, should be the focal object of simulation of complex social organizations. At least if getting a grip on social complexity is the purpose of modeling.},
booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Law},
pages = {171–180},
numpages = {10},
keywords = {legal knowledge engineering, public administration, knowledge acquisition},
location = {Pittsburgh, Pennsylvania},
series = {ICAIL '11}
}

@inproceedings{10.5555/1996039.1996043,
author = {Zhao, Yao and Cao, Yinzhi and Chen, Yan and Zhang, Ming and Goyal, Anup},
title = {Rake: Semantics Assisted Network-Based Tracing Framework},
year = {2011},
publisher = {IEEE Press},
abstract = {The ability to trace request execution paths is critical for diagnosing performance faults in large-scale distributed systems. Previous black-box and white-box approaches are either inaccurate or invasive. We present a novel semantics-assisted gray-box tracing approach, called Rake, which can accurately trace individual request by observing network traffic. Rake infers the causality between messages by identifying polymorphic IDs in messages according to application semantics. To make Rake universally applicable, we design a Rake language so that users can easily describe necessary semantics of their applications while reusing the core Rake component. We evaluate Rake using a few popular distributed applications, including web search, distributed computing cluster, content provider network, and online chatting. Our results demonstrate Rake is much more accurate than the black-box approaches while requiring no modification to OS/applications. In the CoralCDN (a content distributed network) experiments, Rake links messages with much higher accuracy than WAP5, a state-of-the-art blackbox approach. In the Hadoop (a distributed computing cluster platform) experiments, Rake helps reveal several previously unknown issues that may lead to performance degradation, including a RPC (Remote Procedure Call) abusing problem.},
booktitle = {Proceedings of the Nineteenth International Workshop on Quality of Service},
articleno = {3},
numpages = {9},
location = {San Jose, California},
series = {IWQoS '11}
}

@inproceedings{10.1145/1995966.1996000,
author = {Squicciarini, Anna Cinzia and Sundareswaran, Smitha and Lin, Dan and Wede, Josh},
title = {A3P: Adaptive Policy Prediction for Shared Images over Popular Content Sharing Sites},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996000},
doi = {10.1145/1995966.1996000},
abstract = {More and more people go online today and share their personal images using popular web services like Picasa. While enjoying the convenience brought by advanced technology, people also become aware of the privacy issues of data being shared. Recent studies have highlighted that people expect more tools to allow them to regain control over their privacy. In this work, we propose an Adaptive Privacy Policy Prediction (A3P) system to help users compose privacy settings for their images. In particular, we examine the role of image content and metadata as possible indicators of users' privacy preferences. We propose a two-level image classification framework to obtain image categories which may be associated with similar policies. Then, we develop a policy prediction algorithm to automatically generate a policy for each newly uploaded image. Most importantly, the generated policy will follow the trend of the user's privacy concerns evolved with time. We have conducted an extensive user study and the results demonstrate effectiveness of our system with the prediction accuracy around 90%.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {261–270},
numpages = {10},
keywords = {images, privacy},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1993636.1993733,
author = {Woodruff, David P.},
title = {Near-Optimal Private Approximation Protocols via a Black Box Transformation},
year = {2011},
isbn = {9781450306911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993636.1993733},
doi = {10.1145/1993636.1993733},
abstract = {We show the following transformation: any two-party protocol for outputting a (1+ε)-approximation to f(x,y) = ∑j=1n g(xj, yj) with probability at least 2/3, for any non-negative efficienty computable function g, can be transformed into a two-party private approximation protocol with only a polylogarithmic factor loss in communication, computation, and round complexity. In general it is insufficient to use secure function evaluation or fully homomorphic encryption on a standard, non-private protocol for approximating f. This is because the approximation may reveal information about x and y that does not follow from f(x,y). Applying our transformation and variations of it, we obtain near-optimal private approximation protocols for a wide range of problems in the data stream literature for which previously nothing was known. We give near-optimal private approximation protocols for the lp-distance for every p ≥ 0, for the heavy hitters and importance sampling problems with respect to any lp-norm, for the max-dominance and other dominant lp-norms, for the distinct summation problem, for entropy, for cascaded frequency moments, for subspace approximation and block sampling, and for measuring independence of datasets. Using a result for data streams, we obtain private approximation protocols with polylogarithmic communication for every non-decreasing and symmetric function g(xj,yj) = h(xj-yj) with at most quadratic growth. If the original (non-private) protocol is a simultaneous protocol, e.g., a sketching algorithm, then our only cryptographic assumption is efficient symmetric computationally-private information retrieval; otherwise it is fully homomorphic encryption. For all but one of these problems, the original protocol is a sketching algorithm. Our protocols generalize straightforwardly to more than two parties.},
booktitle = {Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing},
pages = {735–744},
numpages = {10},
keywords = {approximation algorithms, data stream algorithms, communication complexity, cryptography},
location = {San Jose, California, USA},
series = {STOC '11}
}

@inproceedings{10.1145/1996092.1996094,
author = {Ren, Kai and L\'{o}pez, Julio and Gibson, Garth},
title = {Otus: Resource Attribution in Data-Intensive Clusters},
year = {2011},
isbn = {9781450307000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996092.1996094},
doi = {10.1145/1996092.1996094},
abstract = {Frameworks for large scale data-intensive applications, such as Hadoop and Dryad, have gained tremendous popularity.Understanding the resource requirements of these frameworks and the performance characteristics of distributed applications is inherently difficult. We present an approach, based on resource attribution, that aims at facilitating performance analyses of distributed data-intensive applications.This approach is embodied in Otus, a monitoring tool to attribute resource usage to jobs and services in Hadoop clusters.Otus collects and correlates performance metrics from distributed components and provides views that display time-series of these metrics filtered and aggregated using multiple criteria.Our evaluation shows that this approach can be deployed without incurring major overheads.Our experience with Otus in a production cluster suggests its effectiveness at helping users and cluster administrators with application performance analysis and troubleshooting.},
booktitle = {Proceedings of the Second International Workshop on MapReduce and Its Applications},
pages = {1–8},
numpages = {8},
keywords = {metrics correlation, monitoring, resource attribution, data-intensive systems},
location = {San Jose, California, USA},
series = {MapReduce '11}
}

@inproceedings{10.1145/1996130.1996143,
author = {Zhou, Bowen and Kulkarni, Milind and Bagchi, Saurabh},
title = {Vrisha: Using Scaling Properties of Parallel Programs for Bug Detection and Localization},
year = {2011},
isbn = {9781450305525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996130.1996143},
doi = {10.1145/1996130.1996143},
abstract = {Detecting and isolating bugs that arise in parallel programs is a tedious and a challenging task. An especially subtle class of bugs are those that are scale-dependent: while small-scale test cases may not exhibit the bug, the bug arises in large-scale production runs, and can change the result or performance of an application. A popular approach to finding bugs is statistical bug detection, where abnormal behavior is detected through comparison with bug-free behavior. Unfortunately, for scale-dependent bugs, there may not be bug-free runs at large scales and therefore traditional statistical techniques are not viable. In this paper, we propose Vrisha, a statistical approach to detecting and localizing scale-dependent bugs. Vrisha detects bugs in large-scale programs by building models of behavior based on bug-free behavior at small scales. These models are constructed using kernel canonical correlation analysis (KCCA) and exploit scale-determined properties, whose values are predictably dependent on application scale. We use Vrisha to detect and diagnose two bugs caused by errors in popular MPI libraries and show that our techniques can be implemented with low overhead and low false-positive rates.},
booktitle = {Proceedings of the 20th International Symposium on High Performance Distributed Computing},
pages = {85–96},
numpages = {12},
keywords = {bug detection, large-scale bugs, KCCA},
location = {San Jose, California, USA},
series = {HPDC '11}
}

@inproceedings{10.1145/1996109.1996119,
author = {Ramakrishnan, Lavanya and Zbiegel, Piotr T. and Campbell, Scott and Bradshaw, Rick and Canon, Richard Shane and Coghlan, Susan and Sakrejda, Iwona and Desai, Narayan and Declerck, Tina and Liu, Anping},
title = {Magellan: Experiences from a Science Cloud},
year = {2011},
isbn = {9781450306997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996109.1996119},
doi = {10.1145/1996109.1996119},
abstract = {Cloud resources promise to be an avenue to address new categories of scientific applications including data-intensive science applications, on-demand/surge computing, and applications that require customized software environments. However, there is a limited understanding on how to operate and use clouds for scientific applications. Magellan, a project funded through the Department of Energy's (DOE) Advanced Scientific Computing Research (ASCR) program, is investigating the use of cloud computing for science at the Argonne Leadership Computing Facility (ALCF) and the National Energy Research Scientific Computing Facility (NERSC). In this paper, we detail the experiences to date at both sites and identify the gaps and open challenges from both a resource provider as well as application perspective.},
booktitle = {Proceedings of the 2nd International Workshop on Scientific Cloud Computing},
pages = {49–58},
numpages = {10},
keywords = {science, mapreduce, programming model, cloud computing, virtual machines},
location = {San Jose, California, USA},
series = {ScienceCloud '11}
}

@inproceedings{10.1145/1996014.1996019,
author = {Weissman, Jon B. and Sundarrajan, Pradeep and Gupta, Abhishek and Ryden, Matthew and Nair, Rohit and Chandra, Abhishek},
title = {Early Experience with the Distributed Nebula Cloud},
year = {2011},
isbn = {9781450307048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996014.1996019},
doi = {10.1145/1996014.1996019},
abstract = {Current cloud infrastructures are important for their ease of use and performance. However, they suffer from several shortcomings. The main problem is inefficient data mobility due to the centralization of cloud resources. We believe such clouds are highly unsuited for dispersed-data-intensive applications, where the data may be spread at multiple geographical locations (e.g., distributed user blogs). Instead, we propose a new cloud model called Nebula: a dispersed, context-aware, and cost-effective cloud. We provide experimental evidence for the need for Nebulas using a distributed blog analysis application followed by the system architecture and components of our system.},
booktitle = {Proceedings of the Fourth International Workshop on Data-Intensive Distributed Computing},
pages = {17–26},
numpages = {10},
keywords = {clouds, dsitributed computing},
location = {San Jose, California, USA},
series = {DIDC '11}
}

@inproceedings{10.1145/2037556.2037607,
author = {Kaschesky, Michael and Sobkowicz, Pawel and Bouchard, Guillaume},
title = {Opinion Mining in Social Media: Modeling, Simulating, and Visualizing Political Opinion Formation in the Web},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037556.2037607},
doi = {10.1145/2037556.2037607},
abstract = {Affordable and ubiquitous online communications (social media) provide the means for flows of ideas and opinions and play an increasing role for the transformation and cohesion of society - yet little is understood about how online opinions emerge, diffuse, and gain momentum. To address this problem, an opinion formation framework based on content analysis of social media and sociophysical system modeling is proposed. Based on prior research and own projects, three building blocks of online opinion tracking and simulation are described: (1) automated topic and opinion detection in real-time, (2) topic and opinion modeling and agent-based simulation, and (3) visualizations of topic and opinion networks. Finally, two application scenarios are presented to illustrate the framework and motivate further research.},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
pages = {317–326},
numpages = {10},
keywords = {economics, management, experimentation, measurement, design, human factors, reliability, standardization, languages},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/2037556.2037581,
author = {Linders, Dennis},
title = {We-Government: An Anatomy of Citizen Coproduction in the Information Age},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037556.2037581},
doi = {10.1145/2037556.2037581},
abstract = {This paper examines whether the tools of the Information Age---principally but not exclusively the Internet---make citizen coproduction of government services more viable and effective. The paper first discusses the re-emergence of citizen coproduction as a fashionable policy option in the face of persistent budget deficits, the rise of "government by network," and the advent of mass "peer-production." Finding a plethora of competing labels, models, and concepts for Internet-facilitated coproduction, the paper proposes a formal taxonomy to provide a more robust framework for systematic analysis. The paper then applies this framework to evaluate the impact of the tools of the Information Age on citizen coproduction. Its findings cautiously support the claim that the Information Age enables and advances new forms of citizen coproduction, namely large-scale "Do-It-Yourself Government" and "Government as a Platform." The paper concludes with a discussion of the potential implications for public administration, including the possible emergence of a new social contract that empowers the public to play a far more active role in the functioning of their government.},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
pages = {167–176},
numpages = {10},
keywords = {digital government, information age, coproduction, e-government, crowdsourcing, government as platform, service delivery, collaborative governance, public administration},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/1989323.1989441,
author = {Silberstein, Adam E. and Sears, Russell and Zhou, Wenchao and Cooper, Brian Frank},
title = {A Batch of PNUTS: Experiences Connecting Cloud Batch and Serving Systems},
year = {2011},
isbn = {9781450306614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989323.1989441},
doi = {10.1145/1989323.1989441},
abstract = {Cloud data management systems are growing in prominence, particularly at large Internet companies like Google, Yahoo!, and Amazon, which prize them for their scalability and elasticity. Each of these systems trades off between low-latency serving performance and batch processing throughput. In this paper, we discuss our experience running batch-oriented Hadoop on top of Yahoo's serving-oriented PNUTS system instead of the standard HDFS file system. Though PNUTS is optimized for and primarily used for serving, a number of applications at Yahoo! must run batch-oriented jobs that read or write data that is stored in PNUTS.Combining these systems reveals several key areas where the fundamental properties of each system are mismatched. We discuss our approaches to accommodating these mismatches, by either bending the batch and serving abstractions, or inventing new ones. Batch systems like Hadoop provide coarse task-level recovery, while serving systems like PNUTS provide finer record or transaction-level recovery. We combine both types to log record-level errors, while detecting and recovering from large-scale errors. Batch systems optimize for read and write throughput of large requests, while serving systems use indexing to provide low latency access to individual records. To improve latency-insensitive write throughput to PNUTS, we introduce a batch write path. The systems provide conflicting consistency models, and we discuss techniques to isolate them from one another.},
booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
pages = {1101–1112},
numpages = {12},
keywords = {serving, batch, bulk load, hybrid, PNUTS, hadoop},
location = {Athens, Greece},
series = {SIGMOD '11}
}

@inproceedings{10.1145/1998076.1998145,
author = {Lagoze, Carl and Patzke, Karin},
title = {A Research Agenda for Data Curation Cyberinfrastructure},
year = {2011},
isbn = {9781450307444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1998076.1998145},
doi = {10.1145/1998076.1998145},
abstract = {In 2008, the National Science Foundation released the DataNet solicitation, which presents an ambitious vision for a comprehensive data curation cyberinfrastructure in support of fourth paradigm science. The program subsequently funded two projects, DataONE and the Data Conservancy. The authors put forth an uncertainty framework for understanding the larger socio-cultural issues that influence the progress of DataNet projects and cyberinfrastructure projects in general. This framework highlights the key technical, organizational, scientific, and institutional contexts that the projects must consider as they mature.},
booktitle = {Proceedings of the 11th Annual International ACM/IEEE Joint Conference on Digital Libraries},
pages = {373–382},
numpages = {10},
keywords = {cyberinfrastructure, data curation},
location = {Ottawa, Ontario, Canada},
series = {JCDL '11}
}

@inproceedings{10.1145/1998196.1998203,
author = {Aanjaneya, Mridul and Chazal, Frederic and Chen, Daniel and Glisse, Marc and Guibas, Leonidas J. and Morozov, Dmitriy},
title = {Metric Graph Reconstruction from Noisy Data},
year = {2011},
isbn = {9781450306829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1998196.1998203},
doi = {10.1145/1998196.1998203},
abstract = {Many real-world data sets can be viewed of as noisy samples of special types of metric spaces called metric graphs [16]. Building on the notions of correspondence and Gromov-Hausdorff distance in metric geometry, we describe a model for such data sets as an approximation of an underlying metric graph. We present a novel algorithm that takes as an input such a data set, and outputs the underlying metric graph with guarantees. We also implement the algorithm, and evaluate its performance on a variety of real world data sets.},
booktitle = {Proceedings of the Twenty-Seventh Annual Symposium on Computational Geometry},
pages = {37–46},
numpages = {10},
keywords = {reconstruction, metric graph, noise, inference},
location = {Paris, France},
series = {SoCG '11}
}

@inproceedings{10.1145/2527031.2527046,
author = {White, Su and Croitoru, Madalina and Bazan, St\'{e}phane and Cerri, Stefano and Davis, Hugh C. and Folgieri, Raffaella and Jonquet, Clement and Scharffe, Fran\c{c}ois and Staab, Steffan and Tiropanis, Thanassis and Vafopoulos, Michalis},
title = {Negotiating the Web Science Curriculum through Shared Educational Artefacts},
year = {2011},
isbn = {9781450308557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2527031.2527046},
doi = {10.1145/2527031.2527046},
abstract = {The far-reaching impact of the Web on society is widely recognised. The interdisciplinary study of this impact has crystallised in the field of study known as Web Science. However, defining an agreed, shared understanding of what constitutes web science requires complex negotiation and translations of understandings across component disciplines, national cultures and educational traditions. Some individual institutions have already established particular curricula, and discussions in the Web Science Curriculum Workshop series have marked the territory to some extent. This paper reports on a process being adopted across a consortium of partners to systematically create a shared understanding of what constitutes web science. It records and critiques the processes instantiated to agree a common curriculum, and presents a framework for future discussion and development.},
booktitle = {Proceedings of the 3rd International Web Science Conference},
articleno = {11},
numpages = {8},
keywords = {web science education, educational repository, web science curriculum, co-creation, negotiated curriculum},
location = {Koblenz, Germany},
series = {WebSci '11}
}

@inproceedings{10.5555/2132890.2132905,
author = {Baumann, Timo and Schlangen, David},
title = {Predicting the Micro-Timing of User Input for an Incremental Spoken Dialogue System That Completes a User's Ongoing Turn},
year = {2011},
isbn = {9781937284107},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present the novel task of predicting temporal features of continuations of user input, while that input is still ongoing. We show that the remaining duration of an ongoing word, as well as the duration of the next can be predicted reasonably well, and we put this information to use in a system that synchronously completes a user's speech. While we focus on collaborative completions, the techniques presented here may also be useful for the alignment of back-channels and immediate turn-taking in an incremental SDS, or to synchronously monitor the user's speech fluency for other reasons.},
booktitle = {Proceedings of the SIGDIAL 2011 Conference},
pages = {120–129},
numpages = {10},
location = {Portland, Oregon},
series = {SIGDIAL '11}
}

@inproceedings{10.5555/2002736.2002754,
author = {Reddy, Sravana and Knight, Kevin},
title = {Unsupervised Discovery of Rhyme Schemes},
year = {2011},
isbn = {9781932432886},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation.},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2},
pages = {77–82},
numpages = {6},
location = {Portland, Oregon},
series = {HLT '11}
}

@inproceedings{10.1145/2000259.2000269,
author = {Hauck, Michael and Kuperberg, Michael and Huber, Nikolaus and Reussner, Ralf},
title = {Ginpex: Deriving Performance-Relevant Infrastructure Properties through Goal-Oriented Experiments},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000269},
doi = {10.1145/2000259.2000269},
abstract = {In software performance engineering, the infrastructure on which an application is running plays a crucial role when predicting the performance of the application. Thus, to yield accurate prediction results, performance-relevant properties and behaviour of the infrastructure have to be integrated into performance models. However, capturing these properties is a cumbersome and error-prone task, as it requires carefully engineered measurements and experiments. Existing approaches for creating infrastructure performance models require manual coding of these experiments, or ignore the detailed properties in the models. The contribution of this paper is the Ginpex approach, which introduces goal-oriented and model-based specification and generation of executable performance experiments for detecting and quantifying performance relevant infrastructure properties. Ginpex provides a metamodel for experiment specification and comes with pre-defined experiment templates that provide automated experiment execution on the target platform and also automate the evaluation of the experiment results. We evaluate Ginpex using two case studies, where experiments are executed to detect the operating system scheduler timeslice length, and to quantify the CPU virtualization overhead for an application executed in a virtualized environment.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {53–62},
numpages = {10},
keywords = {experiments, infrastructure, metamodel, performance prediction, measurements},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/2181101.2181127,
author = {Di Leva, Ciro and Guardascione, Isaia and Newton, Frederick},
title = {International Mission in Kosovo: Program, Project and Knowledge Management},
year = {2011},
isbn = {9781450307833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2181101.2181127},
doi = {10.1145/2181101.2181127},
abstract = {This document is based on the meaningful experience of the authors who are experienced in International Peacekeeping Missions. Its aim is to help improve the skills related to Project and Knowledge Management disciplines required during the reconstruction phase of a country, like Kosovo, which is trying to take on a new lease of life after many years of internal conflict.With this knowledge it is hoped to de-conflict organizational and practice overlaps incurred by the International donor community and other stakeholders in their short, middle and long term investment plans.},
booktitle = {Proceedings of the 12th International Conference on Product Focused Software Development and Process Improvement},
pages = {120–127},
numpages = {8},
keywords = {strategic plan, program and project management, mission, knowledge management, intelligence},
location = {Torre Canne, Brindisi, Italy},
series = {Profes '11}
}

@inproceedings{10.1145/1999030.1999038,
author = {Lamberty, K. K. and Adams, Stephen and Biatek, Jason and Froiland, Katherine and Lapham, Jay},
title = {Using a Large Display in the Periphery to Support Children Learning through Design},
year = {2011},
isbn = {9781450307512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999030.1999038},
doi = {10.1145/1999030.1999038},
abstract = {Learners benefit from creating personally meaningful artifacts for an audience, especially when those artifacts embody the concepts that the learners aim to understand. Our past work examined artistic design as an anchor for mathematical discussions between learners as they explored concepts like symmetry and fractions. In this paper, we present the work from two field studies where we explored ways to expand opportunities for mathematical discussions with a larger audience (beyond learners seated next to each other) by incorporating structured ways to share finished and in-progress work on a large display that all participants could view peripherally. We observed that the large display provided support for sharing designs with children anywhere in the room. The large display increased the students' awareness of their peers' designs, and the displayed designs became an anchor for their discussions. More work remains to increase the frequency of discussions about mathematical aspects of their artifacts.},
booktitle = {Proceedings of the 10th International Conference on Interaction Design and Children},
pages = {62–71},
numpages = {10},
keywords = {reflection, children, engagement, collaboration, ambient, awareness, peripheral display, large display, learning, math},
location = {Ann Arbor, Michigan},
series = {IDC '11}
}

@inproceedings{10.1145/2347504.2347533,
author = {Karahano\u{g}lu, Arma\u{g}an and Erbu\u{g}, \c{C}i\u{g}dem},
title = {Perceived Qualities of Smart Wearables: Determinants of User Acceptance},
year = {2011},
isbn = {9781450312806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2347504.2347533},
doi = {10.1145/2347504.2347533},
abstract = {Wearable computers are one of the new technologies that are expected to be a part of users' lives extensively in near future. While some of the users have positive attitudes towards these new products, some users may reject to use them due to different reasons. User experience is subjective, and effected by various parameters. Among these the first impression, namely the perceived qualities has an important impact on product acceptance. This paper aims to explore the perceived qualities of wearables and define the relations between them. An empirical study is conducted, to find out the hierarchy and meaningful relationships between the perceived qualities of smart wearables. The study is based on personal construct theory and data is presented by Cross-Impact Analysis. The patterns behind affection and affected qualities are explored to understand the design requirements for the best integration of wearables into daily lives.},
booktitle = {Proceedings of the 2011 Conference on Designing Pleasurable Products and Interfaces},
articleno = {26},
numpages = {8},
keywords = {human values, smart wearables, perceived qualities, user preferences, experience design},
location = {Milano, Italy},
series = {DPPI '11}
}

@inproceedings{10.1145/2347504.2347511,
author = {Zhang, Xiao and Wakkary, Ron},
title = {Design Analysis: Understanding e-Waste Recycling by Generation Y},
year = {2011},
isbn = {9781450312806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2347504.2347511},
doi = {10.1145/2347504.2347511},
abstract = {This paper aims to understand e-waste recycling behavior of Generation Y. It presents a pilot study that explores this generation's e-waste recycling practices, their attitudes towards e-waste recycling, and the barriers to e-waste recycling. The findings reveal the complexity of the actual e-waste recycling behavior, many participants in this study hold a positive attitude towards e-waste recycling, yet there is a shortage of convenient recycling options and e-waste recycling information. Based on the Motivation-Opportunity-Abilities model, this paper also uncovers the decision-making process involved in each recycling action. We use these findings to present a preliminary analysis of design implications to provoke design ideas and services that support e-waste recycling, and discuss our further research direction.},
booktitle = {Proceedings of the 2011 Conference on Designing Pleasurable Products and Interfaces},
articleno = {6},
numpages = {8},
keywords = {recycling behavior, e-waste, recycling, design, attitude, recycling action},
location = {Milano, Italy},
series = {DPPI '11}
}

@inproceedings{10.5555/2021153.2021157,
author = {Kolya, Anup Kumar and Das, Dipankar and Ekbal, Asif and Bandyopadhyay, Sivaji},
title = {Identifying Event: Sentiment Association Using Lexical Equivalence and Co-Reference Approaches},
year = {2011},
isbn = {9781932432985},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we have identified event and sentiment expressions at word level from the sentences of TempEval-2010 corpus and evaluated their association in terms of lexical equivalence and co-reference. A hybrid approach that consists of Conditional Random Field (CRF) based machine learning framework in conjunction with several rule based strategies has been adopted for event identification within the TimeML framework. The strategies are based on semantic role labeling, WordNet relations and some handcrafted rules. The sentiment expressions are identified simply based on the cues that are available in the sentiment lexicons such as Subjectivity Wordlist, SentiWordNet and WordNet Affect. The identification of lexical equivalence between event and sentiment expressions based on the part-of-speech (POS) categories is straightforward. The emotional verbs from VerbNet have also been employed to improve the coverage of lexical equivalence. On the other hand, the association of sentiment and event has been analyzed using the notion of co-reference. The parsed dependency relations along with basic rhetoric knowledge help to identify the co-reference between event and sentiment expressions. Manual evaluation on the 171 sentences of TempEval-2010 dataset yields the precision, recall and F-Score values of 61.25%, 70.29% and 65.23% respectively.},
booktitle = {Proceedings of the ACL 2011 Workshop on Relational Models of Semantics},
pages = {19–27},
numpages = {9},
location = {Portland, Oregon},
series = {RELMS '11}
}

@inproceedings{10.5555/2107653.2107678,
author = {G\^{\i}nsc\u{A}, Alexandru-Lucian and Boro\c{s}, Emanuela and Iftene, Adrian and Trandab\u{A}\c{t}, Diana and Toader, Mihai and Cor\^{\i}ci, Marius and Perez, Cenel-Augusto and Cristea, Dan},
title = {Sentimatrix: Multilingual Sentiment Analysis Service},
year = {2011},
isbn = {9781937284060},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes the preliminary results of a system for extracting sentiments opinioned with regard with named entities. It also combines rule-based classification, statistics and machine learning in a new method. The accuracy and speed of extraction and classification are crucial. The service oriented architecture permits the end-user to work with a flexible interface in order to produce applications that range from aggregating consumer feedback on commercial products to measuring public opinion on political issues from blog and forums. The experiment has two versions available for testing, one with concrete extraction results and sentiment calculus and the other with internal metrics validation results.},
booktitle = {Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis},
pages = {189–195},
numpages = {7},
location = {Portland, Oregon},
series = {WASSA '11}
}

@inproceedings{10.5555/2107653.2107673,
author = {Russo, Irene and Caselli, Tommaso and Rubino, Francesco and Boldrini, Ester and Mart\'{\i}nez-Barco, Patricio},
title = {EMOCause: An Easy-Adaptable Approach to Emotion Cause Contexts},
year = {2011},
isbn = {9781937284060},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper we present a method to automatically identify linguistic contexts which contain possible causes of emotions or emotional states from Italian newspaper articles (La Repubblica Corpus). Our methodology is based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge on emotional states and emotion eliciting situations. Our approach has been evaluated with respect to manually annotated data. The results obtained so far are satisfying and support the validity of the methodology proposed.},
booktitle = {Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis},
pages = {153–160},
numpages = {8},
location = {Portland, Oregon},
series = {WASSA '11}
}

@inproceedings{10.1145/1999747.1999817,
author = {Feaster, Yvon and Segars, Luke and Wahba, Sally K. and Hallstrom, Jason O.},
title = {Teaching CS Unplugged in the High School (with Limited Success)},
year = {2011},
isbn = {9781450306973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999747.1999817},
doi = {10.1145/1999747.1999817},
abstract = {CS Unplugged is a set of active learning activities designed to introduce fundamental computer science principles without the use of computers. The program has gained significant momentum in recent years, with proponents citing deep engagement and enjoyment benefits. With these benefits in mind, we initiated a one-year outreach program involving a local high school, using the CS Unplugged program as the foundation. To our disappointment, the results were at odds with our enthusiasm --- significantly. In this paper, we describe our approach to adapting the CS Unplugged materials for use at the high school level, present our experiences teaching it, and summarize the results of our evaluation.},
booktitle = {Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education},
pages = {248–252},
numpages = {5},
keywords = {computer science outreach, high school curriculum, cs unplugged, experimental evaluation},
location = {Darmstadt, Germany},
series = {ITiCSE '11}
}

@inproceedings{10.1145/1999747.1999769,
author = {Wolz, Ursula and Milazzo, Michael and Stone, Meredith},
title = {Kinesthetic Learning of Computing via "off-Beat" Activities},
year = {2011},
isbn = {9781450306973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999747.1999769},
doi = {10.1145/1999747.1999769},
abstract = {To broaden participation in computing requires exposing students to a variety of experiences. CS Unplugged promotes learning through kinesthetic activities. This paper identifies three types of learning (1) problem solving, (2) creative construction, and (3) open-ended invention, that lend themselves to activities that engage middle school students in physical movement to learn computing. Via surveys and a personality traits activity "True Colors" we examined whether self-identified personality types were predisposed to particular kinesthetic learning activities. Our results suggest that personality type as defined by True Colors does not predict selection of an activity type. Furthermore, the students in our summer Interactive Journalism Institute were significantly more predisposed to pick open-ended invention. These results suggest directions in which K-12 computing curriculum should take to reach the broadest constituency.},
booktitle = {Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education},
pages = {68–72},
numpages = {5},
keywords = {scratch, interactive journalism, computational thinking, kinesthetic learning, cs unplugged, broadening participation in cs},
location = {Darmstadt, Germany},
series = {ITiCSE '11}
}

@inproceedings{10.1145/1999916.1999923,
author = {Falaki, Hossein and Mahajan, Ratul and Estrin, Deborah},
title = {SystemSens: A Tool for Monitoring Usage in Smartphone Research Deployments},
year = {2011},
isbn = {9781450307406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999916.1999923},
doi = {10.1145/1999916.1999923},
abstract = {By deploying several research applications, we found that capturing usage context (e.g., CPU and memory) is highly valuable for debugging and interpreting results, even if that context information appears unrelated to the application. We have developed a general tool called SystemSens to help researchers capture usage context in their deployments in an extendible way. This paper describes and motivates the design choices underlying our tool and evaluates its overheads in terms of phone resources and data.},
booktitle = {Proceedings of the Sixth International Workshop on MobiArch},
pages = {25–30},
numpages = {6},
keywords = {android, deployment, smartphone},
location = {Bethesda, Maryland, USA},
series = {MobiArch '11}
}

@inproceedings{10.1145/1999995.2000000,
author = {Ra, Moo-Ryong and Sheth, Anmol and Mummert, Lily and Pillai, Padmanabhan and Wetherall, David and Govindan, Ramesh},
title = {Odessa: Enabling Interactive Perception Applications on Mobile Devices},
year = {2011},
isbn = {9781450306430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999995.2000000},
doi = {10.1145/1999995.2000000},
abstract = {Resource constrained mobile devices need to leverage computation on nearby servers to run responsive applications that recognize objects, people, or gestures from real-time video. The two key questions that impact performance are what computation to offload, and how to structure the parallelism across the mobile device and server. To answer these questions, we develop and evaluate three interactive perceptual applications. We find that offloading and parallelism choices should be dynamic, even for a given application, as performance depends on scene complexity as well as environmental factors such as the network and device capabilities. To this end we develop Odessa, a novel, lightweight, runtime that automatically and adaptively makes offloading and parallelism decisions for mobile interactive perception applications. Our evaluation shows that the incremental greedy strategy of Odessa converges to an operating point that is close to an ideal offline partitioning. It provides more than a 3x improvement in application performance over partitioning suggested by domain experts. Odessa works well across a variety of execution environments, and is agile to changes in the network, device and application inputs.},
booktitle = {Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services},
pages = {43–56},
numpages = {14},
keywords = {mobile perception application, video processing, parallel processing, incremental partitioning, offloading},
location = {Bethesda, Maryland, USA},
series = {MobiSys '11}
}

@inproceedings{10.1145/2159365.2159374,
author = {Canossa, Alessandro and Drachen, Anders and S\o{}rensen, Janus Rau M\o{}ller},
title = {Arrrgghh!!! Blending Quantitative and Qualitative Methods to Detect Player Frustration},
year = {2011},
isbn = {9781450308045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2159365.2159374},
doi = {10.1145/2159365.2159374},
abstract = {Frustration, in small, calibrated doses, can be integral to an enjoyable game experience, but it is a very delicate balance: just a slightly excessive amount of frustration could compel players to terminate prematurely the experience. Another factor with high relevance when analyzing player frustration is the difference in personality between players: some are less willing to endure frustration and might give up on the game earlier than others. This article seeks to identify patterns of behavior that could point to potential frustration before players resolve to quit a game. The method should be applicable independently from the personalities of different players. Furthermore, in order for this method to be relevant during game production, it has been decided to avoid relying on large numbers of players, and instead depend on highly granular data and both qualitative approaches (direct observation of players) and quantitative research (data mining gameplay metrics). The result is a computational model of player frustration that, although applied to a single game (Kane &amp; Lynch 2), is able to raise a red flag whenever a sequence of actions in the game could be interpreted as possible player frustration.},
booktitle = {Proceedings of the 6th International Conference on Foundations of Digital Games},
pages = {61–68},
numpages = {8},
keywords = {qualitative and quantitative research methods, user experience, gameplay metrics, game design, player frustration, game development, small sample size, patterns of behavior},
location = {Bordeaux, France},
series = {FDG '11}
}

@inproceedings{10.1145/2000119.2000126,
author = {Lino, Natasha and Siebra, Clauirton and Ara\'{u}jo, Jonatas and Anabuki, Davi and Patricio, Jos\'{e} and Batista, Marcelle and N\'{o}brega, Ramon and Amaro, Manoel and Lemos, Guido},
title = {Knowledge Tv},
year = {2011},
isbn = {9781450306027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000119.2000126},
doi = {10.1145/2000119.2000126},
abstract = {While iTV content is aimed at human manipulation, it does not appropriately support the performance of computational processes, such as search engines. The principal problem is that the meaning of multimedia documents cannot be explored, reducing the actuation limit of services and applications. In this context, this project proposes the definition of a semantic layer to the iTV platform, based on the Semantic Web concepts, which organizes its content and offers intelligent services, such as semantic queries and recommendation. Thus, this project tries to avoid the same problems that are currently presented by the Web platform, following the Semantic Web trend and increasing the potential to the development of more advanced applications. This technology is being developed in accordance with the Ginga project, which is the official specification for the Brazilian iTV middleware.},
booktitle = {Proceedings of the 9th European Conference on Interactive TV and Video},
pages = {29–38},
numpages = {10},
keywords = {semantic web, kdd, semantic services},
location = {Lisbon, Portugal},
series = {EuroITV '11}
}

@inproceedings{10.1145/2103354.2103356,
author = {Burke, Moira and Settles, Burr},
title = {Plugged in to the Community: Social Motivators in Online Goal-Setting Groups},
year = {2011},
isbn = {9781450308243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103354.2103356},
doi = {10.1145/2103354.2103356},
abstract = {At personal goal-setting websites, people join others in committing to a challenging goal, such as losing ten pounds or writing a novel in a month. Despite the popularity of these online communities, we know little about whether or how they improve goal performance. Based on theories of goal-setting and group attachment, we examine the influence of two social factors in an online "songwriting challenge" community: early feedback evoking a shared social identity, and one-on-one collaborations with other members. Combining five years of longitudinal behavioral data with member surveys, we find that users who engage in these social features perform better on their goals than those who are non-social. Furthermore, these early social experiences are associated with strong community-centric behaviors in the long term, including donating money and providing feedback to others.},
booktitle = {Proceedings of the 5th International Conference on Communities and Technologies},
pages = {1–10},
numpages = {10},
keywords = {collaboration, goal-setting, social facilitation, feedback},
location = {Brisbane, Australia},
series = {C&amp;T '11}
}

@article{10.1145/1970378.1970379,
author = {Lindtner, Silvia and Chen, Judy and Hayes, Gillian R. and Dourish, Paul},
title = {Towards a Framework of Publics: Re-Encountering Media Sharing and Its User},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/1970378.1970379},
doi = {10.1145/1970378.1970379},
abstract = {Design and evaluation of user-generated media production and sharing in Human-Computer Interaction (HCI) often focus on formal and informal media sharing, such as communication within social networks, automatic notifications of activities, and the exchange of digital artifacts. However, conceptual tools for understanding how people relate to the audiences they reach through these systems are limited. The increasing interest in user-generated content in HCI demands the infusion of new methods and theories that explicitly engage the construction and use of media within and among large groups of individuals and systems. In this paper, we suggest that the notion of “publics,” drawn from media theory, provides useful insights into user-driven, social, and cultural forms of technology use and digital content creation. We illustrate this by employing the notion of publics to the findings from a two-month deployment of a mobile photo sharing platform in a youth housing community. The results of this empirical work coupled with a theoretical examination of publics stimulate reflection on prevailing interpretations of user-designer-reader roles. The paper provides an outlook for potentially new and productive ways of understanding interdependencies within those activities. Implications that can be drawn from this work concern the role of digital media creation and sharing for the formation of collectives and how people position themselves collectively in relation to larger social groups and societal norms. The analysis suggests fruitful crossovers among HCI, Media Theory and New Media Research by approaching the user as both consumer and producer of digital content.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jul,
articleno = {5},
numpages = {23},
keywords = {photo sharing, media theory, Public, social networking, media sharing}
}

@inproceedings{10.1145/2016551.2016552,
author = {Ferrari, Laura and Mamei, Marco and Zambonelli, Franco},
title = {"All-about" Diaries: Concepts and Experiences},
year = {2011},
isbn = {9781450305600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016551.2016552},
doi = {10.1145/2016551.2016552},
abstract = {Smart phones and pervasive computing technologies enable the vision of all-about diaries: tools for recording, in a browsable and machine-processable format, the everyday activities and events of people, communities, objects and places. Diaries offer a wealth of opportunities for consumers and industries. Yet, while proposals exist indicating promising approaches to implement parts of them, several challenges still have to be faced to produce fully-edged working systems. In this paper we discuss opportunities and technologies that enable such diaries to be created. Then, we present a prototype of a diary based on location data.},
booktitle = {Proceedings of the 5th International Conference on Communication System Software and Middleware},
articleno = {1},
numpages = {11},
keywords = {pattern analysis location-based services, pervasive computing, context awareness, life logging},
location = {Verona, Italy},
series = {COMSWARE '11}
}

@inproceedings{10.1145/2002259.2002299,
author = {Stojanovic, Nenad and Milenovic, Dejan and Xu, Yongchun and Stojanovic, Ljiljana and Anicic, Darko and Studer, Rudi},
title = {An Intelligent Event-Driven Approach for Efficient Energy Consumption in Commercial Buildings: Smart Office Use Case},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002299},
doi = {10.1145/2002259.2002299},
abstract = {In this paper we present a use case related to the intelligent processing of events coming from the conventional ("cheap") sensors in order to support better energy consumption in commercial buildings. The approach has been implemented using our iCEP framework and deployed in the office space of a real working environment. This research is a kind of the proof of the concept for a new technology that the industry partner would like to exploit.The results are very encouraging: smart decisions for the efficient usage of energy can be made by the intelligent processing of "cheap" sensor events.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {303–312},
numpages = {10},
keywords = {intelligent complex event processing, smart office, energy efficiency},
location = {New York, New York, USA},
series = {DEBS '11}
}

@inproceedings{10.1145/2002259.2002279,
author = {Engel, Yagil and Etzion, Opher},
title = {Towards Proactive Event-Driven Computing},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002279},
doi = {10.1145/2002259.2002279},
abstract = {Event driven architecture is a paradigm shift from traditional computing architectures which employ synchronous, request-response interactions. In this paper we introduce a conceptual architecture for what can be considered the next phase of that evolution: proactive event-driven computing. Proactivity refers to the ability to mitigate or eliminate undesired future events, or to identify and take advantage of future opportunities, by applying prediction and automated decision making technologies. We investigate an extension of the event processing conceptual model and architecture to support proactive event-driven applications, and propose the main building blocks of a novel architecture. We first describe several extensions to the existing event processing functionality that is required to support proactivity; next, we extend the event processing agent model to include two more type of agents: predictive agents that may derive future uncertain events based on prediction models, and proactive agents that compute the best proactive action that should be taken. Those building blocks are demonstrated through a comprehensive scenario that deals with proactive decision making, ensuring timely delivery of critical material for a production plant.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {125–136},
numpages = {12},
keywords = {event processing, proactive computing},
location = {New York, New York, USA},
series = {DEBS '11}
}

@inproceedings{10.1145/2002259.2002293,
author = {Dindar, Nihal and Fischer, Peter M. and Soner, Merve and Tatbul, Nesime},
title = {Efficiently Correlating Complex Events over Live and Archived Data Streams},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002293},
doi = {10.1145/2002259.2002293},
abstract = {Correlating complex events over live and archived data streams, which we call Pattern Correlation Queries (PCQs), provides many benefits for domains which need real time forecasting of events or identification of causal dependencies, while handling data at high rates and in massive amounts, like in financial or medical settings. Existing work has focused either on complex event processing over a single type of stream source (i.e., either live or archived), or on simple stream correlation queries (e.g., live events trigerring a database lookup). In this paper, we specifically focus on recency-based PCQs and provide clear, useful, and optimizable semantics for them. PCQs raise a number of challenges in optimizing data management and query processing, which we address in the setting of the DejaVu complex event processing system. More specifically, we propose three complementary optimizations including recent input buffering, query result caching, and join source ordering. Furthermore, we capture the relevant query processing tradeoffs in a cost model. An extensive performance study on synthetic and real-life data sets not only validates this cost model, but also shows that our optimizations are very effective, achieving more than two orders magnitude throughput improvement and much better scalability compared to a conventional approach.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {243–254},
numpages = {12},
keywords = {complex event processing, stream archiving, pattern matching, stream correlation, data streams},
location = {New York, New York, USA},
series = {DEBS '11}
}

@inproceedings{10.1145/2001576.2001609,
author = {Jones, Ben and Soltoggio, Andrea and Sendhoff, Bernhard and Yao, Xin},
title = {Evolution of Neural Symmetry and Its Coupled Alignment to Body Plan Morphology},
year = {2011},
isbn = {9781450305570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001576.2001609},
doi = {10.1145/2001576.2001609},
abstract = {Body morphology is thought to have heavily influenced the evolution of neural architecture. However, the extent of this interaction and its underlying principles are largely unclear. To help us elucidate these principles, we examine the artificial evolution of a hypothetical nervous system embedded in a fish-inspired animat. The aim is to observe the evolution of neural structures in relation to both body morphology and required motor primitives. Our investigations reveal that increasing the pressure to evolve a wider range of movements also results in higher levels of neural symmetry. We further examine how different body shapes affect the evolution of neural structure; we find that, in order to achieve optimal movements, the neural structure integrates and compensates for asymmetrical body morphology. Our study clearly indicates that different parts of the animat - specifically, nervous system and body plan - evolve in concert with and become highly functional with respect to the other parts. The autonomous emergence of morphological and neural computation in this model contributes to unveiling the surprisingly strong coupling of such systems in nature.},
booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
pages = {235–242},
numpages = {8},
keywords = {neuroevolution, motor primitives, soft robotics, artificial life, morphology, body symmetry},
location = {Dublin, Ireland},
series = {GECCO '11}
}

@inproceedings{10.1145/2396716.2396731,
author = {Pechau, J\"{o}rg},
title = {Rafting the Agile Waterfall: Value Based Conflicts of Agile Software Development},
year = {2011},
isbn = {9781450313025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396716.2396731},
doi = {10.1145/2396716.2396731},
abstract = {Agile software development projects executed in larger project environments often struggle to succeed, despite overall framework conditions being considered good. This often can be related to agile cultures clashing with nonagile cultures, thus leading agile and non-agile value systems into conflict. This paper is a result of my research regarding patterns of value based conflicts of agile software development projects. The patterns shall help to identify aforementioned conflicts and provide a short-term approach to aim for short-term improvements and a long-term approach, to address the value conflicts themselfes when using Agile approaches.},
booktitle = {Proceedings of the 16th European Conference on Pattern Languages of Programs},
articleno = {15},
numpages = {15},
keywords = {software development, agile, software project management, value systems, patterns},
location = {Irsee, Germany},
series = {EuroPLoP '11}
}

@article{10.1145/1978782.1978788,
author = {Bonifaci, Vincenzo and Korteweg, Peter and Marchetti-Spaccamela, Alberto and Stougie, Leen},
title = {Minimizing Flow Time in the Wireless Gathering Problem},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1549-6325},
url = {https://doi.org/10.1145/1978782.1978788},
doi = {10.1145/1978782.1978788},
abstract = {We address the problem of efficient data gathering in a wireless network through multihop communication. We focus on two objectives related to flow times, that is, the times spent by data packets in the system: minimization of the maximum flow time and minimization of the average flow time of the packets. For both problems we prove that, unless P=NP, no polynomial-time algorithm can approximate the optimal solution within a factor less than Ω(m1−ε) for any 0&lt;ε&lt;1, where m is the number of packets. We then assess the performance of two natural algorithms by proving that their cost remains within the optimal cost of the respective problem if we allow the algorithms to transmit data at a speed 5 times higher than that of the optimal solutions to which we compare them.},
journal = {ACM Trans. Algorithms},
month = jul,
articleno = {33},
numpages = {20},
keywords = {Wireless networks, approximation algorithms, data gathering, local algorithms}
}

@article{10.1145/2007183.2007189,
author = {Azmandian, Fatemeh and Moffie, Micha and Alshawabkeh, Malak and Dy, Jennifer and Aslam, Javed and Kaeli, David},
title = {Virtual Machine Monitor-Based Lightweight Intrusion Detection},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5980},
url = {https://doi.org/10.1145/2007183.2007189},
doi = {10.1145/2007183.2007189},
abstract = {As virtualization technology gains in popularity, so do attempts to compromise the security and integrity of virtualized computing resources. Anti-virus software and firewall programs are typically deployed in the guest virtual machine to detect malicious software. These security measures are effective in detecting known malware, but do little to protect against new variants of intrusions. Intrusion detection systems (IDSs) can be used to detect malicious behavior. Most intrusion detection systems for virtual execution environments track behavior at the application or operating system level, using virtualization as a means to isolate themselves from a compromised virtual machine.In this paper, we present a novel approach to intrusion detection of virtual server environments which utilizes only information available from the perspective of the virtual machine monitor (VMM). Such an IDS can harness the ability of the VMM to isolate and manage several virtual machines (VMs), making it possible to provide monitoring of intrusions at a common level across VMs. It also offers unique advantages over recent advances in intrusion detection for virtual machine environments. By working purely at the VMM-level, the IDS does not depend on structures or abstractions visible to the OS (e.g., file systems), which are susceptible to attacks and can be modified by malware to contain corrupted information (e.g., the Windows registry). In addition, being situated within the VMM provides ease of deployment as the IDS is not tied to a specific OS and can be deployed transparently below different operating systems.Due to the semantic gap between the information available to the VMM and the actual application behavior, we employ the power of data mining techniques to extract useful nuggets of knowledge from the raw, low-level architectural data. We show in this paper that by working entirely at the VMM-level, we are able to capture enough information to characterize normal executions and identify the presence of abnormal malicious behavior. Our experiments on over 300 real-world malware and exploits illustrate that there is sufficient information embedded within the VMM-level data to allow accurate detection of malicious attacks, with an acceptable false alarm rate.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jul,
pages = {38–53},
numpages = {16},
keywords = {data mining, virtual machine, intrusion detection, system, virtual machine monitor, virtualization}
}

@inproceedings{10.1145/2016741.2016778,
author = {Demeler, Borries and Singh, Raminderjeet and Pierce, Marlon and Brookes, Emre H. and Marru, Suresh and Dubbs, Bruce},
title = {UltraScan Gateway Enhancements: In Collaboration with TeraGrid Advanced User Support},
year = {2011},
isbn = {9781450308885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016741.2016778},
doi = {10.1145/2016741.2016778},
abstract = {The Ultrascan gateway provides a user friendly web interface for evaluation of experimental analytical ultracentrifuge data using the UltraScan modeling software. The analysis tasks are executed on the TeraGrid and campus computational resources. The gateway is highly successful in providing the service to end users and consistently listed among the top five gateway community account usage. This continued growth and challenges of sustainability needed additional support to revisit the job management architecture.In this paper we describe the enhancements to the Ultrascan gateway middleware infrastructure provided through the TeraGrid Advanced User Support program. The advanced support efforts primarily focused on a) expanding the TeraGrid resources incorporate new machines; b) upgrading UltraScan's job management interfaces to use GRAM5 in place of the deprecated WS-GRAM; c) providing realistic usage scenarios to the GRAM5 and INCA resource testing and monitoring teams; d) creating general-purpose, resource-specific, and UltraScan-specific error handling and fault tolerance strategies; and e) providing forward and backward compatibility for the job management system between UltraScan's version 2 (currently in production) and version 3 (expected to be released mid-2011).},
booktitle = {Proceedings of the 2011 TeraGrid Conference: Extreme Digital Discovery},
articleno = {34},
numpages = {8},
keywords = {scientific workflows, fault tolerance, service oriented architecture, grid computing, scientific application management, distributed application abstractions, science gateways, application web services},
location = {Salt Lake City, Utah},
series = {TG '11}
}

@inproceedings{10.1145/2002951.2002961,
author = {Guo, Philip J.},
title = {Sloppy Python: Using Dynamic Analysis to Automatically Add Error Tolerance to Ad-Hoc Data Processing Scripts},
year = {2011},
isbn = {9781450308113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002951.2002961},
doi = {10.1145/2002951.2002961},
abstract = {Programmers and data analysts get frustrated when their long-running data processing scripts crash without producing results, due to either bugs in their code or inconsistencies in data sources. To alleviate this frustration, we developed a dynamic analysis technique that guarantees scripts will never crash: It converts all uncaught exceptions into special NA (Not Available) objects and continues executing rather than crashing. Thus, imperfect scripts will run to completion and produce partial results and an error log, which is more informative than simply crashing with no results. We implemented our technique as a "Sloppy" Python interpreter that automatically adds error tolerance to existing scripts without any programmer effort or run-time slowdown.},
booktitle = {Proceedings of the Ninth International Workshop on Dynamic Analysis},
pages = {35–40},
numpages = {6},
keywords = {scripting, fault tolerance, data processing},
location = {Toronto, Ontario, Canada},
series = {WODA '11}
}

@inproceedings{10.1145/2016904.2016910,
author = {Roveta, Francesco and Caviglia, Giorgio and Di Mario, Luca and Zanero, Stefano and Maggi, Federico and Ciuccarelli, Paolo},
title = {BURN: Baring Unknown Rogue Networks},
year = {2011},
isbn = {9781450306799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016904.2016910},
doi = {10.1145/2016904.2016910},
abstract = {Manual analysis of security-related events is still a necessity to investigate non-trivial cyber attacks. This task is particularly hard when the events involve slow, stealthy and large-scale activities typical of the modern cybercriminals' strategy. In this regard, visualization tools can effectively help analysts in their investigations. In this paper, we present BURN, an interactive visualization tool for displaying autonomous systems exhibiting rogue activity that helps at finding misbehaving networks through visual and interactive exploration. Up to seven values are displayed in a single visual element, while avoiding cumbersome and confusing maps. To this end, animations and alpha channels are leveraged to create simple views that highlight relevant activity patterns. In addition, BURN incorporates a simple algorithm to identify migrations of nefarious services across autonomous systems, which can support, for instance, root-cause analysis and law enforcement investigations.},
booktitle = {Proceedings of the 8th International Symposium on Visualization for Cyber Security},
articleno = {6},
numpages = {10},
location = {Pittsburgh, Pennsylvania, USA},
series = {VizSec '11}
}

@inproceedings{10.1145/2078827.2078830,
author = {Maurer, Max-Emanuel and De Luca, Alexander and Kempe, Sylvia},
title = {Using Data Type Based Security Alert Dialogs to Raise Online Security Awareness},
year = {2011},
isbn = {9781450309110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2078827.2078830},
doi = {10.1145/2078827.2078830},
abstract = {When browsing the Internet, users are likely to be exposed to security and privacy threats -- like fraudulent websites. Automatic browser mechanisms can protect them only to some extent. In other situations it is still important to raise the users' security awareness at the right moment. Passive indicators are mostly overlooked and blocking warnings are quickly dismissed by habituated users. In this work, we present a new concept of warnings that appear in-context, right next to data the user has just entered. Those dialogs are displayed whenever critical data types -- e.g. credit card data -- are entered by the users into online forms. Since they do not immediately interrupt the users' interaction but appear right in the users' focus, it is possible to place important security information in a way that it can be easily seen.We implemented the concept as a Firefox plugin and evaluated it in a row of studies including two lab studies, one focus group and one real world study. Results show that the concept is very well accepted by the users and that with the plugin, especially non-expert participants were more likely to identify fraudulent (or phishing) websites than using the standard browser warnings. Besides this, we were able to gather interesting findings on warning usage.},
booktitle = {Proceedings of the Seventh Symposium on Usable Privacy and Security},
articleno = {2},
numpages = {13},
keywords = {in-context, web browsing, data type based, security awareness},
location = {Pittsburgh, Pennsylvania},
series = {SOUPS '11}
}

@inproceedings{10.1145/2078827.2078831,
author = {Sotirakopoulos, Andreas and Hawkey, Kirstie and Beznosov, Konstantin},
title = {On the Challenges in Usable Security Lab Studies: Lessons Learned from Replicating a Study on SSL Warnings},
year = {2011},
isbn = {9781450309110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2078827.2078831},
doi = {10.1145/2078827.2078831},
abstract = {We replicated and extended a 2008 study conducted at CMU that investigated the effectiveness of SSL warnings. We adjusted the experimental design to mitigate some of the limitations of that prior study; adjustments include allowing participants to use their web browser of choice and recruiting a more representative user sample. However, during our study we observed a strong disparity between our participants actions during the laboratory tasks and their self-reported "would be" actions during similar tasks in everyday computer practices. Our participants attributed this disparity to the laboratory environment and the security it offered. In this paper we discuss our results and how the introduced changes to the initial study design may have affected them. Also, we discuss the challenges of observing natural behavior in a study environment, as well as the challenges of replicating previous studies given the rapid changes in web technology. We also propose alternatives to traditional laboratory study methodologies that can be considered by the usable security research community when investigating research questions involving sensitive data where trust may influence behavior.},
booktitle = {Proceedings of the Seventh Symposium on Usable Privacy and Security},
articleno = {3},
numpages = {18},
keywords = {study replication, study environment bias, experimental design, SSL warnings, usable security},
location = {Pittsburgh, Pennsylvania},
series = {SOUPS '11}
}

@inproceedings{10.1145/2078827.2078837,
author = {Jaferian, Pooya and Hawkey, Kirstie and Sotirakopoulos, Andreas and Velez-Rojas, Maria and Beznosov, Konstantin},
title = {Heuristics for Evaluating IT Security Management Tools},
year = {2011},
isbn = {9781450309110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2078827.2078837},
doi = {10.1145/2078827.2078837},
abstract = {The usability of IT security management (ITSM) tools is hard to evaluate by regular methods, making heuristic evaluation attractive. However, standard usability heuristics are hard to apply as IT security management occurs within a complex and collaborative context that involves diverse stakeholders. We propose a set of ITSM usability heuristics that are based on activity theory, are supported by prior research, and consider the complex and cooperative nature of security management. In a between-subjects study, we compared the employment of the ITSM and Nielsen's heuristics for evaluation of a commercial identity management system. Participants who used the ITSM set found more problems categorized as severe than those who used Nielsen's. As evaluators identified different types of problems with the two sets of heuristics, we recommend employing both the ITSM and Nielsen's heuristics during evaluation of ITSM tools.},
booktitle = {Proceedings of the Seventh Symposium on Usable Privacy and Security},
articleno = {7},
numpages = {20},
keywords = {complex systems, heuristic evaluation, computer supported cooperative work, IT security management},
location = {Pittsburgh, Pennsylvania},
series = {SOUPS '11}
}

@inproceedings{10.5555/2500753.2500778,
author = {Singh, Devinder Kaur Ajit and Palaniswamy, Vijayakumar and Raman, Vimal A./L. P. and Pearson, Hannah and Sien, Bong Pei and Rajaratnam, Bala S.},
title = {Can Exercises Using Virtual Reality Games Reduce Risk and Fear of Falls among Older Women?},
year = {2011},
publisher = {Singapore Therapeutic, Assistive &amp; Rehabilitative Technologies (START) Centre},
address = {Midview City, SGP},
abstract = {Background &amp; Aim: The objective of this study was to compare the risk and fear of falls among older women pre and post intervention using virtual reality games. Method: Thirty six community dwelling older women aged 56 and above were randomly divided into experimental (exercises using virtual reality games focus on improving balance) and control (conventional balance exercises) groups. Both groups attended an hour exercise session, twice a week for 6 weeks. Risk and fear of falls were measured using Physiological Profile Assessment (PPA) and Activity Specific Balance Scale (ABC-6). Pre and post intervention differences between the groups were examined. Results: Both the groups, virtual experiment and control group, had significant decrease in fall risk score after the interventions of exercises using virtual reality games and conventional balance exercise respectively (Z = - 3.38, p = 0.001 for experimental group; Z = - 2.50, p = 0.01 for control group). Significant difference was only demonstrated in the experiment group for ABC, t(17) = -2.78, p = 0.013. No significant differences were shown between the groups in risk and fear of falls post intervention. Conclusion: Practicing fun and interactive exercises using virtual reality games can focus on improving balance at home and reducing risk of falls in older adults.},
booktitle = {Proceedings of the 5th International Conference on Rehabilitation Engineering &amp; Assistive Technology},
articleno = {21},
numpages = {4},
keywords = {virtual reality, older women, fear of falls, risk of falls},
location = {Bangkok, Thailand},
series = {i-CREATe '11}
}

@inproceedings{10.1145/2009916.2009969,
author = {Feild, Henry Allen and Allan, James and Glatt, Joshua},
title = {CrowdLogging: Distributed, Private, and Anonymous Search Logging},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2009969},
doi = {10.1145/2009916.2009969},
abstract = {We describe CrowdLogging, an approach for distributed search log collection, storage, and mining, with the dual goals of preserving privacy and making the mined information broadly available. Most search log mining approaches and most privacy enhancing schemes have focused on centralized search logs and methods for disseminating them to third parties. In our approach, a user's search log is encrypted and shared in such a way that (a) the source of a search behavior artifact, such as a query, is unknown and (b) extremely rare artifacts---that is, artifacts more likely to contain private information---are not revealed. The approach works with any search behavior artifact that can be extracted from a search log, including queries, query reformulations, and query-click pairs. In this work, we: (1) present a distributed search log collection, storage, and mining framework; (2) compare several privacy policies, including differential privacy, showing the trade-offs between strong guarantees and the utility of the released data; (3) demonstrate the impact of our approach using two existing research query logs; and (4) describe a pilot study for which we implemented a version of the framework.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {375–384},
numpages = {10},
keywords = {distributed query logs, private query log analysis},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.5555/2145432.2145575,
author = {Hopkins, Mark and May, Jonathan},
title = {Tuning as Ranking},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO's scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {1352–1362},
numpages = {11},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/2145432.2145592,
author = {Martins, Andr\'{e} F. T. and Smith, Noah A. and Aguiar, Pedro M. Q. and Figueiredo, M\'{a}rio A. T.},
title = {Structured Sparsity in Structured Prediction},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Linear models have enjoyed great success in structured prediction in NLP. While a lot of progress has been made on efficient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. Common approaches employ ad hoc filtering or L1-regularization; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge. We fill this gap by adopting regularizers that promote structured sparsity, along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {1500–1511},
numpages = {12},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/2145432.2145569,
author = {Bar-Haim, Roy and Dinur, Elad and Feldman, Ronen and Fresko, Moshe and Goldstein, Guy},
title = {Identifying and Following Expert Investors in Stock Microblogs},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Information published in online stock investment message boards, and more recently in stock microblogs, is considered highly valuable by many investors. Previous work focused on aggregation of sentiment from all users. However, in this work we show that it is beneficial to distinguish expert users from non-experts. We propose a general framework for identifying expert investors, and use it as a basis for several models that predict stock rise from stock microblogging messages (stock tweets). In particular, we present two methods that combine expert identification and per-user unsupervised learning. These methods were shown to achieve relatively high precision in predicting stock rise, and significantly outperform our baseline. In addition, our work provides an in-depth analysis of the content and potential usefulness of stock tweets.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {1310–1319},
numpages = {10},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/2132960.2133022,
author = {Costa-juss\`{a}, Marta R. and Banchs, Rafael E.},
title = {The BM-I2R Haitian-Cr\'{e}Ole-to-English Translation System Description for the WMT 2011 Evaluation Campaign},
year = {2011},
isbn = {9781937284121},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This work describes the Haitian-Cr\'{e}ole to English statistical machine translation system built by Barcelona Media Innovation Center (BM) and Institute for Infocomm Research (I2R) for the 6th Workshop on Statistical Machine Translation (WMT 2011). Our system carefully processes the available data and uses it in a standard phrase-based system enhanced with a source context semantic feature that helps conducting a better lexical selection and a feature orthogonalization procedure that helps making MERT optimization more reliable and stable. Our system was ranked first (among a total of 9 participant systems) by the conducted human evaluation.},
booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
pages = {452–456},
numpages = {5},
location = {Edinburgh, Scotland},
series = {WMT '11}
}

@inproceedings{10.5555/2132960.2132964,
author = {Callison-Burch, Chris and Koehn, Philipp and Monz, Christof and Zaidan, Omar F.},
title = {Findings of the 2011 Workshop on Statistical Machine Translation},
year = {2011},
isbn = {9781937284121},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.},
booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
pages = {22–64},
numpages = {43},
location = {Edinburgh, Scotland},
series = {WMT '11}
}

@article{10.14778/3402707.3402744,
author = {Chen, Rui and Mohammed, Noman and Fung, Benjamin C. M. and Desai, Bipin C. and Xiong, Li},
title = {Publishing Set-Valued Data via Differential Privacy},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402707.3402744},
doi = {10.14778/3402707.3402744},
abstract = {Set-valued data provides enormous opportunities for various data mining tasks. In this paper, we study the problem of publishing set-valued data for data mining tasks under the rigorous differential privacy model. All existing data publishing methods for set-valued data are based on partition-based privacy models, for example k-anonymity, which are vulnerable to privacy attacks based on background knowledge. In contrast, differential privacy provides strong privacy guarantees independent of an adversary's background knowledge and computational power. Existing data publishing approaches for differential privacy, however, are not adequate in terms of both utility and scalability in the context of set-valued data due to its high dimensionality.We demonstrate that set-valued data could be efficiently released under differential privacy with guaranteed utility with the help of context-free taxonomy trees. We propose a probabilistic top-down partitioning algorithm to generate a differentially private release, which scales linearly with the input data size. We also discuss the applicability of our idea to the context of relational data. We prove that our result is (∈, δ)-useful for the class of counting queries, the foundation of many data mining tasks. We show that our approach maintains high utility for counting queries and frequent itemset mining and scales to large datasets through extensive experiments on real-life set-valued datasets.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1087–1098},
numpages = {12}
}

@article{10.1145/1978542.1978562,
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek},
title = {An Overview of Business Intelligence Technology},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1978542.1978562},
doi = {10.1145/1978542.1978562},
abstract = {BI technologies are essential to running today's businesses and this technology is going through sea changes.},
journal = {Commun. ACM},
month = aug,
pages = {88–98},
numpages = {11}
}

@article{10.1145/1978542.1978568,
author = {Wright, David},
title = {Should Privacy Impact Assessments Be Mandatory?},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1978542.1978568},
doi = {10.1145/1978542.1978568},
abstract = {Privacy impact assessments should be integrated into the overall approach to risk management with other strategic planning instruments.This article considers the issue of whether privacy impact assessments (PIAs) should be mandatory. I will examine the benefits and disadvantages of PIAs, the case for and against mandatory PIAs, and ultimately conclude they should be mandatory. Even if they are made mandatory, however, other factors, such as independent audits, must be taken into account to make them truly effective.},
journal = {Commun. ACM},
month = aug,
pages = {121–131},
numpages = {11}
}

@inproceedings{10.1145/2147805.2147834,
author = {Soni, Ameet and Shavlik, Jude},
title = {Probabilistic Ensembles for Improved Inference in Protein-Structure Determination},
year = {2011},
isbn = {9781450307963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2147805.2147834},
doi = {10.1145/2147805.2147834},
abstract = {Protein X-ray crystallography -- the most popular method for determining protein structures -- remains a laborious process requiring a great deal of manual crystallographer effort to interpret low-quality protein images. Automating this process is critical in creating a high-throughput protein-structure determination pipeline. Previously, our group developed ACMI, a probabilistic framework for producing protein-structure models from electron-density maps produced via X-ray crystallography. ACMI uses a Markov Random Field to model the 3D location of each non-hydrogen atom in a protein. Calculating the best structure in this model is intractable, so ACMI uses approximate inference methods to estimate the optimal structure. While previous results have shown ACMI to be the state-of-the-art method on this task, its approximate inference algorithm remains computationally expensive and susceptible to errors. In this work, we develop Probabilistic Ensembles in ACMI (PEA), a framework for leveraging multiple, independent runs of approximate inference to produce estimates of protein structures. Our results show statistically significant improvements in the accuracy of inference resulting in more complete and accurate protein structures. In addition, PEA provides a general framework for advanced approximate inference methods in complex problem domains.},
booktitle = {Proceedings of the 2nd ACM Conference on Bioinformatics, Computational Biology and Biomedicine},
pages = {264–273},
numpages = {10},
keywords = {ensembles, computational biology, statistical inference, protein-structure determination},
location = {Chicago, Illinois},
series = {BCB '11}
}

@article{10.14778/3402755.3402759,
author = {Cohen, Jeffrey and Eshleman, John and Hagenbuch, Brian and Kent, Joy and Pedrotti, Christopher and Sherry, Gavin and Waas, Florian},
title = {Online Expansion of Large-Scale Data Warehouses},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402755.3402759},
doi = {10.14778/3402755.3402759},
abstract = {Modern data warehouses store exceedingly large amounts of data, generally considered the crown jewels of an enterprise. The amount of data maintained in such data warehouses increases significantly over time---often at a continuous pace, e.g., by gathering additional data or retaining data for longer periods to derive additional business value, but occasionally also precipitously, e.g., when consolidating disparate data warehouses and Data Marts into a single database. Having to expand a data warehouse with 100's of TB of data by a substantial portion, e.g., 100% or more is a complex and disruptive maintenance operation as it typically involves some sort of dumping and reloading of data which requires substantial downtime.In this paper we describe the methodology and mechanisms we developed in Greenplum Database to expand large-scale data warehouses in an online fashion, i.e., without noticeable downtime. At the core of our approach is a set of robust and transactionally consistent primitives that enable efficient data movement. Special emphasis was put on usability and control that lets an administrator tailor the expansion process to specific operational characteristics via priorities and schedules.We present a number of experiments to quantify the impact of an on-going expansion on query workloads.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1249–1259},
numpages = {11}
}

