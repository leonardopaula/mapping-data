@inproceedings{10.1145/2019406.2019426,
author = {Raptis, Michalis and Kirovski, Darko and Hoppe, Hugues},
title = {Real-Time Classification of Dance Gestures from Skeleton Animation},
year = {2011},
isbn = {9781450309233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019406.2019426},
doi = {10.1145/2019406.2019426},
abstract = {We present a real-time gesture classification system for skeletal wireframe motion. Its key components include an angular representation of the skeleton designed for recognition robustness under noisy input, a cascaded correlation-based classifier for multivariate time-series data, and a distance metric based on dynamic time-warping to evaluate the difference in motion between an acquired gesture and an oracle for the matching gesture. While the first and last tools are generic in nature and could be applied to any gesture-matching scenario, the classifier is conceived based on the assumption that the input motion adheres to a known, canonical time-base: a musical beat. On a benchmark comprising 28 gesture classes, hundreds of gesture instances recorded using the XBOX Kinect platform and performed by dozens of subjects for each gesture class, our classifier has an average accuracy of 96:9%, for approximately 4-second skeletal motion recordings. This accuracy is remarkable given the input noise from the real-time depth sensor.},
booktitle = {Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {147–156},
numpages = {10},
location = {Vancouver, British Columbia, Canada},
series = {SCA '11}
}

@inproceedings{10.1145/2019406.2019423,
author = {Kider, Joseph T. and Pollock, Kaitlin and Safonova, Alla},
title = {A Data-Driven Appearance Model for Human Fatigue},
year = {2011},
isbn = {9781450309233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019406.2019423},
doi = {10.1145/2019406.2019423},
abstract = {Humans become visibly tired during physical activity. After a set of squats, jumping jacks or walking up a flight of stairs, individuals start to pant, sweat, loose their balance, and flush. Simulating these physiological changes due to exertion and exhaustion on an animated character greatly enhances a motion's realism. These fatigue factors depend on the mechanical, physical, and biochemical function states of the human body. The difficulty of simulating fatigue for character animation is due in part to the complex anatomy of the human body. We present a multi-modal capturing technique for acquiring synchronized biosignal data and motion capture data to enhance character animation. The fatigue model utilizes an anatomically derived model of the human body that includes a torso, organs, face, and rigged body. This model is then driven by biosignal output. Our animations show the wide range of exhaustion behaviors synthesized from real biological data output. We demonstrate the fatigue model by augmenting standard motion capture with exhaustion effects to produce more realistic appearance changes during three exercise examples. We compare the fatigue model with both simple procedural methods and a dense marker set data capture of exercise motions.},
booktitle = {Proceedings of the 2011 ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {119–128},
numpages = {10},
location = {Vancouver, British Columbia, Canada},
series = {SCA '11}
}

@inproceedings{10.1145/2037636.2037637,
author = {LaViola, Joseph J. and Keefe, Daniel F.},
title = {3D Spatial Interaction: Applications for Art, Design, and Science},
year = {2011},
isbn = {9781450309677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037636.2037637},
doi = {10.1145/2037636.2037637},
abstract = {3D interfaces use motion sensing, physical input, and spatial interaction techniques to effectively control highly dynamic virtual content. Now, with the advent of the Nintendo Wii, Sony Move, and Microsoft Kinect, game developers and researchers must create compelling interface techniques and game-play mechanics that make use of these technologies. At the same time, it is becoming increasingly clear that emerging game technologies are not just going to change the way we play games, they are also going to change the way we make and view art, design new products, analyze scientific datasets, and more.This introduction to 3D spatial interfaces demystifies the workings of modern videogame motion controllers and provides an overview of how it is used to create 3D interfaces for tasks such as 2D and 3D navigation, object selection and manipulation, and gesture-based application control. Topics include the strengths and limitations of various motion-controller sensing technologies in today's peripherals, useful techniques for working with these devices, and current and future applications of these technologies to areas beyond games. The course presents valuable information on how to utilize existing 3D user-interface techniques with emerging technologies, how to develop interface techniques, and how to learn from the successes and failures of spatial interfaces created for a variety of application domains.},
booktitle = {ACM SIGGRAPH 2011 Courses},
articleno = {1},
numpages = {75},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '11}
}

@inproceedings{10.1145/2071639.2071656,
author = {Jun, Wu and Lei, Mei and Luo, Zhong},
title = {Data Security Mechanism Based on Hierarchy Analysis for Internet of Things},
year = {2011},
isbn = {9781450305679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2071639.2071656},
doi = {10.1145/2071639.2071656},
abstract = {Internet of Things (IoT) is as a new generation of network service platform which could make everything connected to the global network connection. From the viewpoint of data analysis pattern, Internet of Things (IoT) is the new internet which has achieved the efficient integration of various resources which include computing, storage, data and applications. The virtualization and abstract model based on Internet of Things (IoT), which would be needed to build the data security mechanism that could protect the privacy of user data and security of personal information.},
booktitle = {Proceedings of the 2011 International Conference on Innovative Computing and Cloud Computing},
pages = {68–70},
numpages = {3},
keywords = {internet of things (IoT), hierarchy analytic, data security mechanism},
location = {Wuhan, China},
series = {ICCC '11}
}

@article{10.1145/1998384.1998388,
author = {Duong, Minh and Mostow, Jack and Sitaram, Sunayana},
title = {Two Methods for Assessing Oral Reading Prosody},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1550-4875},
url = {https://doi.org/10.1145/1998384.1998388},
doi = {10.1145/1998384.1998388},
abstract = {We compare two types of models to assess the prosody of children's oral reading. Template models measure how well the child's prosodic contour in reading a given sentence correlates in pitch, intensity, pauses, or word reading times with an adult narration of the same sentence. We evaluate template models directly against a common rubric used to assess fluency by hand, and indirectly by their ability to predict fluency and comprehension test scores and gains of 10 children who used Project LISTEN's Reading Tutor; the template models outpredict the human assessment.We also use the same set of adult narrations to train generalized models for mapping text to prosody, and use them to evaluate children's prosody. Using only durational features for both types of models, the generalized models perform better at predicting fluency and comprehension posttest scores of 55 children ages 7--10, with adjusted R2 of 0.6. Such models could help teachers identify which students are making adequate progress. The generalized models have the additional advantage of not requiring an adult narration of every sentence.},
journal = {ACM Trans. Speech Lang. Process.},
month = aug,
articleno = {14},
numpages = {22},
keywords = {children, Oral reading fluency, assessment, prosody, intelligent tutoring system}
}

@inproceedings{10.1145/2020408.2020464,
author = {Ghoting, Amol and Kambadur, Prabhanjan and Pednault, Edwin and Kannan, Ramakrishnan},
title = {NIMBLE: A Toolkit for the Implementation of Parallel Data Mining and Machine Learning Algorithms on Mapreduce},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020464},
doi = {10.1145/2020408.2020464},
abstract = {In the last decade, advances in data collection and storage technologies have led to an increased interest in designing and implementing large-scale parallel algorithms for machine learning and data mining (ML-DM). Existing programming paradigms for expressing large-scale parallelism such as MapReduce (MR) and the Message Passing Interface (MPI) have been the de facto choices for implementing these ML-DM algorithms. The MR programming paradigm has been of particular interest as it gracefully handles large datasets and has built-in resilience against failures. However, the existing parallel programming paradigms are too low-level and ill-suited for implementing ML-DM algorithms. To address this deficiency, we present NIMBLE, a portable infrastructure that has been specifically designed to enable the rapid implementation of parallel ML-DM algorithms. The infrastructure allows one to compose parallel ML-DM algorithms using reusable (serial and parallel) building blocks that can be efficiently executed using MR and other parallel programming models; it currently runs on top of Hadoop, which is an open-source MR implementation. We show how NIMBLE can be used to realize scalable implementations of ML-DM algorithms and present a performance evaluation.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {334–342},
numpages = {9},
keywords = {map/reduce, parallelism, data mining, machine learning},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{10.1145/2002945.2002947,
author = {Ganjisaffar, Yasser and Debeauvais, Thomas and Javanmardi, Sara and Caruana, Rich and Lopes, Cristina Videira},
title = {Distributed Tuning of Machine Learning Algorithms Using MapReduce Clusters},
year = {2011},
isbn = {9781450308441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002945.2002947},
doi = {10.1145/2002945.2002947},
abstract = {Obtaining the best accuracy in machine learning usually requires carefully tuning learning algorithm parameters for each problem. Parameter optimization is computationally challenging for learning methods with many hyperparameters. In this paper we show that MapReduce Clusters are particularly well suited for parallel parameter optimization. We use MapReduce to optimize regularization parameters for boosted trees and random forests on several text problems: three retrieval ranking problems and a Wikipedia vandalism problem. We show how model accuracy improves as a function of the percent of parameter space explored, that accuracy can be hurt by exploring parameter space too aggressively, and that there can be significant interaction between parameters that appear to be independent. Our results suggest that MapReduce is a two-edged sword: it makes parameter optimization feasible on a massive scale that would have been unimaginable just a few years ago, but also creates a new opportunity for overfitting that can reduce accuracy and lead to inferior learning parameters.},
booktitle = {Proceedings of the Third Workshop on Large Scale Data Mining: Theory and Applications},
articleno = {2},
numpages = {8},
keywords = {tuning, hyper-parameter, optimization, machine learning, MapReduce},
location = {San Diego, California},
series = {LDMTA '11}
}

@inproceedings{10.1145/2020408.2020459,
author = {McCloskey, Daniel P. and Kress, Michael E. and Imberman, Susan P. and Kushnir, Igor and Briffa-Mirabella, Susan},
title = {From Market Baskets to Mole Rats: Using Data Mining Techniques to Analyze RFID Data Describing Laboratory Animal Behavior},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020459},
doi = {10.1145/2020408.2020459},
abstract = {The use of new technologies, such as RFID sensors, provides scientists with novel ways of doing experimental research. As scientists become more technologically savvy and use these techniques, the traditional approaches to data analysis fail given the huge amounts of data produced by these methods. In this paper we describe an experiment in which colonies of naked mole rats were tagged with RFID transponders. RFID sensors were strategically placed in the mole rat caging system. The goal of this experiment was to document and analyze the interactions between animals. The huge amount of data produced by the sensors was not analyzable using the traditional methods employed by behavioral neuroscience researchers. Computational methods used by data miners, such as cluster analysis, association rule mining, and graphical models, were able to scale to the data and produce knowledge and insight that was previously unknown. This paper describes in detail the experimental setup and the computational methods used.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {301–306},
numpages = {6},
keywords = {maximal frequent itemsets, radio frequency identification, association rule mining, visualization, cluster analysis},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{10.1145/2002945.2002949,
author = {Dutta, Haimonti and Fiorletta, Huascar and Pooleery, Manoj and Diab, Hatim and German, Stanley and Waltz, David and Schevon, Catherine A.},
title = {A Case-Study on Learning from Large-Scale Intracranial EEG Data Using Multi-Core Machines and Clusters},
year = {2011},
isbn = {9781450308441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002945.2002949},
doi = {10.1145/2002945.2002949},
abstract = {Epilepsy is a chronic neurological disorder characterized by recurrent, unprovoked seizures that manifest in a variety of ways, including emotional or behavioral disturbances, convulsive movements, and loss of awareness. The problem of prediction of epileptic seizures is hard and most algorithms do not perform better than a random predictor [20]. An important reason why studies so far have been less than successful is that electroencephalogram (EEG) is not recorded at the granularity of the seizure generation process. Our collaborators at the Columbia University Medical School (CUMC) have been involved in a clinical trial which entails implanting a Micro-Electrode Array directly into the neocortex of epilepsy patients undergoing surgery to remove the portion of the brain from where seizures originate. The 96-contact grid allows researchers to record at 30 KHz/channel which is a very high resolution data collection procedure compared to known state-of-the-art techniques and yields both local field and action potential data (.5 TB per patient per day). This large volume of data poses challenges for knowledge discovery and mining.In this paper, we describe the steps required for processing the EEG signal and extraction of features; we present a parallel design for scaling up processing on multi-core machines and an in-house cluster. Initial benchmarking results indicate that approximately 6-cores of a machine (processing speed of 2.7 GHz, 32 GB RAM, moderate workload) is sufficient to process a 5 minute chunk of data from 96 channels in approximately 12 mins. Encouraged by these results, we discuss design of other machine learning algorithms for learning from the data.},
booktitle = {Proceedings of the Third Workshop on Large Scale Data Mining: Theory and Applications},
articleno = {4},
numpages = {8},
keywords = {large scale machine learning, multi-core machines, seizure prediction, clusters},
location = {San Diego, California},
series = {LDMTA '11}
}

@inproceedings{10.1145/2003653.2003654,
author = {Gupta, Chetan and Viswanathan, Krishnamurthy and Choudur, Lakshminarayan and Hao, Ming and Dayal, Umeshwar and Vennelakanti, Ravigopal and Helm, Paul and Dev, Anil and Manjunath, Sunil and Dhulipala, Sastry and Bellad, Sangamesh},
title = {Better Drilling through Sensor Analytics: A Case Study in Live Operational Intelligence},
year = {2011},
isbn = {9781450308328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2003653.2003654},
doi = {10.1145/2003653.2003654},
abstract = {In this paper, we present our Live Operational Intelligence (LOI) framework for developing, deploying, and executing applications that mine and analyze large amounts of data collected from multiple data sources to help operations staff take more informed decisions during management of operations in various industry verticals. We illustrate the use of the LOI framework with a case study from oil and gas drilling operations. The application involves characterizing and profiling on-shore wells being tapped for natural gas, and using this knowledge to construct a real-time operational intelligence engine for monitoring oil and gas drilling operations.},
booktitle = {Proceedings of the Fifth International Workshop on Knowledge Discovery from Sensor Data},
pages = {8–15},
numpages = {8},
keywords = {case study, sensor data analytics, operational intelligence},
location = {San Diego, California},
series = {SensorKDD '11}
}

@inproceedings{10.1145/2020408.2020450,
author = {Chen, Feng and Dai, Jing and Wang, Bingsheng and Sahu, Sambit and Naphade, Milind and Lu, Chang-Tien},
title = {Activity Analysis Based on Low Sample Rate Smart Meters},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020450},
doi = {10.1145/2020408.2020450},
abstract = {Activity analysis disaggregates utility consumption from smart meters into specific usage that associates with human activities. It can not only help residents better manage their consumption for sustainable lifestyle, but also allow utility managers to devise conservation programs. Existing research efforts on disaggregating consumption focus on analyzing consumption features with high sample rates (mainly between 1 Hz ~ 1MHz). However, many smart meter deployments support sample rates at most 1/900 Hz, which challenges activity analysis with occurrences of parallel activities, difficulty of aligning events, and lack of consumption features. We propose a novel statistical framework for disaggregation on coarse granular smart meter readings by modeling fixture characteristics, household behavior, and activity correlations. This framework has been implemented into two approaches for different application scenarios, and has been deployed to serve over 300 pilot households in Dubuque, IA. Interesting activity-level consumption patterns have been identified, and the evaluation on both real and synthetic datasets has shown high accuracy on discovering washer and shower.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {240–248},
numpages = {9},
keywords = {gaussian mixture model, classification, smart meter, hidden markov model, disaggregation, low sample rate},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{10.1145/2020408.2020481,
author = {Ramage, Daniel and Manning, Christopher D. and Dumais, Susan},
title = {Partially Labeled Topic Models for Interpretable Text Mining},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020481},
doi = {10.1145/2020408.2020481},
abstract = {Abstract Much of the world's electronic text is annotated with human-interpretable labels, such as tags on web pages and subject codes on academic publications. Effective text mining in this setting requires models that can flexibly account for the textual patterns that underlie the observed labels while still discovering unlabeled topics. Neither supervised classification, with its focus on label prediction, nor purely unsupervised learning, which does not model the labels explicitly, is appropriate. In this paper, we present two new partially supervised generative models of labeled text, Partially Labeled Dirichlet Allocation (PLDA) and the Partially Labeled Dirichlet Process (PLDP). These models make use of the unsupervised learning machinery of topic models to discover the hidden topics within each label, as well as unlabeled, corpus-wide latent topics. We explore applications with qualitative case studies of tagged web pages from del.icio.us and PhD dissertation abstracts, demonstrating improved model interpretability over traditional topic models. We use the many tags present in our del.icio.us dataset to quantitatively demonstrate the new models' higher correlation with human relatedness scores over several strong baselines.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {457–465},
numpages = {9},
keywords = {text mining, partially supervised learning},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{10.1145/2020408.2020479,
author = {El-Arini, Khalid and Guestrin, Carlos},
title = {Beyond Keyword Search: Discovering Relevant Scientific Literature},
year = {2011},
isbn = {9781450308137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020408.2020479},
doi = {10.1145/2020408.2020479},
abstract = {In scientific research, it is often difficult to express information needs as simple keyword queries. We present a more natural way of searching for relevant scientific literature. Rather than a string of keywords, we define a query as a small set of papers deemed relevant to the research task at hand. By optimizing an objective function based on a fine-grained notion of influence between documents, our approach efficiently selects a set of highly relevant articles. Moreover, as scientists trust some authors more than others, results are personalized to individual preferences. In a user study, researchers found the papers recommended by our method to be more useful, trustworthy and diverse than those selected by popular alternatives, such as Google Scholar and a state-of-the-art topic modeling approach.},
booktitle = {Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {439–447},
numpages = {9},
keywords = {personalization, citation analysis},
location = {San Diego, California, USA},
series = {KDD '11}
}

@inproceedings{10.1145/2023598.2023604,
author = {Sottara, D. and Mello, P. and Sartori, C. and Fry, E.},
title = {Enhancing a Production Rule Engine with Predictive Models Using Pmml},
year = {2011},
isbn = {9781450308373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2023598.2023604},
doi = {10.1145/2023598.2023604},
abstract = {In this paper we describe how the Predictive Model Markup Language (PMML) standard enhances the JBoss Drools production rule engine with native support for using predictive models in business rules. The historic debate between symbolic and connectionist approaches to rule/model orchestration provides numerous examples of hybrid systems combining "hard" and "soft" computing techniques to achieve different levels of integration. Rules are often used to decide when and which model to invoke; model outputs, in turn, can be used to evaluate the preconditions of a rule. In a loosely coupled system, the rule engine calls an external component implementing the predictive model, but this has several disadvantages, most notably the need to setup proper communications and reconcile any difference in the way the components encode the data. We propose instead, a tightly integrated system where predictive models and rules become part of the same reasoning framework. The models, encoded using the PMML 4 standard, are loaded and processed by a compiler implemented using the rule engine itself. The PMML document is transformed into a set of facts that define the model, and a series of rules that formalize the model's behavior. In addition, most PMML data processing, validation, and transformation procedures are also implemented using auto-generated rules. Finally, in oder to integrate model inputs and outputs seamlessly in the inference process, we exploit an extension of the Drools engine which adds native support for uncertainty and/or fuzziness.},
booktitle = {Proceedings of the 2011 Workshop on Predictive Markup Language Modeling},
pages = {39–47},
numpages = {9},
keywords = {pmml, predictive models, rule-based systems},
location = {San Diego, California, USA},
series = {PMML '11}
}

@inproceedings{10.1145/2074712.2074738,
author = {Turner, Phil},
title = {Everyday Coping: The Appropriation of Technology},
year = {2011},
isbn = {9781450310291},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2074712.2074738},
doi = {10.1145/2074712.2074738},
abstract = {Motivation -- To understand what everyday use of technology entails.Research approach -- A qualitative study.Findings/Design -- Highlighting the importance of everydayness in the design of interactive technologyResearch limitations/Implications -- to redefine what is meant by the everyday use of technology.Originality/Value -- Highlights the importance of understanding and designing for everydayness.Take away message -- everyday coping should be a focus of design and evaluation research.},
booktitle = {Proceedings of the 29th Annual European Conference on Cognitive Ergonomics},
pages = {127–133},
numpages = {7},
keywords = {coping, everydayness, familiarity},
location = {Rostock, Germany},
series = {ECCE '11}
}

@article{10.1145/2000791.2000796,
author = {Binkley, David W. and Harman, Mark and Lakhotia, Kiran},
title = {FlagRemover: A Testability Transformation for Transforming Loop-Assigned Flags},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000796},
doi = {10.1145/2000791.2000796},
abstract = {Search-Based Testing is a widely studied technique for automatically generating test inputs, with the aim of reducing the cost of software engineering activities that rely upon testing. However, search-based approaches degenerate to random testing in the presence of flag variables, because flags create spikes and plateaux in the fitness landscape. Both these features are known to denote hard optimization problems for all search-based optimization techniques. Several authors have studied flag removal transformations and fitness function refinements to address the issue of flags, but the problem of loop-assigned flags remains unsolved. This article introduces a testability transformation along with a tool that transforms programs with loop-assigned flags into flag-free equivalents, so that existing search-based test data generation approaches can successfully be applied. The article presents the results of an empirical study that demonstrates the effectiveness and efficiency of the testability transformation on programs including those made up of open source and industrial production code, as well as test data generation problems specifically created to denote hard optimization problems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {12},
numpages = {33},
keywords = {empirical evaluation, flags, testability transformation, Evolutionary testing}
}

@article{10.1145/2003690.2003693,
author = {Reddi, Vijay Janapa and Lee, Benjamin C. and Chilimbi, Trishul and Vaid, Kushagra},
title = {Mobile Processors for Energy-Efficient Web Search},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/2003690.2003693},
doi = {10.1145/2003690.2003693},
abstract = {As cloud and utility computing spreads, computer architects must ensure continued capability growth for the data centers that comprise the cloud. Given megawatt scale power budgets, increasing data center capability requires increasing computing hardware energy efficiency. To increase the data center's capability for work, the work done per Joule must increase. We pursue this efficiency even as the nature of data center applications evolves. Unlike traditional enterprise workloads, which are typically memory or I/O bound, big data computation and analytics exhibit greater compute intensity. This article examines the efficiency of mobile processors as a means for data center capability. In particular, we compare and contrast the performance and efficiency of the Microsoft Bing search engine executing on the mobile-class Atom processor and the server-class Xeon processor. Bing implements statistical machine learning to dynamically rank pages, producing sophisticated search results but also increasing computational intensity. While mobile processors are energy-efficient, they exact a price for that efficiency. The Atom is 5\texttimes{} more energy-efficient than the Xeon when comparing queries per Joule. However, search queries on Atom encounter higher latencies, different page results, and diminished robustness for complex queries. Despite these challenges, quality-of-service is maintained for most, common queries. Moreover, as different computational phases of the search engine encounter different bottlenecks, we describe implications for future architectural enhancements, application tuning, and system architectures. After optimizing the Atom server platform, a large share of power and cost go toward processor capability. With optimized Atoms, more servers can fit in a given data center power budget. For a data center with 15MW critical load, Atom-based servers increase capability by 3.2\texttimes{} for Bing.},
journal = {ACM Trans. Comput. Syst.},
month = aug,
articleno = {9},
numpages = {39},
keywords = {mobile hardware architectures, Energy-efficient data centers, web search}
}

@inproceedings{10.1145/2037373.2037400,
author = {Cherubini, Mauro and de Oliveira, Rodrigo and Hiltunen, Anna and Oliver, Nuria},
title = {Barriers and Bridges in the Adoption of Today's Mobile Phone Contextual Services},
year = {2011},
isbn = {9781450305419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037373.2037400},
doi = {10.1145/2037373.2037400},
abstract = {This paper presents ethnographic observations, a diary study and a large-scale quantitative questionnaire (n=395) designed to study the reasons for adoption and refusal of context-aware mobile applications. Through a qualitative study we identify 24 user needs that these applications fulfill and 9 barriers for adoption. We found that for many of the identified needs the end-goal is not that of receiving information, thus complementing work on mobile information needs. Also, this work offers an actionable list of obstacles that prevent contextual services to reach a larger audience. Finally, our findings suggest the opportunity to develop novel mobile applications that fulfill needs in the activity and personal contextual dimensions, and that of developing an application store for feature phones.},
booktitle = {Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {167–176},
numpages = {10},
keywords = {context, mobile, human needs, ethnographic study, diary study},
location = {Stockholm, Sweden},
series = {MobileHCI '11}
}

@inproceedings{10.1145/2037373.2037432,
author = {Obrist, Marianna and Beck, Elke and Wurhofer, Daniela and Tscheligi, Manfred},
title = {<i>Experience Characters</i>: A Design Tool for Communicating Mobile Phone Experiences to Designers},
year = {2011},
isbn = {9781450305419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037373.2037432},
doi = {10.1145/2037373.2037432},
abstract = {Different methods, techniques and tools exist for supporting design activities in a user-centered design process. Due to the increasing relevance of experience-centered design, the need to advance existing design methods and tools becomes evident. Within this paper we present "experience characters" as a design tool for communicating a richer understanding of mobile phone experiences towards designers. We conducted a qualitative text analysis study of written experience reports from an online forum in the tradition of grounded theory. Major experience types were grouped into categories and further transformed into five fictive characters that are easy to remember and likely to invoke empathy in design. Thus, designers' communication on and understanding of mobile phone experiences are supported. We describe in detail the steps taken for analysing experience reports and developing the experience characters, including relevant properties for informing and supporting an experience-centered design process.},
booktitle = {Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {385–394},
numpages = {10},
keywords = {text analysis, grounded theory, non-reactive research method, mobile user experience, experience reports, design methods},
location = {Stockholm, Sweden},
series = {MobileHCI '11}
}

@inproceedings{10.1145/2037373.2037402,
author = {Fischer, Joel E. and Greenhalgh, Chris and Benford, Steve},
title = {Investigating Episodes of Mobile Phone Activity as Indicators of Opportune Moments to Deliver Notifications},
year = {2011},
isbn = {9781450305419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037373.2037402},
doi = {10.1145/2037373.2037402},
abstract = {We investigate whether opportune moments to deliver notifications surface at the endings of episodes of mobile interaction (making voice calls or receiving SMS) based on the assumption that the endings collocate with naturally occurring breakpoint in the user's primary task. Testing this with a naturalistic experiment we find that interruptions (notifications) are attended to and dealt with significantly more quickly after a user has finished an episode of mobile interaction compared to a random baseline condition, supporting the potential utility of this notification strategy. We also find that the workload and situational appropriateness of the secondary interruption task significantly affect subsequent delay and completion rate of the tasks. In situ self-reports and interviews reveal complexities in the subjective experience of the interruption, which suggest that a more nuanced classification of the particular call or SMS and its relationship to the primary task(s) would be desirable.},
booktitle = {Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services},
pages = {181–190},
numpages = {10},
keywords = {interruptions, context-awareness, interruptibility, ESM, experience-sampling, studies in the wild, receptivity},
location = {Stockholm, Sweden},
series = {MobileHCI '11}
}

@inproceedings{10.1145/2030376.2030391,
author = {Whissell, John S. and Clarke, Charles L. A.},
title = {Clustering for Semi-Supervised Spam Filtering},
year = {2011},
isbn = {9781450307888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030376.2030391},
doi = {10.1145/2030376.2030391},
abstract = {We present a novel investigation of email clustering, demonstrating that clustering can be a powerful tool for email spam filtering. We first extend the well-known notion that ham and spam emails can be divided into clusters, showing the striking result that almost any reasonable clustering algorithm will naturally partition an email dataset into almost entirely spam and entirely spam clusters. We then consider the specific semi-supervised spam filtering scenario of filtering when a large amount of training data is available, but only a few true labels can be obtained for that data. We present two spam filtering approaches for this scenario, both of which start with a clustering of training email. Our first approach uses the true labels of the medoids of each cluster to train a spam filter; our second approach functions similar to the first, except that the true label of each cluster's medoid is used as the label of every email within the cluster, giving a much larger set of labels for training, while still only requiring only a few labels. We evaluate our approaches using the TREC2005 and CEAS2008 spam email datasets. For a large range of different numbers of true labels, we show that both of our approaches significantly outperform training on the same number of randomly selected email messages. The results of our second approach are also better than those of a previously published state-of-the-art semi-supervised small sample spam filtering approach.},
booktitle = {Proceedings of the 8th Annual Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference},
pages = {125–134},
numpages = {10},
keywords = {clustering, spam filtering, email},
location = {Perth, Australia},
series = {CEAS '11}
}

@article{10.1145/2047478.2047481,
author = {Schatz, Bruce R.},
title = {New Health Systems Using Network Information Technologies},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/2047478.2047481},
doi = {10.1145/2047478.2047481},
abstract = {A new health system is coming based on network information technologies. It consists of health measurement across the full spectrum of health determinants, using mobile devices such as smartphones. This measurement generates personal health records for each individual within a population, which can then be analyzed to identify cohorts, using statistical clustering on supercomputers. These cohorts determine "which persons have which outcomes", so health management can be supported for higher quality at lower cost. This tutorial concisely summarizes the arguments in the recent book Healthcare Infrastructure: Health Systems for Individuals and Populations.},
journal = {SIGHIT Rec.},
month = sep,
pages = {23–26},
numpages = {4},
keywords = {health determinants, mobile devices, population cohorts, smartphones, health systems, supercomputers, healthcare infrastructure}
}

@article{10.1145/2000799.2000804,
author = {Maoz, Shahar and Harel, David and Kleinbort, Asaf},
title = {A Compiler for Multimodal Scenarios: Transforming LSCs into AspectJ},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000799.2000804},
doi = {10.1145/2000799.2000804},
abstract = {We exploit the main similarity between the aspect-oriented programming paradigm and the inter-object, scenario-based approach to specification, in order to construct a new way of executing systems based on the latter. Specifically, we transform multimodal scenario-based specifications, given in the visual language of live sequence charts (LSC), into what we call scenario aspects, implemented in AspectJ. Unlike synthesis approaches, which attempt to take the inter-object scenarios and construct intra-object state-based per-object specifications or a single controller automaton, we follow the ideas behind the LSC play-out algorithm to coordinate the simultaneous monitoring and direct execution of the specified scenarios. Thus, the structure of the specification is reflected in the structure of the generated code; the high-level inter-object requirements and their structure are not lost in the translation.The transformation/compilation scheme is fully implemented in a UML2-compliant tool we term the S2A compiler (for Scenarios to Aspects), which provides full code generation of reactive behavior from inter-object multimodal scenarios. S2A supports advanced scenario-based programming features, such as multiple instances and exact and symbolic parameters. We demonstrate our work with an application whose inter-object behaviors are specified using LSCs. We discuss advantages and challenges of the compilation scheme in the context of the more general vision of scenario-based programming.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {18},
numpages = {41},
keywords = {UML sequence diagrams, live sequence charts, scenario-based programming, code generation, Aspect oriented programming, inter-object approach, scenarios, visual formalisms}
}

@inproceedings{10.1145/2030376.2030394,
author = {West, Andrew G. and Chang, Jian and Venkatasubramanian, Krishna and Sokolsky, Oleg and Lee, Insup},
title = {Link Spamming Wikipedia for Profit},
year = {2011},
isbn = {9781450307888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030376.2030394},
doi = {10.1145/2030376.2030394},
abstract = {Collaborative functionality is an increasingly prevalent web technology. To encourage participation, these systems usually have low barriers-to-entry and permissive privileges. Unsurprisingly, ill-intentioned users try to leverage these characteristics for nefarious purposes. In this work, a particular abuse is examined -- link spamming -- the addition of promotional or otherwise inappropriate hyperlinks.Our analysis focuses on the wiki model and the collaborative encyclopedia, Wikipedia, in particular. A principal goal of spammers is to maximize exposure, the quantity of people who view a link. Creating and analyzing the first Wikipedia link spam corpus, we find that existing spam strategies perform quite poorly in this regard. The status quo spamming model relies on link persistence to accumulate exposures, a strategy that fails given the diligence of the Wikipedia community. Instead, we propose a model that exploits the latency inherent in human anti-spam enforcement.Statistical estimation suggests our novel model would produce significantly more link exposures than status quo techniques. More critically, the strategy could prove economically viable for perpetrators, incentivizing its exploitation. To this end, we address mitigation strategies.},
booktitle = {Proceedings of the 8th Annual Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference},
pages = {152–161},
numpages = {10},
keywords = {collaborative security, wikis, spam economics, link spam, web 2.0 spam, attack model, measurement study, Wikipedia},
location = {Perth, Australia},
series = {CEAS '11}
}

@article{10.14778/2047485.2047487,
author = {Marcus, Adam and Wu, Eugene and Karger, David and Madden, Samuel and Miller, Robert},
title = {Human-Powered Sorts and Joins},
year = {2011},
issue_date = {September 2011},
publisher = {VLDB Endowment},
volume = {5},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/2047485.2047487},
doi = {10.14778/2047485.2047487},
abstract = {Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible to task people with small jobs, such as labeling images or looking up phone numbers, via a programmatic interface. MTurk tasks for processing datasets with humans are currently designed with significant reimplementation of common workflows and ad-hoc selection of parameters such as price to pay per task. We describe how we have integrated crowds into a declarative workflow engine called Qurk to reduce the burden on workflow designers. In this paper, we focus on how to use humans to compare items for sorting and joining data, two of the most common operations in DBMSs. We describe our basic query interface and the user interface of the tasks we post to MTurk. We also propose a number of optimizations, including task batching, replacing pairwise comparisons with numerical ratings, and pre-filtering tables before joining them, which dramatically reduce the overall cost of running sorts and joins on the crowd. In an experiment joining two sets of images, we reduce the overall cost from $67 in a naive implementation to about $3, without substantially affecting accuracy or latency. In an end-to-end experiment, we reduced cost by a factor of 14.5.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {13–24},
numpages = {12}
}

@inproceedings{10.1145/2024288.2024344,
author = {Federico, Paolo and Aigner, Wolfgang and Miksch, Silvia and Windhager, Florian and Zenk, Lukas},
title = {A Visual Analytics Approach to Dynamic Social Networks},
year = {2011},
isbn = {9781450307321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024288.2024344},
doi = {10.1145/2024288.2024344},
abstract = {The visualization and analysis of dynamic networks have become increasingly important in several fields, for instance sociology or economics. The dynamic and multi-relational nature of this data poses the challenge of understanding both its topological structure and how it changes over time. In this paper we propose a visual analytics approach for analyzing dynamic networks that integrates: a dynamic layout with user-controlled trade-off between stability and consistency; three temporal views based on different combinations of node-link diagrams (layer superimposition, layer juxtaposition, and two-and-a-half-dimensional view); the visualization of social network analysis metrics; and specific interaction techniques for tracking node trajectories and node connectivity over time. This integration of visual, interactive, and automatic methods supports the multi-faceted analysis of dynamically changing networks.},
booktitle = {Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies},
articleno = {47},
numpages = {8},
keywords = {graph drawing, interaction, dynamic layout, visual analytics, information visualization, social network analysis, dynamic networks},
location = {Graz, Austria},
series = {i-KNOW '11}
}

@inproceedings{10.1145/2095667.2095669,
author = {Doppler, Jakob and Rubisch, Julian and Jaksche, Michael and Raffaseder, Hannes},
title = {RaPScoM: Towards Composition Strategies in a Rapid Score Music Prototyping Framework},
year = {2011},
isbn = {9781450310819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095667.2095669},
doi = {10.1145/2095667.2095669},
abstract = {Especially in the low-budget and amateur score music production workflow the triangular communication between editor, director and composer is constrained by limited resources, tight schedules and the lack of common domain knowledge. Often sound-a-like ideas and precomposed material end up as static temp tracks in the rough cut and thus limit the flexibility in the composition process. Our rapid score music prototyping framework RaPScoM aims at supporting the workflow with a toolchain for semi-automated and customizeable temp track generation by exposing a set of high-level parameters based on semantic movie annotation derived from movie clip analysis and a set of basic composition rules that could be derived from exemplary (score) music. In this paper we give an overview of the framework architecture and discuss technical details on our semantic movie annotation strategy and some core composition components.},
booktitle = {Proceedings of the 6th Audio Mostly Conference: A Conference on Interaction with Sound},
pages = {8–14},
numpages = {7},
keywords = {dynamic composition, generative music, new music interfaces},
location = {Coimbra, Portugal},
series = {AM '11}
}

@inproceedings{10.1145/2025113.2025129,
author = {Treude, Christoph and Storey, Margaret-Anne},
title = {Effective Communication of Software Development Knowledge through Community Portals},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025129},
doi = {10.1145/2025113.2025129},
abstract = {Knowledge management plays an important role in many software organizations. Knowledge can be captured and distributed using a variety of media, including traditional help files and manuals, videos, technical articles, wikis, and blogs. In recent years, web-based community portals have emerged as an important mechanism for combining various communication channels. However, there is little advice on how they can be effectively deployed in a software project.In this paper, we present a first study of a community portal used by a closed source software project. Using grounded theory, we develop a model that characterizes documentation artifacts along several dimensions, such as content type, intended audience, feedback options, and review mechanisms. Our findings lead to actionable advice for industry by articulating the benefits and possible shortcomings of the various communication channels in a knowledge-sharing portal. We conclude by suggesting future research on the increasing adoption of community portals in software engineering projects.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {91–101},
numpages = {11},
keywords = {community portal, knowledge, documentation},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/2073276.2073288,
author = {Hamacher, Kay and Katzenbeisser, Stefan},
title = {Public Security: Simulations Need to Replace Conventional Wisdom},
year = {2011},
isbn = {9781450310789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2073276.2073288},
doi = {10.1145/2073276.2073288},
abstract = {Is more always better? Is conventional wisdom always the right guideline in the development of security policies that have large opportunity costs? Is the evaluation of security measures after their introduction the best way? In the past, these questions were frequently left unasked before the introduction of many public security measures. In this paper we put forward the new paradigm that agent-based simulations are an effective and most likely the only sustainable way for the evaluation of public security measures in a complex environment. As a case-study we provide a critical assessment of the power of Telecommunications Data Retention (TDR), which was introduced in most European countries, despite its huge impact on privacy. Up to now it is unknown whether TDR has any benefits in the identification of terrorist dark nets in the period before an attack. The results of our agent-based simulations suggest, contrary to conventional wisdom, that the current practice of acquiring more data may not necessarily yield higher identification rates.},
booktitle = {Proceedings of the 2011 New Security Paradigms Workshop},
pages = {115–124},
numpages = {10},
keywords = {privacy, simulations, data retention},
location = {Marin County, California, USA},
series = {NSPW '11}
}

@inproceedings{10.1145/2076006.2076019,
author = {Hienert, Daniel and Zapilko, Benjamin and Schaer, Philipp and Mathiak, Brigitte},
title = {Web-Based Multi-View Visualizations for Aggregated Statistics},
year = {2011},
isbn = {9781450308236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2076006.2076019},
doi = {10.1145/2076006.2076019},
abstract = {With the rise of the open data movement a lot of statistical data has been made publicly available by governments, statistical offices and other organizations. First efforts to visualize are made by the data providers themselves. Data aggregators go a step beyond: they collect data from different open data repositories and make them comparable by providing data sets from different providers and showing different statistics in the same chart. Another approach is to visualize two different indicators in a scatter plot or on a map. The integration of several data sets in one graph can have several drawbacks: different scales and units are mixed, the graph gets visually cluttered and one cannot easily distinguish between different indicators. Our approach marks a combination of (1) the integration of live data from different data sources, (2) presenting different indicators in coordinated visualizations and (3) allows adding user visualizations to enrich official statistics with personal data. Each indicator gets its own visualization, which fits best for the individual indicator in case of visualization type, scale, unit etc. The different visualizations are linked, so that related items can easily be identified by using mouse over effects on data items.},
booktitle = {Proceedings of the 5th International Workshop on Web APIs and Service Mashups},
articleno = {11},
numpages = {8},
keywords = {coordinated views, dashboards, information visualization, open data, visual analytics, statistics, time series},
location = {Lugano, Switzerland},
series = {Mashups '11}
}

@inproceedings{10.1145/2030112.2030164,
author = {Rabbi, Mashfiqui and Ali, Shahid and Choudhury, Tanzeem and Berke, Ethan},
title = {Passive and In-Situ Assessment of Mental and Physical Well-Being Using Mobile Sensors},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030164},
doi = {10.1145/2030112.2030164},
abstract = {The idea of continuously monitoring well-being using mobile-sensing systems is gaining popularity. In-situ measurement of human behavior has the potential to overcome the short comings of gold-standard surveys that have been used for decades by the medical community. However, current sensing systems have mainly focused on tracking physical health; some have approximated aspects of mental health based on proximity measurements but have not been compared against medically accepted screening instruments. In this paper, we show the feasibility of a multi-modal mobile sensing system to simultaneously assess mental and physical health. By continuously capturing fine-grained motion and privacy-sensitive audio data, we are able to derive different metrics that reflect the results of commonly used surveys for assessing well-being by the medical community. In addition, we present a case study that highlights how errors in assessment due to the subjective nature of the responses could potentially be avoided by continuous mobile sensing.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {385–394},
numpages = {10},
keywords = {machine learning, physical health, mental health, mobile sensing, activity inference},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2030112.2030121,
author = {Ho, Bo-Jhang and Kao, Hsin-Liu Cindy and Chen, Nan-Chen and You, Chuang-Wen and Chu, Hao-Hua and Chen, Ming-Syan},
title = {HeatProbe: A Thermal-Based Power Meter for Accounting Disaggregated Electricity Usage},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030121},
doi = {10.1145/2030112.2030121},
abstract = {To promote energy-saving behavior, disaggregating electricity usage is critical for increasing consumer awareness of energy usage behavior. This study proposes HeatProbe, a thermal-based power meter system that uses thermal imaging to track disaggregated appliance usage. We have designed, prototyped, and tested the HeatProbe system. Results show that HeatProbe successfully senses individual appliance operating durations with an average error of 125.03 seconds, achieving 80.2% appliance power accounting accuracy in different appliance usage scenarios.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {55–64},
numpages = {10},
keywords = {power consumption monitoring, per-appliance power dis-aggregation},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2030112.2030128,
author = {Yuan, Jing and Zheng, Yu and Zhang, Liuhang and Xie, XIng and Sun, Guangzhong},
title = {Where to Find My next Passenger},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030128},
doi = {10.1145/2030112.2030128},
abstract = {We present a recommender for taxi drivers and people expecting to take a taxi, using the knowledge of 1) passengers' mobility patterns and 2) taxi drivers' pick-up behaviors learned from the GPS trajectories of taxicabs. First, this recommender provides taxi drivers with some locations and the routes to these locations, towards which they are more likely to pick up passengers quickly (during the routes or at these locations) and maximize the profit. Second, it recommends people with some locations (within a walking distance) where they can easily find vacant taxis. In our method, we learn the above knowledge (represented by probabilities) from GPS trajectories of taxis. Then, we feed the knowledge into a probabilistic model which estimates the profit of the candidate locations for a particular driver based on where and when the driver requests for the recommendation. We validate our recommender using historical trajectories generated by over 12,000 taxis during 110 days.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {109–118},
numpages = {10},
keywords = {taxicab, recommender, parking place},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2030112.2030154,
author = {Fukui, Rui and Watanabe, Masahiko and Gyota, Tomoaki and Shimosaka, Masamichi and Sato, Tomomasa},
title = {Hand Shape Classification with a Wrist Contour Sensor: Development of a Prototype Device},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030154},
doi = {10.1145/2030112.2030154},
abstract = {In this paper, we describe a novel sensor device which recognizes hand shapes using wrist contours. Although hand shapes can express various meanings with small gestures, utilization of hand shapes as an interface is rare in domestic use. That is because a concise recognition method has not been established. To recognize hand shapes anywhere with no stress on the user, we developed a wearable wrist contour sensor device and a recognition system. In the system, features, such as sum of gaps, were extracted from wrist contours. We conducted a classification test of eight hand shapes, and realized approximately 70% classification rate.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {311–314},
numpages = {4},
keywords = {gestureinterface., classification, wrist contour, hand shape recognition},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2030112.2030163,
author = {Larson, Eric C. and Lee, TienJui and Liu, Sean and Rosenfeld, Margaret and Patel, Shwetak N.},
title = {Accurate and Privacy Preserving Cough Sensing Using a Low-Cost Microphone},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030163},
doi = {10.1145/2030112.2030163},
abstract = {Audio-based cough detection has become more pervasive in recent years because of its utility in evaluating treatments and the potential to impact the quality of life for individuals with chronic cough. We critically examine the current state of the art in cough detection, concluding that existing approaches expose private audio recordings of users and bystanders. We present a novel algorithm for detecting coughs from the audio stream of a mobile phone. Our system allows cough sounds to be reconstructed from the feature set, but prevents speech from being reconstructed intelligibly. We evaluate our algorithm on data collected in the wild and report an average true positive rate of 92% and false positive rate of 0.5%. We also present the results of two psychoacoustic experiments which characterize the tradeoff between the fidelity of reconstructed cough sounds and the intelligibility of reconstructed speech.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {375–384},
numpages = {10},
keywords = {cough detection, privacy, sensing, mobile phones, signal processing, health},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2030112.2030141,
author = {Tang, Karen P. and Hong, Jason I. and Siewiorek, Daniel P.},
title = {Understanding How Visual Representations of Location Feeds Affect End-User Privacy Concerns},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030141},
doi = {10.1145/2030112.2030141},
abstract = {While past work has looked extensively at how to design privacy configuration UIs for sharing current location, there has not yet been work done to examine how visual representations of historical locations can influence end-user privacy. We present results for a study examining three visualization types (text-, map-, and time-based) for social sharing of past locations. Our results reveal that there are important design implications for location sharing applications, as certain visual elements led to more privacy concerns and inaccurate perceptions of privacy control.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {207–216},
numpages = {10},
keywords = {location, visualization, location sharing, privacy concerns},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2030112.2030130,
author = {Becker, Richard A. and Caceres, Ramon and Hanson, Karrie and Loh, Ji Meng and Urbanek, Simon and Varshavsky, Alexander and Volinsky, Chris},
title = {Route Classification Using Cellular Handoff Patterns},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030130},
doi = {10.1145/2030112.2030130},
abstract = {Understanding utilization of city roads is important for urban planners. In this paper, we show how to use handoff patterns from cellular phone networks to identify which routes people take through a city. Specifically, this paper makes three contributions. First, we show that cellular handoff patterns on a given route are stable across a range of conditions and propose a way to measure stability within and between routes using a variant of Earth Mover's Distance. Second, we present two accurate classification algorithms for matching cellular handoff patterns to routes: one requires test drives on the routes while the other uses signal strength data collected by high-resolution scanners. Finally, we present an application of our algorithms for measuring relative volumes of traffic on routes leading into and out of a specific city, and validate our methods using statistics published by a state transportation authority.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {123–132},
numpages = {10},
keywords = {handoff patterns, route classification},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/2030080.2030086,
author = {Veloso, Marco and Phithakkitnukoon, Santi and Bento, Carlos},
title = {Urban Mobility Study Using Taxi Traces},
year = {2011},
isbn = {9781450309332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030080.2030086},
doi = {10.1145/2030080.2030086},
abstract = {In this work, we analyze taxi-GPS traces collected in Lisbon, Portugal. We perform an exploratory analysis to visualize the spatiotemporal variation of taxi services; explore the relationships between pick-up and drop-off locations; and analyze the behavior in downtime (between the previous drop-off and the following pick-up). We also carry out the analysis of predictability of taxi trips for the next pick-up area type given history of taxi flow in time and space.},
booktitle = {Proceedings of the 2011 International Workshop on Trajectory Data Mining and Analysis},
pages = {23–30},
numpages = {8},
keywords = {na?ve bayesian classifier, taxi-gps traces, spatiotemporal analysis, urban mobility},
location = {Beijing, China},
series = {TDMA '11}
}

@inproceedings{10.1145/2030045.2030055,
author = {Kadouche, Rachid and Abdulrazak, Bessam},
title = {Inhabitant Prediction in Smart Houses},
year = {2011},
isbn = {9781450309264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030045.2030055},
doi = {10.1145/2030045.2030055},
abstract = {This paper addresses the inhabitant prediction issue in smart houses based on daily life activities. We use data provided by non intrusive sensors and devices to predict the house occupant. Support Vector Machines (SVM) classifier was applied to build a Behavior Classification Model (BCM) and learn the users' habits when they perform activities for predicting and identifying the house occupant. The model was tested using data coming from the Washington State University smart apartment tesbed and data from experiment held with six users at the DOMUS apartment. The BCM model results was also compared with a frequency based approach.},
booktitle = {Proceedings of the 2011 International Workshop on Situation Activity &amp; Goal Awareness},
pages = {27–36},
numpages = {10},
keywords = {support vector machines, smart houses, inhabitant recognition},
location = {Beijing, China},
series = {SAGAware '11}
}

@inproceedings{10.1145/2030066.2030076,
author = {Teraoka, Teruhiko},
title = {Aggregation and Exploration of Heterogeneous Data Collected from Diverse Information Sources},
year = {2011},
isbn = {9781450309257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030066.2030076},
doi = {10.1145/2030066.2030076},
abstract = {This paper describes aggregation and exploration of heterogeneous data collected from various information sources. We studied the organization of such data from a user's point of view. Interactive visual interfaces are essential for exploring heterogeneous data. Our current research on the exploration of data is discussed and a study of social use of heterogeneous data is explained. An early prototype system is also introduced for data collected from several Web services and mobile devices.},
booktitle = {Proceedings of 1st International Symposium on From Digital Footprints to Social and Community Intelligence},
pages = {31–36},
numpages = {6},
keywords = {aggregation, heterogeneous data, community memory, exploration, personal data},
location = {Beijing, China},
series = {SCI '11}
}

@inproceedings{10.1145/2030613.2030630,
author = {Zang, Hui and Bolot, Jean},
title = {Anonymization of Location Data Does Not Work: A Large-Scale Measurement Study},
year = {2011},
isbn = {9781450304924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030613.2030630},
doi = {10.1145/2030613.2030630},
abstract = {We examine a very large-scale data set of more than 30 billion call records made by 25 million cell phone users across all 50 states of the US and attempt to determine to what extent anonymized location data can reveal private user information. Our approach is to infer, from the call records, the "top N" locations for each user and correlate this information with publicly-available side information such as census data. For example, the measured "top 2" locations likely correspond to home and work locations, the "top 3" to home, work, and shopping/school/commute path locations. We consider the cases where those "top N" locations are measured with different levels of granularity, ranging from a cell sector to whole cell, zip code, city, county and state. We then compute the anonymity set, namely the number of users uniquely identified by a given set of "top N" locations at different granularity levels. We find that the "top 1" location does not typically yield small anonymity sets. However, the top 2 and top 3 locations do, certainly at the sector or cell-level granularity. We consider a variety of different factors that might impact the size of the anonymity set, for example the distance between the "top N" locations or the geographic environment (rural vs urban). We also examine to what extent specific side information, in particular the size of the user's social network, decrease the anonymity set and therefore increase risks to privacy. Our study shows that sharing anonymized location data will likely lead to privacy risks and that, at a minimum, the data needs to be coarse in either the time domain (meaning the data is collected over short periods of time, in which case inferring the top N locations reliably is difficult) or the space domain (meaning the data granularity is strictly higher than the cell level). In both cases, the utility of the anonymized location data will be decreased, potentially by a significant amount.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Computing and Networking},
pages = {145–156},
numpages = {12},
keywords = {k-anonymity, privacy, cellular data, location},
location = {Las Vegas, Nevada, USA},
series = {MobiCom '11}
}

@inproceedings{10.1145/2030613.2030623,
author = {Rachuri, Kiran K. and Mascolo, Cecilia and Musolesi, Mirco and Rentfrow, Peter J.},
title = {SociableSense: Exploring the Trade-Offs of Adaptive Sampling and Computation Offloading for Social Sensing},
year = {2011},
isbn = {9781450304924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030613.2030623},
doi = {10.1145/2030613.2030623},
abstract = {The interactions and social relations among users in workplaces have been studied by many generations of social psychologists. There is evidence that groups of users that interact more in workplaces are more productive. However, it is still hard for social scientists to capture fine-grained data about phenomena of this kind and to find the right means to facilitate interaction. It is also difficult for users to keep track of their level of sociability with colleagues. While mobile phones offer a fantastic platform for harvesting long term and fine grained data, they also pose challenges: battery power is limited and needs to be traded-off for sensor reading accuracy and data transmission, while energy costs in processing computationally intensive tasks are high.In this paper, we propose SociableSense, a smart phones based platform that captures user behavior in office environments, while providing the users with a quantitative measure of their sociability and that of colleagues. We tackle the technical challenges of building such a tool: the system provides an adaptive sampling mechanism as well as models to decide whether to perform computation of tasks, such as the execution of classification and inference algorithms, locally or remotely. We perform several micro-benchmark tests to fine-tune and evaluate the performance of these mechanisms and we show that the adaptive sampling and computation distribution schemes balance trade-offs among accuracy, energy, latency, and data traffic. Finally, by means of a social psychological study with ten participants for two working weeks, we demonstrate that SociableSense fosters interactions among the participants and helps in enhancing their sociability.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Computing and Networking},
pages = {73–84},
numpages = {12},
keywords = {computation offloading, energy-accuracy trade-offs, social interactions, mobile phone sensing, social psychology.},
location = {Las Vegas, Nevada, USA},
series = {MobiCom '11}
}

@inproceedings{10.1145/2030613.2030645,
author = {Aditya, Siripuram and Katti, Sachin},
title = {FlexCast: Graceful Wireless Video Streaming},
year = {2011},
isbn = {9781450304924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030613.2030645},
doi = {10.1145/2030613.2030645},
abstract = {Video streaming performance on wireless networks is choppy. The culprit is the unpredictable wireless medium, whose fluctuations results in fluctuating throughput and bit errors. Current video codecs are not equipped to handle such variations since they exhibit an all or nothing behavior. If the channel is strong and above a threshold, the video stream gets decoded perfectly. If not, typically nothing gets decoded. Thus, there is no graceful degradation with wireless conditions.In this paper, we present a new technique FlexCast, that delivers a video reconstruction whose quality automatically varies with the channel conditions. The key idea is a novel joint-source channel code, that allows the sender to continuously transmit video bits, and the receiver to decode a video quality corresponding to the number of bits it receives and the instantaneous wireless channel quality. We show via experimental evaluation that name performs almost as well as the optimal scheme, and outperforms the state of the art graceful video delivery systems by nearly 6dB PSNR.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Computing and Networking},
pages = {277–288},
numpages = {12},
keywords = {coding, video, wireless},
location = {Las Vegas, Nevada, USA},
series = {MobiCom '11}
}

@inproceedings{10.1145/2030686.2030694,
author = {Rachuri, Kiran K. and Mascolo, Cecilia},
title = {Smart Phone Based Systems for Social Psychological Research: Challenges and Design Guidelines},
year = {2011},
isbn = {9781450308687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030686.2030694},
doi = {10.1145/2030686.2030694},
abstract = {Social psychology research deals with understanding many aspects of human behavior, and this helps not only to gain insights into this complex phenomenon but also to provide useful feedback to the participants. Social psychological research is mainly conducted through self-reports and surveys, however, this methodology is laborious and requires considerable offline analysis. Moreover, self-reports are also found to be biased towards pleasant experiences. Mobile phones represent a perfect platform for conducting social psychological research as they are ubiquitous, unobtrusive, and sensor-rich devices. However, limited battery and computing power, and expensive data plans make it difficult to support various demanding sensing and computation requirements of the social psychological research. In this paper, we describe the specific challenges in building systems based on off-the-shelf mobile phones for conducting social experiments, and provide design guidelines based on our recent works for implementing such systems.},
booktitle = {Proceedings of the 3rd ACM Workshop on Wireless of the Students, by the Students, for the Students},
pages = {21–24},
numpages = {4},
keywords = {mobile phone sensing, social psychology, system design},
location = {Las Vegas, Nevada, USA},
series = {S3 '11}
}

@inproceedings{10.1145/2030613.2030633,
author = {Gunawardena, Dinan and Karagiannis, Thomas and Proutiere, Alexandre and Santos-Neto, Elizeu and Vojnovic, Milan},
title = {Scoop: Decentralized and Opportunistic Multicasting of Information Streams},
year = {2011},
isbn = {9781450304924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030613.2030633},
doi = {10.1145/2030613.2030633},
abstract = {We consider the problem of delivering information streams to interested mobile users, leveraging both access to the infrastructure and device-to-device data transfers. The goal is to design practical relaying algorithms that aim at optimizing a global system objective that accounts for two important aspects: first, the user interest in content with respect to its type and delivery time; and, second, resource constraints such as storage and transmission costs. We first examine a set of real-world datasets reporting contacts between users moving in relatively restricted geographic areas (e.g. a city). These datasets provide evidence that significant performance gains can be achieved by extending the information dissemination from one to two hops, and that using longer paths only brings marginal benefits. We also show that correlation of delays through different paths is typically significant, thus asking for system design that would allow for general user mobility.We then propose a class of relaying strategies (referred to as SCOOP) that aim at optimizing a global system objective, are fully decentralized, require only locally observable states by individual devices, and allow for general user mobility. These properties characterize a practical scheme whose efficiency is evaluated using real-world mobility traces.},
booktitle = {Proceedings of the 17th Annual International Conference on Mobile Computing and Networking},
pages = {169–180},
numpages = {12},
keywords = {DTNs, two-hop relaying, decentralized opportunistic relaying},
location = {Las Vegas, Nevada, USA},
series = {MobiCom '11}
}

@inproceedings{10.1145/2037509.2037531,
author = {Burkitt, M. and Walker, D. and Romano, D. M. and Fazeli, A.},
title = {Modelling Sperm Behaviour in a 3D Environment},
year = {2011},
isbn = {9781450308175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037509.2037531},
doi = {10.1145/2037509.2037531},
abstract = {The processes used by mammalian sperm to find the egg in the female reproductive tract are highly complex and poorly understood. Due to ethical and practical limitations, observing and measuring the behaviour of sperm within a live animal is difficult if not impossible without influencing their behaviour. One way to help understand the processes involved is through the use of computational modelling. The methodology used to construct the first agent based model of sperm movement within a 3D model of the mammalian oviduct is presented. The different processes represented within the model and the implementation of those processes is described. The simulation runtime is significantly reduced using collision detection optimisation and by harnessing the parallel processing power of the GPU to make concurrent simulation replicates. Validation of the model and the potential uses of the model once fully validated are described.},
booktitle = {Proceedings of the 9th International Conference on Computational Methods in Systems Biology},
pages = {141–149},
numpages = {9},
location = {Paris, France},
series = {CMSB '11}
}

@article{10.1145/2038056.2038058,
author = {Basu, Vedabrata and Lederer, Al},
title = {Agency Theory and Consultant Management in Enterprise Resource Planning Systems Implementation},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/2038056.2038058},
doi = {10.1145/2038056.2038058},
abstract = {Enterprise resource planning (ERP) systems are rapidly becoming the foundations to the decision support systems of many organizations, but have all too often failed to deliver their expected benefits. Most ERP implementation projects are carried out using implementation consultants hired from outside the client firm. The current study treated the consultants as agents and the clients as principals, and applied agency theory to test the consultant-client relationship as a predictor of ERP implementation project success.A survey collected responses from 192 client project managers who worked on the ERP implementation process in their organizations. Data analysis via structural equation modeling showed that more consultant monitoring predicts less moral hazard (i.e., consultant shirking), that less moral hazard predicts greater ERP project success, and that more monitoring directly predicts such success. However, the analysis failed to support expectations that more pre-qualification efforts (i.e., screening of the consultant) would lead to less adverse selection (i.e., the consultant's misrepresentation of skills) or that less adverse selection would lead to greater ERP success. Surprisingly greater incentive alignment predicted greater moral hazard, thus suggesting the potential of incentives to de-motivate rather than motivate.This study contributed by extending agency theory to outsourced information systems project implementation. It provided new, validated measures of prequalification efforts, monitoring, incentive alignment, moral hazard, and adverse selection constructs. The findings suggest that future researchers may want to learn more about incentive alignment and its impact, and that information systems project managers may want to monitor more to improve ERP and other large-scale system implementation success.},
journal = {SIGMIS Database},
month = sep,
pages = {10–33},
numpages = {24},
keywords = {erp}
}

@inproceedings{10.1145/2181037.2181049,
author = {Taylor, Anna-Sofia Alklind and Backlund, Per},
title = {Letting the Students Create and the Teacher Play: Expanding the Roles in Serious Gaming},
year = {2011},
isbn = {9781450308168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2181037.2181049},
doi = {10.1145/2181037.2181049},
abstract = {In this paper we describe four player roles in game-based learning and training, namely student player, student author, teacher player and teacher author. By this, we want to emphasise the creative and collaborative nature of gameplay, such as in-game feedback, scenario creation and "puckstering", that put students and teachers, not on opposite sides of a spectrum, but as members of a community of practice with varying degrees of expertise. Using these four player roles as a basis for analysis, we have observed training sessions for vocational education within military and rescue contexts. The result is multifaceted insights into both the strengths and draw-backs of incorporating these roles into a serious game.},
booktitle = {Proceedings of the 15th International Academic MindTrek Conference: Envisioning Future Media Environments},
pages = {63–70},
numpages = {8},
keywords = {game-based learning, player roles, game-based training, serious games, player-centred design},
location = {Tampere, Finland},
series = {MindTrek '11}
}

@inproceedings{10.1145/2047456.2047462,
author = {Mahmood, Rand A. and Al-Hamdani, Wasim A.},
title = {Is RFID Technology Secure and Private?},
year = {2011},
isbn = {9781450308120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047456.2047462},
doi = {10.1145/2047456.2047462},
abstract = {Radio Frequency Identification (RFID) has been used in a variety of applications, such as inventory management, anti-theft monitoring of consumer merchandise, and the tagging of livestock. With previous applications, it is difficult to link information stored on an RFID transponder to a specific individual. New applications for RFID technology include embedding transponders in everyday things used by individuals, such as library books, payment cards, and personal identification cards and documents. While RFID technology has existed for decades these new applications carry with them substantial new privacy and security risks for individuals. In this paper I study the risks and security issues of RFID, such as the targeting or tracking of individuals, or the potential disclosure of personal practices or preferences to unauthorized third parties, and how it could be attacked at any part of the RFID system (between RFID tag and reader attacks, middleware attacks and Backend station attacks). Despite the increasing popularity of RFID technology, the electronic information it deals with may not be as secure as was once thought.},
booktitle = {Proceedings of the 2011 Information Security Curriculum Development Conference},
pages = {42–49},
numpages = {8},
keywords = {RFID security, middleware attacks, backend attacks, nSpeedPass},
location = {Kennesaw, Georgia},
series = {InfoSecCD '11}
}

@article{10.1145/2027066.2027069,
author = {Zhang, Xuechen and Xu, Yuehai and Jiang, Song},
title = {YouChoose: Choosing Your Storage Device as a Performance Interface to Consolidated I/O Service},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/2027066.2027069},
doi = {10.1145/2027066.2027069},
abstract = {Currently the QoS requirements for storage systems are usually presented in the form of service-level agreement (SLA) to bound I/O measures such as latency and throughput of I/O requests. However, SLA is not an effective performance interface for users to specify their required I/O service quality for two major reasons. First, for users it is difficult to determine appropriate latency and throughput bounds to ensure their required application performance without resource over-provisioning. Second, for storage system administrators it is a challenge to estimate a user’s real resource demand because the specified SLA measures are not consistently correlated with the user’s resource demand. This makes resource provisioning and scheduling less informative and can greatly reduce system efficiency.We propose the concept of reference storage system (RSS), which can be a storage system chosen by users and whose performance can be measured offline and mimicked online, as a performance interface between applications and storage servers. By designating an RSS to represent I/O performance requirement, a user can expect the performance received from a shared storage server servicing his I/O workload is not worse than the performance received from the RSS servicing the same workload. The storage system is responsible for implementing the RSS interface. The key enabling techniques are a machine learning model that derives request-specific performance requirements and an RSS-centric scheduling that efficiently allocates resource among requests from different users. The proposed scheme, named as YouChoose, supports the user-chosen performance interface through efficiently implementing and migrating virtual storage devices in a host storage system. Our evaluation based on trace-driven simulations shows that YouChoose can precisely implement the RSS performance interface, achieve a strong performance assurance and isolation, and improve the efficiency of a consolidated storage system consisting of different types of storage devices.},
journal = {ACM Trans. Storage},
month = oct,
articleno = {9},
numpages = {18},
keywords = {model, quality of service, machine learning, performance interface, I/O QoS}
}

@article{10.1145/2036264.2036274,
author = {Cioffi-Revilla, Claudio and Rogers, J. Daniel and Hailegiorgis, Atesmachew},
title = {Geographic Information Systems and Spatial Agent-Based Model Simulations for Sustainable Development},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036274},
doi = {10.1145/2036264.2036274},
abstract = {In recent years the interdisciplinary field of Computational Social Science has developed theory and methodologies for building spatial Agent-Based Social Simulation (ABSS) models of human societies that are situated in ecosystems with land cover and climate. This article explains the needs and demand for Geographic Information Systems (GIS) in these types of agent-based models, with an emphasis on models applied to Eastern Africa and Inner Asia and relevance for understanding and analyzing development issues. The models are implemented with the MASON (Multi-Agent Simulator Of Networks and Neighborhoods) system, an open-source simulation environment in the Java language and suitable for developing ABSS models with GIS for representing spatial features.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {10},
numpages = {11},
keywords = {Geographic Information Systems (GIS), computational social science, Spatial Agent-based Modeling (ABM), Multi-agent Systems (MAS), Inner Asia, Eastern Africa, Multi-agent Simulator of Networks and Neighborhoods (MASON)}
}

@article{10.1145/2001269.2001282,
author = {Neville-Neil, George V.},
title = {File-System Litter},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2001269.2001282},
doi = {10.1145/2001269.2001282},
abstract = {Cleaning up your storage space quickly and efficiently.},
journal = {Commun. ACM},
month = oct,
pages = {25–26},
numpages = {2}
}

@inproceedings{10.1145/2038476.2038481,
author = {Albers, Michael J.},
title = {Tapping as a Measure of Cognitive Load and Website Usability},
year = {2011},
isbn = {9781450309363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038476.2038481},
doi = {10.1145/2038476.2038481},
abstract = {This article examines how cognitive load theory applies to website design and then considers using a tapping test as a practical method of measuring cognitive load. By identifying the design elements causing cognitive overload, the designer can potentially redesign the website to reduce it. The results of a pilot tapping test study are discussed which respect to its ability to determine if it can identify points of cognitive overload in a design. The results showed that the tapping test was an acceptable method, which could be easily integrated into current usability testing procedures.},
booktitle = {Proceedings of the 29th ACM International Conference on Design of Communication},
pages = {25–32},
numpages = {8},
keywords = {usability data analysis, usability method, empirical findings},
location = {Pisa, Italy},
series = {SIGDOC '11}
}

@inproceedings{10.1145/2072221.2072225,
author = {Brown, Nancy and Brown, Irwin},
title = {Contextual Factors Influencing Strategic Information Systems Plan Implementation},
year = {2011},
isbn = {9781450308786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072221.2072225},
doi = {10.1145/2072221.2072225},
abstract = {Research in the field of strategic information systems planning (SISP) has primarily focused on information systems (IS) strategy formulation. Despite the great effort put into understanding and executing SISP, organisations still grapple with effectively translating strategic information systems plans into effective implementations. Effective IS strategy plan implementation is fundamental to the survival, profitability and shaping the future of an organisation. Contextual conditions are acknowledged as having a major bearing on the success of IS implementations in organizations. Since limited attention has been given to this situation of concern with respect to SISP, this study aimed to understand how contextual factors influence strategic IS plan implementation. A triangulated research design of an exploratory nature was employed, using qualitative data collected from 11 professionals across 6 organisations. Within each organisation, data was collected through semi-structured interviews from a business person and an IS person involved in strategic IS plan implementation. Company strategic IS plans were also studied. The results of the data analysis enrich understanding of how factors related to the SISP practice context, the IS function context, the stakeholder and management context, the organisational context and the environmental context shape strategic IS plan implementation.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists Conference on Knowledge, Innovation and Leadership in a Diverse, Multidisciplinary Environment},
pages = {21–30},
numpages = {10},
keywords = {SISP, contextual factors, IS plan implementation},
location = {Cape Town, South Africa},
series = {SAICSIT '11}
}

@inproceedings{10.1145/2038558.2038586,
author = {Musicant, David R. and Ren, Yuqing and Johnson, James A. and Riedl, John},
title = {Mentoring in Wikipedia: A Clash of Cultures},
year = {2011},
isbn = {9781450309097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038558.2038586},
doi = {10.1145/2038558.2038586},
abstract = {The continuous success of Wikipedia depends upon its capability to recruit and engage new editors, especially those with new knowledge and perspectives. Yet Wikipedia over the years has become a complicated bureaucracy that may be difficult for newcomers to navigate. Mentoring is a practice that has been widely used in offline organizations to help new members adjust to their roles. In this paper, we draw insights from the offline mentoring literature to analyze mentoring practices in Wikipedia and how they influence editor behaviors. Our quantitative analysis of the Adopt-a-user program shows mixed success of the program. Communication between adopters and adoptees is correlated with the amount of article editing done by adoptees shortly after adoption. Our qualitative analysis of the communication between adopters and adoptees suggests that several key functions of mentoring are missing or not fulfilled consistently. Most adopters focus on establishing their legitimacy rather than acting proactively to guide, protect, and support the long-term growth of adoptees. We conclude with recommendations of how Wikipedia mentoring programs can evolve to take advantage of offline best practices.},
booktitle = {Proceedings of the 7th International Symposium on Wikis and Open Collaboration},
pages = {173–182},
numpages = {10},
keywords = {newcomer retention, mentoring, Wikipedia},
location = {Mountain View, California},
series = {WikiSym '11}
}

@inproceedings{10.1145/2072221.2072245,
author = {Shih, Ray and Blignaut, Pieter},
title = {The Influence of Gender and Internet Experience on the Acceptability of Smell as Interaction Modality},
year = {2011},
isbn = {9781450308786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072221.2072245},
doi = {10.1145/2072221.2072245},
abstract = {Smell simulation technology allows humans to use their sense of smell to enhance or accelerate their interaction with computers. The technology can be categorised as being either perceiving or generating odours. In this paper, the major capabilities and application areas of both these categories are highlighted. Typical application areas include the food industry, multimedia, marketing, chemistry, art, medical and health sciences, etc.Since its first usage in the 1950s, the lack of acceptance and instability of the technology motivated scientists to try to improve the technology and adapt it for human needs. In this study, questionnaires were used to determine whether computer users are ready to accept smell generation as a means of interaction during online shopping. The current social acceptability of the technology as well as the factors which might deter end-user's willingness to use the technology was investigated. It was found that the interest of people in the use of this technology as interaction modality is, to some extent, affected by gender as well as by the amount of Internet experience and that technical difficulties such as cost, potential health risks and lack of accuracy stand in the way of general acceptability.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists Conference on Knowledge, Innovation and Leadership in a Diverse, Multidisciplinary Environment},
pages = {205–214},
numpages = {10},
keywords = {olfactory related technology, smell perception and generation},
location = {Cape Town, South Africa},
series = {SAICSIT '11}
}

@inproceedings{10.1145/2072221.2072266,
author = {Meissner, Fritz and Blake, Edwin},
title = {Understanding Culturally Distant End-Users through Intermediary-Derived Personas},
year = {2011},
isbn = {9781450308786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072221.2072266},
doi = {10.1145/2072221.2072266},
abstract = {In this paper we present the use of a persona creation process for gathering information from intermediaries for information and communication technology for development (ICT4D) projects. Our approach represents a departure from traditional persona use in that it does not organise pre-existing data about potential users, but is itself a data gathering process. We present a case study of our use of the process on an ICT4D project, during which time we observed the combined benefits of using personas and of working with a non-governmental organisation (NGO) intermediary materialise. We attribute these outcomes to our persona creation workshops. Information which the NGO furnished regarding sensitive personal circumstances typical of our end users greatly improved our understanding of our users. This information would likely not have come to light had we interviewed the users ourselves, which we believe demonstrates the validity of relying on intermediary information for persona creation rather than first-hand information.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists Conference on Knowledge, Innovation and Leadership in a Diverse, Multidisciplinary Environment},
pages = {314–317},
numpages = {4},
keywords = {design, cultural distance, intermediaries, ICT4D, NGOs, personas},
location = {Cape Town, South Africa},
series = {SAICSIT '11}
}

@inproceedings{10.1145/2038558.2038566,
author = {McCully, Wyl and Lampe, Cliff and Sarkar, Chandan and Velasquez, Alcides and Sreevinasan, Akshaya},
title = {Online and Offline Interactions in Online Communities},
year = {2011},
isbn = {9781450309097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038558.2038566},
doi = {10.1145/2038558.2038566},
abstract = {Online communities, while primarily enacted through technology-mediated environments, can also include offline meetings between members, promoting interactivity and community building. This study explores the offline interactions of online community members and its subsequent impact on online participation. We argue that offline interactions have a counterintuitive impact on online participation. Although these offline interactions strengthen relationships, these relationships undermine the community's sustainability in terms of site participation. Participation has been defined as contribution of content to the online community. A multi-method analysis technique using content analysis, qualitative interviews, and server level quantitative data of users in Everything2.com supports our claim.},
booktitle = {Proceedings of the 7th International Symposium on Wikis and Open Collaboration},
pages = {39–48},
numpages = {10},
keywords = {offline interaction, Everything2, online contribution, online communities},
location = {Mountain View, California},
series = {WikiSym '11}
}

@inproceedings{10.5555/2206329.2206344,
author = {Xu, Feiyu and Li, Hong and Zhang, Yi and Uszkoreit, Hans and Krause, Sebastian},
title = {Minimally Supervised Domain-Adaptive Parse Reranking for Relation Extraction},
year = {2011},
isbn = {9781932432046},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted head-driven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired reranking model improves the performance of RE in both training and test phases with the new first parses. The obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for task-specific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task.},
booktitle = {Proceedings of the 12th International Conference on Parsing Technologies},
pages = {118–128},
numpages = {11},
location = {Dublin, Ireland},
series = {IWPT '11}
}

@inproceedings{10.5555/2157848.2157855,
author = {Suznjevic, Mirko and Stupar, Ivana and Matijasevic, Maja},
title = {MMORPG Player Behavior Model Based on Player Action Categories},
year = {2011},
isbn = {9781457719349},
publisher = {IEEE Press},
abstract = {In this paper we present the modeling of player behavior for Massively Multiplayer Online Role-Playing Games (MMORPGs). We have performed action specific measurements of player sessions in terms of the previously defined action categories for MMORPGs (Trading, Questing, Dungeons, Raiding, and Player versus Player Combat). We explore the hourly trends in user behavior and form models based on observed patterns. We explore the session duration as well as lengths and probability of session segments (i.e., parts of the session consisting of only one category of player actions). Our aim is to create the user behavior model and to combine it with previously established traffic models in order to be able to explain, predict, and generate network traffic of MMORPGs more accurately. As a case study we use World of Warcraft.},
booktitle = {Proceedings of the 10th Annual Workshop on Network and Systems Support for Games},
articleno = {6},
numpages = {6},
location = {Ottawa, Canada},
series = {NetGames '11}
}

@inproceedings{10.1145/2072274.2072278,
author = {Fehnker, Ansgar and Huuck, Ralf and R\"{o}diger, Wolf},
title = {Model Checking Dataflow for Malicious Input},
year = {2011},
isbn = {9781450308199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072274.2072278},
doi = {10.1145/2072274.2072278},
abstract = {Many embedded systems today are no longer isolated control units, but are fully fledged miniature desktops with their own kernel and sometimes operating system networked with the outside world. This opens up a whole new set of security issues previously not known to embedded systems. One example is potentially malicious input that exploits source code weaknesses leading to critical mission failures. In this paper we propose a new automated malicious input detection approach that works on a staged application of traditional tainted dataflow analysis and syntactic software model checking. The advantages of this approach are that tainted data can be tracked from its source to its application point, a precise path through the source code can be computed, speed and precision can be custom-tuned by automated refinement, and the approach is flexible to deal with real-life security threats. We illustrate our approach with a number of analysis examples taken from existing open source C/C++ projects.},
booktitle = {Proceedings of the Workshop on Embedded Systems Security},
articleno = {4},
numpages = {10},
keywords = {command injection, static analysis, model checking, security},
location = {Taipei, Taiwan},
series = {WESS '11}
}

@inproceedings{10.1145/2377978.2377981,
author = {Stuart, Jeff A. and Balaji, Pavan and Owens, John D.},
title = {Extending MPI to Accelerators},
year = {2011},
isbn = {9781450314398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377978.2377981},
doi = {10.1145/2377978.2377981},
abstract = {Current trends in computing and system architecture point towards a need for accelerators such as GPUs to have inherent communication capabilities. We review previous and current software libraries that provide pseudo-communication abilities through direct message passing. We show how these libraries are beneficial to the HPC community, but are not forward-thinking enough. We give motivation as to why MPI should be extended to support these accelerators, and provide a road map of achievable milestones to complete such an extension, some of which require advances in hardware and device drivers.},
booktitle = {Proceedings of the 1st Workshop on Architectures and Systems for Big Data},
pages = {19–23},
numpages = {5},
location = {Galveston Island, Texas, USA},
series = {ASBD '11}
}

@inproceedings{10.1145/2077546.2077548,
author = {Bankole, Azziza and Anderson, Martha and Knight, Aubrey and Oh, Kyunghui and Smith-Jackson, Tonya and Hanson, Mark A. and Barth, Adam T. and Lach, John},
title = {Continuous, Non-Invasive Assessment of Agitation in Dementia Using Inertial Body Sensors},
year = {2011},
isbn = {9781450309820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2077546.2077548},
doi = {10.1145/2077546.2077548},
abstract = {Agitated behavior is one of the most frequent reasons that patients with dementia are placed in long-term care settings. These behaviors are indicators of distress and are associated with increased risk of injury to the patients and their caregivers. This study aims to explore the ability of a custom inertial wireless body sensor network (BSN) to objectively detect and quantify agitation, validating against currently accepted subjective clinical measures -- the Cohen-Mansfield Agitation Inventory (CMAI) and the Aggressive Behavior Scale (ABS) -- within the nursing home setting. The ultimate goal is to enable continuous, real-time monitoring of physical agitation in any location over an extended period. Continuous, longitudinal assessment facilitates timely response to agitation events in order to minimize patient distress and risk for injury, to more appropriately titrate pharmacotherapy, and to enable staff (or caregivers) to successfully intervene.Six patients identified as being at high risk for agitated behaviors were enrolled in this pilot study. Patients underwent a series of the above validated tests of memory and agitation. The BSN nodes were applied at three sites on body for three hours while behaviors were annotated simultaneously. This process was subsequently repeated twice for each enrolled subject. The BSN data was then processed using Teager energy analysis, which an earlier study suggested was a promising method for extracting jerky and repetitive movements from inertial data. Results based on construct validity testing for agitation (CMAI) and aggression (ABS) were promising and suggest that additional study with larger sample sizes is warranted.},
booktitle = {Proceedings of the 2nd Conference on Wireless Health},
articleno = {1},
numpages = {9},
keywords = {dementia patients, remote monitoring, agitation assessment, inertial body sensor network (BSN)},
location = {San Diego, California},
series = {WH '11}
}

@inproceedings{10.1145/2077546.2077557,
author = {Rahman, Md. Mahbubur and Ali, Amin Ahsan and Plarre, Kurt and al'Absi, Mustafa and Ertin, Emre and Kumar, Santosh},
title = {MConverse: Inferring Conversation Episodes from Respiratory Measurements Collected in the Field},
year = {2011},
isbn = {9781450309820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2077546.2077557},
doi = {10.1145/2077546.2077557},
abstract = {Automated detection of social interactions in the natural environment has resulted in promising advances in organizational behavior, consumer behavior, and behavioral health. Progress, however, has been limited since the primary means of assessing social interactions today (i.e., audio recording) has several issues in field usage such as microphone occlusion, lack of speaker specificity, and high energy drain, in addition to significant privacy concerns.In this paper, we present mConverse, a new mobile-based system to infer conversation episodes from respiration measurements collected in the field from an unobtrusively wearable respiratory inductive plethysmograph (RIP) band worn around the user's chest. The measurements are wire-lessly transmitted to a mobile phone, where they are used in a novel machine learning model to determine whether the wearer is speaking, listening, or quiet. Our model incorporates several innovations to address issues that naturally arise in the noisy field environment such as confounding events, poor data quality due to sensor loosening and detachment, losses in the wireless channel, etc. Our basic model obtains 83% accuracy for the three class classification. We formulate a Hidden Markov Model to further improve the accuracy to 87%. Finally, we apply our model to data collected from 22 subjects who wore the sensor for 2 full days in the field to observe conversation behavior in daily life and find that people spend 25% of their day in conversations.},
booktitle = {Proceedings of the 2nd Conference on Wireless Health},
articleno = {10},
numpages = {10},
keywords = {conversation, respiration, wearable sensors},
location = {San Diego, California},
series = {WH '11}
}

@inproceedings{10.1145/2179298.2179331,
author = {Polverini, Becker and Pottenger, William},
title = {Using Clustering to Detect Chinese Censorware},
year = {2011},
isbn = {9781450309455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2179298.2179331},
doi = {10.1145/2179298.2179331},
booktitle = {Proceedings of the Seventh Annual Workshop on Cyber Security and Information Intelligence Research},
articleno = {30},
numpages = {1},
location = {Oak Ridge, Tennessee, USA},
series = {CSIIRW '11}
}

@inproceedings{10.1145/2179298.2179360,
author = {Azab, Mohamed and Eltoweissy, Mohamed},
title = {Towards a Cooperative Autonomous Resilient Defense Platform for Cyber-Physical Systems},
year = {2011},
isbn = {9781450309455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2179298.2179360},
doi = {10.1145/2179298.2179360},
booktitle = {Proceedings of the Seventh Annual Workshop on Cyber Security and Information Intelligence Research},
articleno = {56},
numpages = {1},
location = {Oak Ridge, Tennessee, USA},
series = {CSIIRW '11}
}

@inproceedings{10.1145/2047196.2047206,
author = {Hangal, Sudheendra and Lam, Monica S. and Heer, Jeffrey},
title = {MUSE: Reviving Memories Using Email Archives},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047206},
doi = {10.1145/2047196.2047206},
abstract = {Email archives silently record our actions and thoughts over the years, forming a passively acquired and detailed life-log that contains rich material for reminiscing on our lives. However, exploratory browsing of archives containing thousands of messages is tedious without effective ways to guide the user towards interesting events and messages. We present Muse (Memories USing Email), a system that combines data mining techniques and an interactive interface to help users browse a long-term email archive. Muse analyzes the contents of the archive and generates a set of cues that help to spark users' memories: communication activity with inferred social groups, a summary of recurring named entities, occurrence of sentimental words, and image attachments. These cues serve as salient entry points into a browsing interface that enables faceted navigation and rapid skimming of email messages. In our user studies, we found that users generally enjoyed browsing their archives with Muse, and extracted a range of benefits, from summarizing work progress to renewing friendships and making serendipitous discoveries.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–84},
numpages = {10},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047262,
author = {Chigira, Hiroshi and Maeda, Atsuhiko and Kobayashi, Minoru},
title = {Area-Based Photo-Plethysmographic Sensing Method for the Surfaces of Handheld Devices},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047262},
doi = {10.1145/2047196.2047262},
abstract = {Capturing the user's vital signs is an urgent goal in the HCI community. Photo-plethysmography (PPG) is one approach; it can collect data from the finger tips that indicate the user's autonomic nervous system (ANS) and offers new potentials such as mental stress measurement and drowsy state detection. Our goal is to set PPG sensors on the surfaces of ordinary devices such as mice, smartphones, and steering wheels. This will offer smart monitoring without the burden of additional wearable sensors. Unfortunately, current PPG sensors are very narrow, and even if the sensor is attached to the surface of a device, the user is forced to align and hold the finger to the sensor point, which degrades device usability. To solve this problem, we propose an area-based sensing method that relaxes the alignment requirement. The proposed method uses two thin acrylic plates, a diffuser plate and a detection plate, as an IR waveguide. The proposed method can yield very thin sensing surfaces and gentle curvatures are possible. An experiment compares the proposed method to the conventional point-sensor in terms of LF/HF discrimination performance with the participant in the resting state, and the proposed method is shown to offer comparable sensing performance with superior usability.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {499–508},
numpages = {10},
keywords = {wave guide, lf/hf, light guide, drowsiness, autonomic nervous system, mental stress, vital sensing, photo-plethysmography, heart rate variability},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2046707.2046720,
author = {McLaughlin, Stephen and McDaniel, Patrick and Aiello, William},
title = {Protecting Consumer Privacy from Electric Load Monitoring},
year = {2011},
isbn = {9781450309486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046707.2046720},
doi = {10.1145/2046707.2046720},
abstract = {The smart grid introduces concerns for the loss of consumer privacy; recently deployed smart meters retain and distribute highly accurate profiles of home energy use. These profiles can be mined by Non Intrusive Load Monitors (NILMs) to expose much of the human activity within the served site. This paper introduces a new class of algorithms and systems, called Non Intrusive Load Leveling (NILL) to combat potential invasions of privacy. NILL uses an in-residence battery to mask variance in load on the grid, thus eliminating exposure of the appliance-driven information used to compromise consumer privacy. We use real residential energy use profiles to drive four simulated deployments of NILL. The simulations show that NILL exposes only 1.1 to 5.9 useful energy events per day hidden amongst hundreds or thousands of similar battery-suppressed events. Thus, the energy profiles exhibited by NILL are largely useless for current NILM algorithms. Surprisingly, such privacy gains can be achieved using battery systems whose storage capacity is far lower than the residence's aggregate load average. We conclude by discussing how the costs of NILL can be offset by energy savings under tiered energy schedules.},
booktitle = {Proceedings of the 18th ACM Conference on Computer and Communications Security},
pages = {87–98},
numpages = {12},
keywords = {load monitor, privacy, smart meter},
location = {Chicago, Illinois, USA},
series = {CCS '11}
}

@inproceedings{10.1145/2046707.2046786,
author = {Kerschbaum, Florian},
title = {Automatically Optimizing Secure Computation},
year = {2011},
isbn = {9781450309486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046707.2046786},
doi = {10.1145/2046707.2046786},
abstract = {On the one hand, compilers for secure computation protocols, such as FairPlay or FairPlayMP, have significantly simplified the development of such protocols. On the other hand, optimized protocols with high performance for special problems demand manual development and security verification. The question considered in this paper is: Can we construct a compiler that produces optimized protocols? We present an optimization technique based on logic inference about what is known from input and output. Using the example of median computation we can show that our program analysis and rewriting technique translates a FairPlay program into an equivalent -- in functionality and security -- program that corresponds to the protocol by Aggarwal et al. Nevertheless our technique is general and can be applied to optimize a wide variety of secure computation protocols.},
booktitle = {Proceedings of the 18th ACM Conference on Computer and Communications Security},
pages = {703–714},
numpages = {12},
keywords = {optimization, secure two-party computation, programming},
location = {Chicago, Illinois, USA},
series = {CCS '11}
}

@article{10.1145/1978802.1978815,
author = {Sadri, Fariba},
title = {Ambient Intelligence: A Survey},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/1978802.1978815},
doi = {10.1145/1978802.1978815},
abstract = {In this article we survey ambient intelligence (AmI), including its applications, some of the technologies it uses, and its social and ethical implications. The applications include AmI at home, care of the elderly, healthcare, commerce, and business, recommender systems, museums and tourist scenarios, and group decision making. Among technologies, we focus on ambient data management and artificial intelligence; for example planning, learning, event-condition-action rules, temporal reasoning, and agent-oriented technologies. The survey is not intended to be exhaustive, but to convey a broad range of applications, technologies, and technical, social, and ethical challenges.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {36},
numpages = {66},
keywords = {Ambient intelligence, multiagent systems, assisted living, agents, social and ethical issues}
}

@article{10.1145/1978802.1978806,
author = {Damas, Sergio and Cord\'{o}n, Oscar and Ib\'{a}\~{n}ez, Oscar and Santamar\'{\i}a, Jose and Alem\'{a}n, Inmaculada and Botella, Miguel and Navarro, Fernando},
title = {Forensic Identification by Computer-Aided Craniofacial Superimposition: A Survey},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/1978802.1978806},
doi = {10.1145/1978802.1978806},
abstract = {Craniofacial superimposition is a forensic process in which a photograph of a missing person is compared with a skull found to determine its identity. After one century of development, craniofacial superimposition has become an interdisciplinary research field where computer sciences have acquired a key role as a complement of forensic sciences. Moreover, the availability of new digital equipment (such as computers and 3D scanners) has resulted in a significant advance in the applicability of this forensic identification technique. The purpose of this contribution is twofold. On the one hand, we aim to clearly define the different stages involved in the computer-aided craniofacial superimposition process. Besides, we aim to clarify the role played by computers in the methods considered.In order to accomplish these objectives, an up-to-date review of the recent works is presented along with a discussion of advantages and drawbacks of the existing approaches, with an emphasis on the automatic ones. Future case studies will be easily categorized by identifying which stage is tackled and which kind of computer-aided approach is chosen to face the identification problem. Remaining challenges are indicated and some directions for future research are given.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {27},
numpages = {27},
keywords = {craniofacial superimposition, Forensic identification, skull modeling, video superimposition, photographic superimposition, decision making, photographic supra-projection, skull-face superimposition}
}

@inproceedings{10.1145/2079216.2079236,
author = {Thoring, Katja and M\"{u}ller, Roland M.},
title = {Understanding the Creative Mechanisms of Design Thinking: An Evolutionary Approach},
year = {2011},
isbn = {9781450307543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2079216.2079236},
doi = {10.1145/2079216.2079236},
abstract = {In this article, we analyse the concept of design thinking with its process, the team structure, the work environment, the specific culture, and certain brainstorming rules and techniques. The goal of this work is to understand how the creative mechanisms of design thinking work and how they might be improved. For this purpose, we refer to the idea of creativity as an evolutionary process, which is determined by generation (i.e., recombination and mutation), selection, and retention of ideas. We evaluate the design thinking process in terms of its capabilities to activate these mechanisms, and we propose possible improvements. This paper contributes to a better understanding of creative design processes in general and the design thinking process in particular, and will serve as a foundation for further research about creative mechanisms.},
booktitle = {Procedings of the Second Conference on Creativity and Innovation in Design},
pages = {137–147},
numpages = {11},
keywords = {innovation methods, creative education, evolutionary creativity, design thinking},
location = {Eindhoven, Netherlands},
series = {DESIRE '11}
}

@inproceedings{10.1145/2048147.2048181,
author = {Poncin, Wouter and Serebrenik, Alexander and van den Brand, Mark},
title = {Mining Student Capstone Projects with FRASR and ProM},
year = {2011},
isbn = {9781450309424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048147.2048181},
doi = {10.1145/2048147.2048181},
abstract = {Capstone projects are commonly carried out at the end of an undergraduate program of study in software engineering or computer science. While traditionally such projects solely focussed on the software product to be developed, in more recent work importance of the development process has been stressed. Currently process quality assessment techniques are limited to review of intermediary artifacts, self- and peer evaluations. We advocate augmenting the assessment by mining software repositories used by the students during the development. We present the assessment methodology and illustrate it by applying to a number of software engineering capstone projects.},
booktitle = {Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion},
pages = {87–96},
numpages = {10},
keywords = {software engineering, mining software repositories, capstone project},
location = {Portland, Oregon, USA},
series = {OOPSLA '11}
}

@inproceedings{10.1145/2089131.2089140,
author = {Verna, Didier},
title = {Biological Realms in Computer Science},
year = {2011},
isbn = {9781450309417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2089131.2089140},
doi = {10.1145/2089131.2089140},
abstract = {In biology, evolution is usually seen as a tinkering process, different from what an engineer does when he plans the development of his systems. Recently, studies have shown that even in biology, there is a part of good engineering. As computer scientists, we have much more difficulty to admit that there is also a great deal of tinkering in what we do, and that our software systems behave more and more like biological realms every day. This essay relates my personal experience about this discovery.},
booktitle = {Proceedings of the 10th SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {167–176},
numpages = {10},
keywords = {software evolution, trans-disciplinary models, latex},
location = {Portland, Oregon, USA},
series = {Onward! 2011}
}

@inproceedings{10.1145/2048066.2048103,
author = {Tian, Kai and Zhang, Eddy and Shen, Xipeng},
title = {A Step towards Transparent Integration of Input-Consciousness into Dynamic Program Optimizations},
year = {2011},
isbn = {9781450309400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048066.2048103},
doi = {10.1145/2048066.2048103},
abstract = {Dynamic program optimizations are critical for the efficiency of applications in managed programming languages and scripting languages. Recent studies have shown that exploitation of program inputs may enhance the effectiveness of dynamic optimizations significantly. However, current solutions for enabling the exploitation require either programmers' annotations or intensive offline profiling, impairing the practical adoption of the techniques.This current work examines the basic feasibility of transparent integration of input-consciousness into dynamic program optimizations, particularly in managed execution environments. It uses transparent learning across production runs as the basic vehicle, and investigates the implications of cross-run learning on each main component of input-conscious dynamic optimizations. It proposes several techniques to address some key challenges for the transparent integration, including randomized inspection-instrumentation for cross-user data collection, a sparsity-tolerant algorithm for input characterization, and selective prediction for efficiency protection. These techniques make it possible to automatically recognize the relations between the inputs to a program and the appropriate ways to optimize it. The whole process happens transparently across production runs; no need for offline profiling or programmer intervention. Experiments on a number of Java programs demonstrate the effectiveness of the techniques in enabling input-consciousness for dynamic optimizations, revealing the feasibility and potential benefits of the new optimization paradigm in some basic settings.},
booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {445–462},
numpages = {18},
keywords = {seminal behaviors, dynamic optimizations, proactivity, program inputs, just-in-time compilation, java virtual machine, dynamic version selection},
location = {Portland, Oregon, USA},
series = {OOPSLA '11}
}

@article{10.1145/2076021.2048103,
author = {Tian, Kai and Zhang, Eddy and Shen, Xipeng},
title = {A Step towards Transparent Integration of Input-Consciousness into Dynamic Program Optimizations},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2076021.2048103},
doi = {10.1145/2076021.2048103},
abstract = {Dynamic program optimizations are critical for the efficiency of applications in managed programming languages and scripting languages. Recent studies have shown that exploitation of program inputs may enhance the effectiveness of dynamic optimizations significantly. However, current solutions for enabling the exploitation require either programmers' annotations or intensive offline profiling, impairing the practical adoption of the techniques.This current work examines the basic feasibility of transparent integration of input-consciousness into dynamic program optimizations, particularly in managed execution environments. It uses transparent learning across production runs as the basic vehicle, and investigates the implications of cross-run learning on each main component of input-conscious dynamic optimizations. It proposes several techniques to address some key challenges for the transparent integration, including randomized inspection-instrumentation for cross-user data collection, a sparsity-tolerant algorithm for input characterization, and selective prediction for efficiency protection. These techniques make it possible to automatically recognize the relations between the inputs to a program and the appropriate ways to optimize it. The whole process happens transparently across production runs; no need for offline profiling or programmer intervention. Experiments on a number of Java programs demonstrate the effectiveness of the techniques in enabling input-consciousness for dynamic optimizations, revealing the feasibility and potential benefits of the new optimization paradigm in some basic settings.},
journal = {SIGPLAN Not.},
month = oct,
pages = {445–462},
numpages = {18},
keywords = {java virtual machine, just-in-time compilation, proactivity, program inputs, dynamic optimizations, seminal behaviors, dynamic version selection}
}

@inproceedings{10.1145/2048237.2048249,
author = {Singer, Jeremy},
title = {A Literate Experimentation Manifesto},
year = {2011},
isbn = {9781450309417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048237.2048249},
doi = {10.1145/2048237.2048249},
abstract = {This paper proposes a new approach to experimental computer systems research, which we call Literate Experimentation. Conventionally, experimental procedure and writeup are divided into distinct phases: i.e. setup (the method), data collection (the results) and analysis (the evaluation of the results). Our concept of a literate experiment is to have a single, rich, human-generated, text-based description of a particular experiment, from which can be automatically derived: (1) a summary of the experimental setup to include in the paper; (2) a sequence of executable commands to setup a computer platform ready to perform the actual experiment; (3) the experiment itself, executed on this appropriately configured platform; and, (4) a means of generating results tables and graphs from the experimental output, ready for inclusion in the paper.Our Literate Experimentation style has largely been inspired by Knuth's Literate Programming philosophy. Effectively, a literate experiment is a small step towards the executable paper panacea. In this work, we argue that a literate experimentation approach makes it easier to produce rigorous experimental evaluation papers. We suggest that such papers are more likely to be accepted for publication, due to (a) the imposed uniformity of structure, and (b) the assurance that experimental results are easily reproducible. We present a case study of a prototype literate experiment involving memory management in Jikes RVM.},
booktitle = {Proceedings of the 10th SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {91–102},
numpages = {12},
keywords = {literate programming, experimental write-up},
location = {Portland, Oregon, USA},
series = {Onward! 2011}
}

@inproceedings{10.1145/2043556.2043585,
author = {Erlingsson, \'{U}lfar and Peinado, Marcus and Peter, Simon and Budiu, Mihai},
title = {Fay: Extensible Distributed Tracing from Kernels to Clusters},
year = {2011},
isbn = {9781450309776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2043556.2043585},
doi = {10.1145/2043556.2043585},
abstract = {Fay is a flexible platform for the efficient collection, processing, and analysis of software execution traces. Fay provides dynamic tracing through use of runtime instrumentation and distributed aggregation within machines and across clusters. At the lowest level, Fay can be safely extended with new tracing primitives, including even untrusted, fully-optimized machine code, and Fay can be applied to running user-mode or kernel-mode software without compromising system stability. At the highest level, Fay provides a unified, declarative means of specifying what events to trace, as well as the aggregation, processing, and analysis of those events.We have implemented the Fay tracing platform for Windows and integrated it with two powerful, expressive systems for distributed programming. Our implementation is easy to use, can be applied to unmodified production systems, and provides primitives that allow the overhead of tracing to be greatly reduced, compared to previous dynamic tracing platforms. To show the generality of Fay tracing, we reimplement, in experiments, a range of tracing strategies and several custom mechanisms from existing tracing frameworks.Fay shows that modern techniques for high-level querying and data-parallel processing of disaggregated data streams are well suited to comprehensive monitoring of software execution in distributed systems. Revisiting a lesson from the late 1960's [15], Fay also demonstrates the efficiency and extensibility benefits of using safe, statically-verified machine code as the basis for low-level execution tracing. Finally, Fay establishes that, by automatically deriving optimized query plans and code for safe extensions, the expressiveness and performance of high-level tracing queries can equal or even surpass that of specialized monitoring tools.},
booktitle = {Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles},
pages = {311–326},
numpages = {16},
location = {Cascais, Portugal},
series = {SOSP '11}
}

@inproceedings{10.1145/2043556.2043572,
author = {Yin, Zuoning and Ma, Xiao and Zheng, Jing and Zhou, Yuanyuan and Bairavasundaram, Lakshmi N. and Pasupathy, Shankar},
title = {An Empirical Study on Configuration Errors in Commercial and Open Source Systems},
year = {2011},
isbn = {9781450309776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2043556.2043572},
doi = {10.1145/2043556.2043572},
abstract = {Configuration errors (i.e., misconfigurations) are among the dominant causes of system failures. Their importance has inspired many research efforts on detecting, diagnosing, and fixing misconfigurations; such research would benefit greatly from a real-world characteristic study on misconfigurations. Unfortunately, few such studies have been conducted in the past, primarily because historical misconfigurations usually have not been recorded rigorously in databases.In this work, we undertake one of the first attempts to conduct a real-world misconfiguration characteristic study. We study a total of 546 real world misconfigurations, including 309 misconfigurations from a commercial storage system deployed at thousands of customers, and 237 from four widely used open source systems (CentOS, MySQL, Apache HTTP Server, and OpenLDAP). Some of our major findings include: (1) A majority of misconfigurations (70.0%~85.5%) are due to mistakes in setting configuration parameters; however, a significant number of misconfigurations are due to compatibility issues or component configurations (i.e., not parameter-related). (2) 38.1%~53.7% of parameter mistakes are caused by illegal parameters that clearly violate some format or rules, motivating the use of an automatic configuration checker to detect these misconfigurations. (3) A significant percentage (12.2%~29.7%) of parameter-based mistakes are due to inconsistencies between different parameter values. (4) 21.7%~57.3% of the misconfigurations involve configurations external to the examined system, some even on entirely different hosts. (5) A significant portion of misconfigurations can cause hard-to-diagnose failures, such as crashes, hangs, or severe performance degradation, indicating that systems should be better-equipped to handle misconfigurations.},
booktitle = {Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles},
pages = {159–172},
numpages = {14},
keywords = {misconfigurations, characteristic study},
location = {Cascais, Portugal},
series = {SOSP '11}
}

@inproceedings{10.5555/2147671.2147742,
author = {Bartolini, Claudio and Stefanelli, Cesare and Targa, Davide and Tortonesi, Mauro},
title = {A Web-Based What-If Scenario Analysis Tool for Performance Improvement of IT Support Organizations},
year = {2011},
isbn = {9783901882449},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {IT support organizations are in charge of restoring normal operations after IT service disruptions and can be complex systems. Thire performance assessment and optimization is an extremely challenging task that requires considering organization-specific structure, behavior, and business-level objectives. This paper presents Symian-Web, a decision support tool that enables IT managers to assess and improve IT support organization performance. Symian-Web features advanced information visualization concepts and metaphors, allowing for precise and timely assessment of IT support organization performance, and facilitating their redesign.},
booktitle = {Proceedings of the 7th International Conference on Network and Services Management},
pages = {394–398},
numpages = {5},
keywords = {IT service management, decision support tools, incident management},
location = {Paris, France},
series = {CNSM '11}
}

@inproceedings{10.1145/2063576.2063844,
author = {Malik, Hassan H. and MacGillivray, Ian and Olof-Ors, M\r{a}ns and Sun, Siming and Saroha, Shailesh},
title = {Exploring the Corporate Ecosystem with a Semi-Supervised Entity Graph},
year = {2011},
isbn = {9781450307178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063576.2063844},
doi = {10.1145/2063576.2063844},
abstract = {Investment decisions in the financial markets require careful analysis of information available from multiple data sources. In this paper, we present Atlas, a novel entity-based information analysis and content aggregation platform that uses heterogeneous data sources to construct and maintain the "ecosystem" around tangible and logical entities such as organizations, products, industries, geographies, commodities and macroeconomic indicators. Entities are represented as vertices in a directed graph, and edges are generated using entity co-occurrences in unstructured documents and supervised information from structured data sources. Significance scores for the edges are computed using a method that combines supervised, unsupervised and temporal factors into a single score. Important entity attributes from the structured content and the entity neighborhood in the graph are automatically summarized as the entity "fingerprint". A highly interactive user interface provides exploratory access to the graph and supports common business use cases. We present results of experiments performed on five years of news and broker research data, and show that Atlas is able to accurately identify important and interesting connections in real-world entities. We also demonstrate that Atlas entity fingerprints are particularly useful in entity similarity queries, with a quality that rivals existing human maintained databases.},
booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
pages = {1857–1866},
numpages = {10},
keywords = {finding similar entities, corporate entity relationships, entity significance, entity graph, entity modeling, semi-supervised learning, finding similar companies, decision support, ranking},
location = {Glasgow, Scotland, UK},
series = {CIKM '11}
}

@inproceedings{10.1145/2089155.2089170,
author = {Bari\v{s}i\'{c}, Ankica and Amaral, Vasco and Goul\~{a}o, Miguel and Barroca, Bruno},
title = {Quality in Use of Domain-Specific Languages: A Case Study},
year = {2011},
isbn = {9781450310246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2089155.2089170},
doi = {10.1145/2089155.2089170},
abstract = {Domain-Specific Languages (DSLs) are claimed to increment productivity, while reducing the required maintenance and programming expertise. In this context, DSLs usability is a key factor for its successful adoption.In this paper, we propose a systematic approach based on User Interfaces Experimental validation techniques to assess the impact of the introduction of DSLs on the productivity of domain experts. To illustrate this evaluation approach we present a case study of a DSL for High Energy Physics (HEP).The DSL on this case study, called Pheasant (PHysicist's EAsy Analysis Tool), is assessed in contrast with a pre-existing baseline, using General Purpose Languages (GPLs) such as C++. The comparison combines quantitative and qualitative data, collected with users from a real-world setting. Our assessment includes Physicists with programming experience with two profiles; ones with no experience with the previous framework used in the project and other experienced.This work's contribution highlights the problem of the absence of systematic approaches for experimental validation of DSLs. It also illustrates how an experimental approach can be used in the context of a DSL evaluation during the Software Languages Engineering activity, with respect to its impact on effectiveness and efficiency.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN Workshop on Evaluation and Usability of Programming Languages and Tools},
pages = {65–72},
numpages = {8},
keywords = {domain-specific languages, software language engineering, language evaluation, experimental software engineering, usability},
location = {Portland, Oregon, USA},
series = {PLATEAU '11}
}

@inproceedings{10.1145/2049536.2049544,
author = {Yang, Rayoung and Park, Sangmi and Mishra, Sonali R. and Hong, Zhenan and Newsom, Clint and Joo, Hyeon and Hofer, Erik and Newman, Mark W.},
title = {Supporting Spatial Awareness and Independent Wayfinding for Pedestrians with Visual Impairments},
year = {2011},
isbn = {9781450309202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2049536.2049544},
doi = {10.1145/2049536.2049544},
abstract = {Much of the information designed to help people navigate the built environment is conveyed through visual channels, which means it is not accessible to people with visual impairments. Due to this limitation, travelers with visual impairments often have difficulty navigating and discovering locations in unfamiliar environments, which reduces their sense of independence with respect to traveling by foot. In this paper, we examine how mobile location-based computing systems can be used to increase the feeling of independence in travelers with visual impairments. A set of formative interviews with people with visual impairments showed that increasing one's general spatial awareness is the key to greater independence. This insight guided the design of Talking Points 3 (TP3), a mobile location-aware system for people with visual impairments that seeks to increase the legibility of the environment for its users in order to facilitate navigating to desired locations, exploration, serendipitous discovery, and improvisation. We conducted studies with eight legally blind participants in three campus buildings in order to explore how and to what extent TP3 helps promote spatial awareness for its users. The results shed light on how TP3 helped users find destinations in unfamiliar environments, but also allowed them to discover new points of interest, improvise solutions to problems encountered, develop personalized strategies for navigating, and, in general, enjoy a greater sense of independence.},
booktitle = {The Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {27–34},
numpages = {8},
keywords = {accessibility, wayfinding},
location = {Dundee, Scotland, UK},
series = {ASSETS '11}
}

@inproceedings{10.5555/2254436.2254447,
author = {Oliveira, Eduardo A. and Falc\~{a}o, Taciana Pontual and Ximenes, Victor and Melo, Paulo},
title = {Widgets Internship: Developing Learners Skills in a User-Centered Development Process},
year = {2011},
isbn = {9788576692577},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {This paper describes an internship program in design and computer programming, based on a user-centered development process to develop mobile applications for an industry partner. With the support of a team of professionals, and inspired by a problem-based learning approach, the interns followed a user-centered design cycle formed by research, ideation, prototyping and testing. They were involved in collecting and analyzing data from user research, generating concepts and implementing widgets for mobile phones. Within the program, where eighty-five widgets were expected, one hundred and eleven were produced, indicating that the process inspired and engaged the interns in applying the skills and knowledge acquired. Additionally, the high rate of downloads of the applications developed is a good indication that the user-centered process resulted in products that satisfied user and market demands.},
booktitle = {Proceedings of the 10th Brazilian Symposium on Human Factors in Computing Systems and the 5th Latin American Conference on Human-Computer Interaction},
pages = {48–55},
numpages = {8},
keywords = {problem-based learning, user-centered design, mobile, widgets},
location = {Porto de Galinhas, Pernambuco, Brazil},
series = {IHC+CLIHC '11}
}

@inproceedings{10.1145/2038916.2038944,
author = {Hunter, Timothy and Moldovan, Teodor and Zaharia, Matei and Merzgui, Samy and Ma, Justin and Franklin, Michael J. and Abbeel, Pieter and Bayen, Alexandre M.},
title = {Scaling the Mobile Millennium System in the Cloud},
year = {2011},
isbn = {9781450309769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038916.2038944},
doi = {10.1145/2038916.2038944},
abstract = {We report on our experience scaling up the Mobile Millennium traffic information system using cloud computing and the Spark cluster computing framework. Mobile Millennium uses machine learning to infer traffic conditions for large metropolitan areas from crowdsourced data, and Spark was specifically designed to support such applications. Many studies of cloud computing frameworks have demonstrated scalability and performance improvements for simple machine learning algorithms. Our experience implementing a real-world machine learning-based application corroborates such benefits, but we also encountered several challenges that have not been widely reported. These include: managing large parameter vectors, using memory efficiently, and integrating with the application's existing storage infrastructure. This paper describes these challenges and the changes they required in both the Spark framework and the Mobile Millennium software. While we focus on a system for traffic estimation, we believe that the lessons learned are applicable to other machine learning-based applications.},
booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
articleno = {28},
numpages = {8},
location = {Cascais, Portugal},
series = {SOCC '11}
}

@inproceedings{10.1145/2038916.2038927,
author = {Wang, Xiaodan and Olston, Christopher and Sarma, Anish Das and Burns, Randal},
title = {CoScan: Cooperative Scan Sharing in the Cloud},
year = {2011},
isbn = {9781450309769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038916.2038927},
doi = {10.1145/2038916.2038927},
abstract = {We present CoScan, a scheduling framework that eliminates redundant processing in workflows that scan large batches of data in a map-reduce computing environment. CoScan merges Pig programs from multiple users at runtime to reduce I/O contention while adhering to soft deadline requirements in scheduling. This includes support for join workflows that operate on multiple data sources. Our solution maps well to workflows at many Internet companies which reuse data from a common set of inputs. Experiments on the PigMix data analytics benchmark exhibit orders of magnitude reduction in resource contention with minimal impact on latency.},
booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
articleno = {11},
numpages = {12},
location = {Cascais, Portugal},
series = {SOCC '11}
}

@inproceedings{10.1145/2093698.2093801,
author = {Jabbari, Amir and Balasingham, Ilangko},
title = {Modeling Nano-Communication Networks Using Neurocomputing Algorithm},
year = {2011},
isbn = {9781450309134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2093698.2093801},
doi = {10.1145/2093698.2093801},
abstract = {In this paper, a novel practical neurocomputing algorithm is introduced and elaborated in order to design and implement a nano-communication network for various applications such as medical and industrial signal processing. Firstly, the idea of artificial neural network (ANN) for data processing is explained and feasibility of modeling a nano-scale network by an optimized neurocomputing algorithm is discussed using binary neuro-modeling. Moreover, it is stressed how nano-scaling increases the complexity of the communication network considering the existing constraints on computation resources, and accuracy of the proposed networking algorithm, either for communication or computation. Furthermore, the developed nano-scale networking technique is more optimized in order to assist the so-called neural nano-machines, to conduct the simple nano-nodes working more effectively and collaboratively. To experiment the performance of the presented bio-inspired nano-network, a practical test scenario is implemented on Imote2 sensor nodes to compare the accuracy of data processing techniques, showing how a large-scale network is replaced by an efficient nano-scale networking algorithm. Finally, the obtained results are illustrated and more elaborated to provide a complete procedure for future developments of the bio-inspired networking in nano-scale.},
booktitle = {Proceedings of the 4th International Symposium on Applied Sciences in Biomedical and Communication Technologies},
articleno = {103},
numpages = {5},
keywords = {self-organizing networks, smart communications, bio-inspired modeling, neural networks, medical data processing, nano-scale networking},
location = {Barcelona, Spain},
series = {ISABEL '11}
}

@inproceedings{10.1145/2093698.2093854,
author = {Surlea, Cristina and Kurugollu, Fatih and Milligan, Peter and Ong, Stephen},
title = {Foetal Motion Classification Using Optical Flow Displacement Histograms},
year = {2011},
isbn = {9781450309134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2093698.2093854},
doi = {10.1145/2093698.2093854},
abstract = {Foetal movement has been linked with foetal well-being. In the absence of medical input, its estimation depends exclusively on the mother's subjective opinion. Automatic classification of foetal movement from segmented two dimensional ultrasound scans is a step towards automatic estimation of foetal well-being through foetal motion evaluation. In this paper, optical flow displacement histograms are used to train a backpropagation neural network for classifying foetal movement by means of manually segmented frames that were evaluated independently by two medical experts. Results are promising towards developing an automated foetal motion analysis system but there is still the need for further testing of the hypothesis on a larger number of input samples.},
booktitle = {Proceedings of the 4th International Symposium on Applied Sciences in Biomedical and Communication Technologies},
articleno = {156},
numpages = {5},
keywords = {optical flow, foetal motion},
location = {Barcelona, Spain},
series = {ISABEL '11}
}

@inproceedings{10.1145/2038916.2038929,
author = {Zhang, Yanfeng and Gao, Qixin and Gao, Lixin and Wang, Cuirong},
title = {PrIter: A Distributed Framework for Prioritized Iterative Computations},
year = {2011},
isbn = {9781450309769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038916.2038929},
doi = {10.1145/2038916.2038929},
abstract = {Iterative computations are pervasive among data analysis applications in the cloud, including Web search, online social network analysis, recommendation systems, and so on. These cloud applications typically involve data sets of massive scale. Fast convergence of the iterative computation on the massive data set is essential for these applications. In this paper, we explore the opportunity for accelerating iterative computations and propose a distributed computing framework, PrIter, which enables fast iterative computation by providing the support of prioritized iteration. Instead of performing computations on all data records without discrimination, PrIter prioritizes the computations that help convergence the most, so that the convergence speed of iterative process is significantly improved. We evaluate PrIter on a local cluster of machines as well as on Amazon EC2 Cloud. The results show that PrIter achieves up to 50x speedup over Hadoop for a series of iterative algorithms.},
booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
articleno = {13},
numpages = {14},
keywords = {prioritized iteration, iterative algorithms, distributed framework, PrIter, MapReduce},
location = {Cascais, Portugal},
series = {SOCC '11}
}

@inproceedings{10.1145/2038916.2038943,
author = {Leibert, Florian and Mannix, Jake and Lin, Jimmy and Hamadani, Babak},
title = {Automatic Management of Partitioned, Replicated Search Services},
year = {2011},
isbn = {9781450309769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038916.2038943},
doi = {10.1145/2038916.2038943},
abstract = {Low-latency, high-throughput web services are typically achieved through partitioning, replication, and caching. Although these strategies and the general design of large-scale distributed search systems are well known, the academic literature provides surprisingly few details on deployment and operational considerations in production environments. In this paper, we address this gap by sharing the distributed search architecture that underlies Twitter user search, a service for discovering relevant accounts on the popular microblogging service. Our design makes use of the principle that eliminates the distinction between failure and other anticipated service disruptions: as a result, most operational scenarios share exactly the same code path. This simplicity leads to greater robustness and fault-tolerance. Another salient feature of our architecture is its exclusive reliance on open-source software components, which makes it easier for the community to learn from our experiences and replicate our findings.},
booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
articleno = {27},
numpages = {8},
keywords = {information retrieval, failover, distributed retrieval architectures, configuration management, robustness},
location = {Cascais, Portugal},
series = {SOCC '11}
}

@inproceedings{10.1145/2038916.2038935,
author = {Gulati, Ajay and Shanmuganathan, Ganesha and Ahmad, Irfan and Waldspurger, Carl and Uysal, Mustafa},
title = {Pesto: Online Storage Performance Management in Virtualized Datacenters},
year = {2011},
isbn = {9781450309769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038916.2038935},
doi = {10.1145/2038916.2038935},
abstract = {Virtualized datacenters strive to reduce costs through workload consolidation. Workloads exhibit a diverse set of IO behaviors and varying IO load that makes it difficult to estimate the IO performance on shared storage. As a result, system administrators often resort to gross overprovisioning or static partitioning of storage to meet application demands. In this paper, we introduce Pesto, a unified storage performance management system for heterogeneous virtualized datacenters. Pesto is the first system that completely automates storage performance management for virtualized datacenters, providing IO load balancing with cost-benefit analysis, per-device congestion management, and initial placement of new workloads.At its core, Pesto constructs and adapts approximate black-box performance models of storage devices automatically, leveraging our analysis linking device throughput and latency to outstanding IOs.Experimental results for a wide range of devices and configurations validate the accuracy of these models. We implemented Pesto in a commercial product and tested its performance on tens of devices, running hundreds of test cases over the past year. End-to-end experiments demonstrate that Pesto is efficient, adapts to changes quickly and can improve workload performance by up to 19%, achieving our objective of lowering storage management costs through automation.},
booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
articleno = {19},
numpages = {14},
keywords = {storage, QoS, VM, device, virtualization, modeling},
location = {Cascais, Portugal},
series = {SOCC '11}
}

@inproceedings{10.1145/2038916.2038923,
author = {Bhatotia, Pramod and Wieder, Alexander and Rodrigues, Rodrigo and Acar, Umut A. and Pasquin, Rafael},
title = {Incoop: MapReduce for Incremental Computations},
year = {2011},
isbn = {9781450309769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038916.2038923},
doi = {10.1145/2038916.2038923},
abstract = {Many online data sets evolve over time as new entries are slowly added and existing entries are deleted or modified. Taking advantage of this, systems for incremental bulk data processing, such as Google's Percolator, can achieve efficient updates. To achieve this efficiency, however, these systems lose compatibility with the simple programming models offered by non-incremental systems, e.g., MapReduce, and more importantly, requires the programmer to implement application-specific dynamic algorithms, ultimately increasing algorithm and code complexity.In this paper, we describe the architecture, implementation, and evaluation of Incoop, a generic MapReduce framework for incremental computations. Incoop detects changes to the input and automatically updates the output by employing an efficient, fine-grained result reuse mechanism. To achieve efficiency without sacrificing transparency, we adopt recent advances in the area of programming languages to identify the shortcomings of task-level memoization approaches, and to address these shortcomings by using several novel techniques: a storage system, a contraction phase for Reduce tasks, and an affinity-based scheduling algorithm. We have implemented Incoop by extending the Hadoop framework, and evaluated it by considering several applications and case studies. Our results show significant performance improvements without changing a single line of application code.},
booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
articleno = {7},
numpages = {14},
keywords = {memoization, stability, self-adjusting computation},
location = {Cascais, Portugal},
series = {SOCC '11}
}

@inproceedings{10.1145/2064747.2064766,
author = {Emerencia, Ando and van der Krieke, Lian and Petkov, Nicolai and Aiello, Marco},
title = {Assessing Schizophrenia with an Interoperable Architecture},
year = {2011},
isbn = {9781450309547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064747.2064766},
doi = {10.1145/2064747.2064766},
abstract = {With the introduction of electronic personal health records and e-health applications spreading, interoperability concerns are of increasing importance to hospitals and care facilities. Interoperability between distributed and complex systems requires, among other things, compatible data formats. The recommended approach is to store data using international terminology standards. For data that is not stored in this way, a conversion process must happen. This can be tedious manual work when multiple input and output formats are to be supported. We present WEGWEIS, a web application for schizophrenia patients that converts questionnaire answers into advice. The system's advice delivery is based on data extracted from the electronic medical records of 1379 patients. In WEGWEIS, we handle the conversion by decoupling input formats from output formats, using an ontology as intermediate layer. We present the algorithm and provide details on its implementation.},
booktitle = {Proceedings of the First International Workshop on Managing Interoperability and Complexity in Health Systems},
pages = {79–82},
numpages = {4},
keywords = {ontology, case-based reasoning, routine outcome monitoring},
location = {Glasgow, Scotland, UK},
series = {MIXHS '11}
}

@inproceedings{10.1145/2065023.2065028,
author = {Roshchina, Alexandra and Cardiff, John and Rosso, Paolo},
title = {A Comparative Evaluation of Personality Estimation Algorithms for the Twin Recommender System},
year = {2011},
isbn = {9781450309493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2065023.2065028},
doi = {10.1145/2065023.2065028},
abstract = {The appearance of the so-called recommender systems has led to the possibility of reducing the information overload experienced by individuals searching among online resources. One of the areas of application of recommender systems is the online tourism domain where sites like TripAdvisor allow people to post reviews of various hotels to help others make a good choice when planning their trip. As the number of such reviews grows in size every day, clearly it is impractical for the individual to go through all of them. We propose the TWIN ("Tell me What I Need") Personality-based Recommender System that analyzes the textual content of the reviews and estimates the personality of the user according to the Big Five model to suggest the reviews written by "twin-minded" people. In this paper we compare a number of algorithms to select the better option for personality estimation in the task of user profile construction.},
booktitle = {Proceedings of the 3rd International Workshop on Search and Mining User-Generated Contents},
pages = {11–18},
numpages = {8},
keywords = {personality-based profile, personality from the text, natural language processing, recommender system},
location = {Glasgow, Scotland, UK},
series = {SMUC '11}
}

@inproceedings{10.1145/2065003.2065016,
author = {Wojciechowski, Artur},
title = {E-ETL: Framework for Managing Evolving Etl Processes},
year = {2011},
isbn = {9781450309530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2065003.2065016},
doi = {10.1145/2065003.2065016},
abstract = {External data sources (EDSs) being integrated in a data warehouse (DW) frequently change their data structures (schemas). As a consequence, in many cases, an already deployed ETL workflow executes with errors. Since structural changes of EDSs are frequent, an automatic reparation of an ETL workflow after such changes is of a high importance. In this paper we present a framework for handling the evolution of an ETL layer. To this end, structural changes are monitored and stored in a Metabase. An erroneous execution of an ETL workflow causes a reparation of the ETL activities that interact with the changed EDS, so that the repaired activities can work on the changed EDS schema. The reparation of the ETL activities is guided by several customizable reparation algorithms. The proposed framework was developed as a module external to an ETL engine, accessing the engine by means of API. The innovation of this framework are algorithms for semi-automatic reparation of an ETL workflow.},
booktitle = {Proceedings of the 4th Workshop on Workshop for Ph.D. Students in Information &amp; Knowledge Management},
pages = {59–66},
numpages = {8},
keywords = {etl, e-etl, evolving etl, evolution},
location = {Glasgow, Scotland, UK},
series = {PIKM '11}
}

@inproceedings{10.1145/2064969.2064971,
author = {Dallachiesa, Michele and Nushi, Besmira and Mirylenka, Katsiaryna and Palpanas, Themis},
title = {Similarity Matching for Uncertain Time Series: Analytical and Experimental Comparison},
year = {2011},
isbn = {9781450310376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064969.2064971},
doi = {10.1145/2064969.2064971},
abstract = {In the last years there has been a considerable increase in the availability of continuous sensor measurements in a wide range of application domains, such as Location-Based Services (LBS), medical monitoring systems, manufacturing plants and engineering facilities to ensure efficiency, product quality and safety, hydrologic and geologic observing systems, pollution management, and others.Due to the inherent imprecision of sensor observations, many investigations have recently turned into querying, mining and storing uncertain data. Uncertainty can also be due to data aggregation, privacy-preserving transforms, and error-prone mining algorithms.In this study, we survey the techniques that have been proposed specifically for modeling and processing uncertain time series, an important model for temporal data. We provide both an analytical evaluation of the alternatives that have been proposed in the literature, highlighting the advantages and disadvantages of each approach. We additionally conduct an extensive experimental evaluation with 17 real datasets, and discuss some surprising results. Based on our evaluations, we also provide guidelines useful for practitioners in the field.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Querying and Mining Uncertain Spatio-Temporal Data},
pages = {8–15},
numpages = {8},
keywords = {time series, uncertain data, distance measure, similarity},
location = {Chicago, Illinois},
series = {QUeST '11}
}

@inproceedings{10.1145/2093973.2094015,
author = {Adams, Benjamin and Janowicz, Krzysztof},
title = {Constructing Geo-Ontologies by Reification of Observation Data},
year = {2011},
isbn = {9781450310314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2093973.2094015},
doi = {10.1145/2093973.2094015},
abstract = {The semantic integration of heterogeneous, spatiotemporal information is a major challenge for achieving the vision of a multi-thematic and multi-perspective Digital Earth. The Semantic Web technology stack has been proposed to address the integration problem by knowledge representation languages and reasoning. However approaches such as the Web Ontology Languages (OWL) were developed with decidability in mind. They do not integrate well with established modeling paradigms in the geosciences that are dominated by numerical and geometric methods. Additionally, work on the Semantic Web is mostly feature-centric and a field-based view is difficult to integrate. A layer specifying the transition from observation data to classes and relations is missing. In this work we combine OWL with geometric and topological language constructs based on similarity spaces. Our approach provides three main benefits. First, class constructors can be built from a larger palette of mathematical operations based on vector algebra. Second, it affords the representation of prototype-based classes. Third, it facilitates the representation of classes derived from machine learning classifiers that utilize a multi-dimensional feature space. Instead of following a one-size-fits-all approach, our work allows one to derive contextualized OWL ontologies by reification of observation data.},
booktitle = {Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {309–318},
numpages = {10},
keywords = {ontology, context, conceptual spaces, provenance, observations},
location = {Chicago, Illinois},
series = {GIS '11}
}

@article{10.5555/2835445.2835447,
author = {Romano Bergstrom, Jennifer C. and Olmsted-Hawala, Erica L. and Chen, Jennifer M. and Murphy, Elizabeth D.},
title = {Conducting Iterative Usability Testing on a Web Site: Challenges and Benefits},
year = {2011},
issue_date = {November 2011},
publisher = {Usability Professionals' Association},
address = {Bloomingdale, IL},
volume = {7},
number = {1},
issn = {1931-3357},
abstract = {This paper demonstrates the benefits and challenges of working collaboratively with designers and developers while conducting iterative usability testing during the course of Web site design. Four rounds of usability testing were conducted using materials of increasing realism to represent the user interface of a public government site: 1) low-fidelity paper prototypes; 2) medium-fidelity, non-clickable HTML images; and 3) and 4) high-fidelity, partially-clickable Web pages. Through three rounds of usability testing, usability increased, but in the fourth round, usability declined. Iterative testing enabled evaluators to collect quantitative and qualitative data from typical users, address usability issues, and test new, revised designs throughout the design process. This study demonstrates the challenges and value of working collaboratively with designers and developers to create tasks, collect participant data, and create and test solutions to usability issues throughout the entire cycle of user-interface design.},
journal = {J. Usability Studies},
month = nov,
pages = {9–30},
numpages = {22},
keywords = {iterative testing, low fidelity, high fidelity, eye tracking, usability, medium fidelity, user interface, paper prototypes}
}

@article{10.1145/2039339.2039342,
author = {Alankus, Gazihan and Proffitt, Rachel and Kelleher, Caitlin and Engsberg, Jack},
title = {Stroke Therapy through Motion-Based Games: A Case Study},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/2039339.2039342},
doi = {10.1145/2039339.2039342},
abstract = {In the United States alone, more than five million people are living with long term motor impairments caused by a stroke. Recently, video games with affordable motion-based input devices have been proposed as a part of therapy to help people recover lost range of motion and motor control. While researchers have demonstrated the potential utility of therapeutic games through controlled studies, relatively little work has explored their long-term home-based use. We conducted a six-week home study with a 62-year-old woman who was seventeen years post-stroke. She played therapeutic games for approximately one hour a day, five days a week. Over the six weeks, she recovered significant motor abilities, which is unexpected given the time since her stroke. We explore detecting such improvements early, using game logs for daily measurements of motor ability to complement the standard measurements that are taken less often. Through observations and interviews, we present lessons learned about the barriers and opportunities that arise from long-term home-based use of therapeutic games.},
journal = {ACM Trans. Access. Comput.},
month = nov,
articleno = {3},
numpages = {35},
keywords = {video games, Stroke rehabilitation, therapy}
}

@inproceedings{10.1145/2063212.2063224,
author = {Jackoway, Alan and Samet, Hanan and Sankaranarayanan, Jagan},
title = {Identification of Live News Events Using Twitter},
year = {2011},
isbn = {9781450310338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063212.2063224},
doi = {10.1145/2063212.2063224},
abstract = {Twitter presents a source of information that cannot easily be obtained anywhere else. However, though many posts on Twitter reveal up-to-the-minute information about events in the world or interesting sentiments, far more posts are of no interest to the general audience. A method to determine which Twitter users are posting reliable information and which posts are interesting is presented. Using this information a search through a large, online news corpus is conducted to discover future events before they occur along with information about the location of the event. These events can be identified with a high degree of accuracy by verifying that an event found in one news article is found in other similar news articles, since any event interesting to a general audience will likely have more than one news story written about it. Twitter posts near the time of the event can then be identified as interesting if they match the event in terms of keywords or location. This method enables the discovery of interesting posts about current and future events and helps in the identification of reliable users.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Social Networks},
pages = {25–32},
numpages = {8},
keywords = {event detection, tense identification, Twitter, future tense, geotagging},
location = {Chicago, Illinois},
series = {LBSN '11}
}

