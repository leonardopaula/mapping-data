@article{10.1145/2184442.2184443,
author = {Sunyaev, Ali and Chornyi, Dmitry},
title = {Supporting Chronic Disease Care Quality: Design and Implementation of a Health Service and Its Integration with Electronic Health Records},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2184442.2184443},
doi = {10.1145/2184442.2184443},
abstract = {Chronic medical conditions take a huge toll on lives of a growing number of people and are a major contributor to the rising costs in healthcare. As patients are increasingly willing to take an active part in managing their conditions, chronic disease self-management programs and information systems that support them are recognized for their potential to improve the quality of healthcare delivery. These programs often rely on recording longitudinal patient data and analyzing it. Therefore, maintaining appropriate data quality is important for self-management programs to be efficient and safe. We designed and implemented a prototype of a health self-management service for chronically ill people. It is a distributed application that supports patients with diabetes at tracking their blood glucose levels. The main design goals were usability, extensibility, security, and interoperability. The system integrates with the Microsoft HealthVault and Google Health personal health record platforms. It utilizes industry-strength storage and security mechanisms, is scalable, and as a result, can be used to gather, securely store, and analyze patient data over long periods of time. In this article we examine how software information technology can support chronic disease self-management and its impact on the quality of patient data. Furthermore, we describe the requirements that drove the system's development, its architecture, and design decisions.},
journal = {J. Data and Information Quality},
month = may,
articleno = {3},
numpages = {21},
keywords = {health management system, personal health records, Data quality, chronic disease management}
}

@inproceedings{10.1145/2214091.2214104,
author = {Litecky, Chuck and Igou, Amy J. and Aken, Andrew},
title = {Skills in the Management Oriented IS and Enterprise System Job Markets},
year = {2012},
isbn = {9781450311106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2214091.2214104},
doi = {10.1145/2214091.2214104},
abstract = {With turmoil in the U.S. and world economies, it is more necessary than ever to ensure that graduates and employees have the skills necessary to compete in the job market. Some of the previous research on job skills has looked at job advertisements in print and online media to determine skills that employers were seeking; other research interviewed or surveyed employers to determine employers' needs. Results were often conflicting as previously summarized in Litecky, Zwieg and Huang. Now web content data mining techniques allow for much greater sampling and specific analysis of current MIS (simply defined as Management oriented Information Systems types of skills) oriented job ads. In this approach over two million advertisements were filtered to over one half million U.S. job advertisements for MIS degree graduates from various job websites. Management-oriented IS skills were compiled in a thesaurus and then those skills were extracted from the selected job ads. These skills were compared to the job advertisements for subsequent analysis. Results suggest much more concordance between disparate job advertisements and survey-based research. These results reinforce much of the desired uniqueness of MIS curricula as compared to other computing degree curricula. Results herein are limited to U.S. and are also contrasted with the similar content and analysis of job skills in the Australian IS job market. In addition an analysis is performed on the cross disciplinary data on Enterprise System/SAP jobs which were not limited to MIS graduates.},
booktitle = {Proceedings of the 50th Annual Conference on Computers and People Research},
pages = {35–44},
numpages = {10},
keywords = {enterprise systems, careers, SAP, MIS, job skills, job markets},
location = {Milwaukee, Wisconsin, USA},
series = {SIGMIS-CPR '12}
}

@article{10.1145/2167076.2167083,
author = {Sen, Pradeep and Darabi, Soheil},
title = {On Filtering the Noise from the Random Parameters in Monte Carlo Rendering},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/2167076.2167083},
doi = {10.1145/2167076.2167083},
abstract = {Monte Carlo (MC) rendering systems can produce spectacular images but are plagued with noise at low sampling rates. In this work, we observe that this noise occurs in regions of the image where the sample values are a direct function of the random parameters used in the Monte Carlo system. Therefore, we propose a way to identify MC noise by estimating this functional relationship from a small number of input samples. To do this, we treat the rendering system as a black box and calculate the statistical dependency between the outputs and inputs of the system. We then use this information to reduce the importance of the sample values affected by MC noise when applying an image-space, cross-bilateral filter, which removes only the noise caused by the random parameters but preserves important scene detail. The process of using the functional relationships between sample values and the random parameter inputs to filter MC noise is called Random Parameter Filtering (RPF), and we demonstrate that it can produce images in a few minutes that are comparable to those rendered with a thousand times more samples. Furthermore, our algorithm is general because we do not assign any physical meaning to the random parameters, so it works for a wide range of Monte Carlo effects, including depth of field, area light sources, motion blur, and path-tracing. We present results for still images and animated sequences at low sampling rates that have higher quality than those produced with previous approaches.},
journal = {ACM Trans. Graph.},
month = may,
articleno = {18},
numpages = {15},
keywords = {Monte Carlo rendering, global illumination}
}

@article{10.14778/2336664.2336672,
author = {Papapetrou, Odysseas and Garofalakis, Minos and Deligiannakis, Antonios},
title = {Sketch-Based Querying of Distributed Sliding-Window Data Streams},
year = {2012},
issue_date = {June 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2336664.2336672},
doi = {10.14778/2336664.2336672},
abstract = {While traditional data-management systems focus on evaluating single, ad-hoc queries over static data sets in a centralized setting, several emerging applications require (possibly, continuous) answers to queries on dynamic data that is widely distributed and constantly updated. Furthermore, such query answers often need to discount data that is "stale", and operate solely on a sliding window of recent data arrivals (e.g., data updates occurring over the last 24 hours). Such distributed data streaming applications mandate novel algorithmic solutions that are both time- and space-efficient (to manage high-speed data streams), and also communication-efficient (to deal with physical data distribution). In this paper, we consider the problem of complex query answering over distributed, high-dimensional data streams in the sliding-window model. We introduce a novel sketching technique (termed ECM-sketch) that allows effective summarization of streaming data over both time-based and count-based sliding windows with probabilistic accuracy guarantees. Our sketch structure enables point as well as inner-product queries, and can be employed to address a broad range of problems, such as maintaining frequency statistics, finding heavy hitters, and computing quantiles in the sliding-window model. Focusing on distributed environments, we demonstrate how ECM-sketches of individual, local streams can be composed to generate a (low-error) ECM-sketch summary of the order-preserving aggregation of all streams; furthermore, we show how ECM-sketches can be exploited for continuous monitoring of sliding-window queries over distributed streams. Our extensive experimental study with two real-life data sets validates our theoretical claims and verifies the effectiveness of our techniques. To the best of our knowledge, ours is the first work to address efficient, guaranteed-error complex query answering over distributed data streams in the sliding-window model.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {992–1003},
numpages = {12}
}

@article{10.1145/2173637.2173645,
author = {Cidon, Asaf and London, Tomer},
title = {An Interview with Mendel Rosenblum},
year = {2012},
issue_date = {Summer 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1528-4972},
url = {https://doi.org/10.1145/2173637.2173645},
doi = {10.1145/2173637.2173645},
abstract = {The co-founder of VMware and Associate Professor of Computer Science at Stanford explains how an academic research project evolved into a commercial idea.},
journal = {XRDS},
month = jun,
pages = {13–15},
numpages = {3}
}

@article{10.1145/2209310.2209314,
author = {Cremonesi, Paolo and Garzotto, Franca and Turrin, Roberto},
title = {Investigating the Persuasion Potential of Recommender Systems from a Quality Perspective: An Empirical Study},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2209310.2209314},
doi = {10.1145/2209310.2209314},
abstract = {Recommender Systems (RSs) help users search large amounts of digital contents and services by allowing them to identify the items that are likely to be more attractive or useful. RSs play an important persuasion role, as they can potentially augment the users’ trust towards in an application and orient their decisions or actions towards specific directions. This article explores the persuasiveness of RSs, presenting two vast empirical studies that address a number of research questions.First, we investigate if a design property of RSs, defined by the statistically measured quality of algorithms, is a reliable predictor of their potential for persuasion. This factor is measured in terms of perceived quality, defined by the overall satisfaction, as well as by how users judge the accuracy and novelty of recommendations. For our purposes, we designed an empirical study involving 210 subjects and implemented seven full-sized versions of a commercial RS, each one using the same interface and dataset (a subset of Netflix), but each with a different recommender algorithm. In each experimental configuration we computed the statistical quality (recall and F-measures) and collected data regarding the quality perceived by 30 users. The results show us that algorithmic attributes are less crucial than we might expect in determining the user’s perception of an RS’s quality, and suggest that the user’s judgment and attitude towards a recommender are likely to be more affected by factors related to the user experience.Second, we explore the persuasiveness of RSs in the context of large interactive TV services. We report a study aimed at assessing whether measurable persuasion effects (e.g., changes of shopping behavior) can be achieved through the introduction of a recommender. Our data, collected for more than one year, allow us to conclude that, (1) the adoption of an RS can affect both the lift factor and the conversion rate, determining an increased volume of sales and influencing the user’s decision to actually buy one of the recommended products, (2) the introduction of an RS tends to diversify purchases and orient users towards less obvious choices (the long tail), and (3) the perceived novelty of recommendations is likely to be more influential than their perceived accuracy.Overall, the results of these studies improve our understanding of the persuasion phenomena induced by RSs, and have implications that can be of interest to academic scholars, designers, and adopters of this class of systems.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {11},
numpages = {41},
keywords = {satisfaction, empirical study, design, quality evaluation, persuasion, accuracy, lift factor, recommender algorithm, conversion rate, perceived quality, novelty, F-measure, iTV, Recommender Systems, fallout, recall, long tail}
}

@inproceedings{10.5555/2664446.2664458,
author = {Hindle, Abram},
title = {Green Mining: A Methodology of Relating Software Change to Power Consumption},
year = {2012},
isbn = {9781467317610},
publisher = {IEEE Press},
abstract = {Power consumption is becoming more and more important with the increased popularity of smart-phones, tablets and laptops. The threat of reducing a customer's battery-life now hangs over the software developer who asks, "will this next change be the one that causes my software to drain a customer's battery?" One solution is to detect power consumption regressions by measuring the power usage of tests, but this is time-consuming and often noisy. An alternative is to rely on software metrics that allow us to estimate the impact that a change might have on power consumption thus relieving the developer from expensive testing. This paper presents a general methodology for investigating the impact of software change on power consumption, we relate power consumption to software changes, and then investigate the impact of static OO software metrics on power consumption. We demonstrated that software change can effect power consumption using the Firefox web-browser and the Azureus/Vuze BitTorrent client. We found evidence of a potential relationship between some software metrics and power consumption. In conclusion, we explored the effect of software change on power consumption on two projects; and we provide an initial investigation on the impact of software metrics on power consumption.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
pages = {78–87},
numpages = {10},
keywords = {power, software metrics, mining software repositories, dynamic analysis, power consumption, sustainable-software},
location = {Zurich, Switzerland},
series = {MSR '12}
}

@inproceedings{10.5555/2663638.2663653,
author = {Klendauer, Ruth and Hoffmann, Axel and Leimeister, Jan Marco and Berkovich, Marina and Krcmar, Helmut},
title = {Using the IDEAL Software Process Improvement Model for the Implementation of Automotive SPICE},
year = {2012},
isbn = {9781467318242},
publisher = {IEEE Press},
abstract = {Most suppliers in the automotive industry are facing the challenge of implementing Automotive SPICE, a domain-specific model of ISO/IEC 15504, which was first published in 2005. Original equipment manufacturers have increasingly requested SPICE certifications from their suppliers in order to effectively evaluate the development processes and identify reliable partners. To support organizations in their software process improvement effort, the Software Engineering Institute (SEI) developed the IDEAL model. This guide is, however, influenced by the SEI's work on CMM and based on the experiences with very large organizations. The goal of this paper is to examine the suitability of the IDEAL model for the implementation of Automotive SPICE at a medium-sized R&amp;D department. Preliminary results from an action research study at a global systems engineering company are discussed. The data indicate that adjustments are especially needed with regard to the factors participation and communication. Also, informal networks between companies, early decision-making and usability are shown to play an important role and might be added to the model.},
booktitle = {Proceedings of the 5th International Workshop on Co-Operative and Human Aspects of Software Engineering},
pages = {66–72},
numpages = {7},
keywords = {improvement models, automotive-SPICE, software process improvement, IDEAL-model},
location = {Zurich, Switzerland},
series = {CHASE '12}
}

@inproceedings{10.5555/2382029.2382089,
author = {Cherry, Colin and Foster, George},
title = {Batch Tuning Strategies for Statistical Machine Translation},
year = {2012},
isbn = {9781937284206},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.},
booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {427–436},
numpages = {10},
location = {Montreal, Canada},
series = {NAACL HLT '12}
}

@inproceedings{10.1145/2228360.2228486,
author = {Zhang, Xuehui and Tuzzio, Nicholas and Tehranipoor, Mohammad},
title = {Identification of Recovered ICs Using Fingerprints from a Light-Weight on-Chip Sensor},
year = {2012},
isbn = {9781450311991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2228360.2228486},
doi = {10.1145/2228360.2228486},
abstract = {The counterfeiting and recycling of integrated circuits (ICs) have become major problems in recent years, potentially impacting the security of electronic systems bound for military, financial, or other critical applications. With identical functionality and packaging, it is extremely difficult to distinguish recovered ICs from unused ICs. A technique is proposed to distinguish used ICs from the unused ones using a fingerprint generated by a light-weight on-chip sensor. Using statistical data analysis, process and temperature variations' effects on the sensors can be separated from aging experienced by the sensors in the ICs when used in the field. Simulation results, featuring the sensor using 90nm technology, and silicon results from 90nm test chips demonstrate the effectiveness of this technique for identification of recovered ICs.},
booktitle = {Proceedings of the 49th Annual Design Automation Conference},
pages = {703–708},
numpages = {6},
keywords = {recovered ICs, circuit aging, counterfeiting, hardware security},
location = {San Francisco, California},
series = {DAC '12}
}

@inproceedings{10.5555/2667062.2667063,
author = {Schiller, Todd W. and Lucia, Brandon},
title = {Playing Cupid: The IDE as a Matchmaker for Plug-Ins},
year = {2012},
isbn = {9781467318204},
publisher = {IEEE Press},
abstract = {We describe a composable, data-driven, plug-in ecosystem for IDEs. Inspired by Unix's and Windows Power-Shell's pipeline communication models, each plug-in declares data-driven capabilities. Developers can then seamlessly mix, match, and combine plug-in capabilities to produce new insight, without modifying the plug-ins.We formalize the architecture using the polymorphic lambda calculus, with special types for source and source locations; the type system prevents nonsensical plug-in combinations, and helps to inform the design of new tools and plug-ins. To illustrate the power of the formalism, we describe several synergies between existing plug-ins (and tools) made possible by the ecosystem.},
booktitle = {Proceedings of the Second International Workshop on Developing Tools as Plug-Ins},
pages = {1–6},
numpages = {6},
location = {Zurich, Switzerland},
series = {TOPI '12}
}

@inproceedings{10.5555/2669379.2669390,
author = {Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
title = {PIVoT: Project Insights and Visualization Toolkit},
year = {2012},
isbn = {9781467317627},
publisher = {IEEE Press},
abstract = {An in-process view into a software development project's health is critical for its success. However, in services organizations, a typical software development team employs a heterogeneous set of tools based on client requirements through the different phases of the software project. The use of disparate tools with non-compatible outputs makes it very difficult to extract one coherent picture of the project's health and status. Existing project management tools either work at the process layer and rely on manually entered information, or are activity centric, without a holistic view. In this paper, we present PIVoT, a metric-based framework for automated, non-invasive, and in-process data collection and analysis in heterogeneous software project environments, that provides rich, multi-dimensional insights into the project's health and trajectory. Here, we introduce the different analyses, insights and metrics, and discuss their usage in typical software projects.},
booktitle = {Proceedings of the 3rd International Workshop on Emerging Trends in Software Metrics},
pages = {63–69},
numpages = {7},
keywords = {project management, software development management, software metrics},
location = {Zurich, Switzerland},
series = {WETSoM '12}
}

@article{10.1145/2180861.2180865,
author = {Desnoyers, Peter and Wood, Timothy and Shenoy, Prashant and Singh, Rahul and Patil, Sangameshwar and Vin, Harrick},
title = {Modellus: Automated Modeling of Complex Internet Data Center Applications},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/2180861.2180865},
doi = {10.1145/2180861.2180865},
abstract = {The rising complexity of distributed server applications in Internet data centers has made the tasks of modeling and analyzing their behavior increasingly difficult. This article presents Modellus, a novel system for automated modeling of complex web-based data center applications using methods from queuing theory, data mining, and machine learning. Modellus uses queuing theory and statistical methods to automatically derive models to predict the resource usage of an application and the workload it triggers; these models can be composed to capture multiple dependencies between interacting applications.Model accuracy is maintained by fast, distributed testing, automated relearning of models when they change, and methods to bound prediction errors in composite models. We have implemented a prototype of Modellus, deployed it on a data center testbed, and evaluated its efficacy for modeling and analysis of several distributed multitier web applications. Our results show that this feature-based modeling technique is able to make predictions across several data center tiers, and maintain predictive accuracy (typically 95% or better) in the face of significant shifts in workload composition; we also demonstrate practical applications of the Modellus system to prediction and provisioning of real-world data center applications.},
journal = {ACM Trans. Web},
month = jun,
articleno = {8},
numpages = {29},
keywords = {Workload and performance modeling, internet applications}
}

@inproceedings{10.1145/2307729.2307732,
author = {Kaschesky, Michael and Gschwend, Adrian and Bouchard, Guillaume and Furrer, Patrick and Gamard, Stephane and Riedl, Reinhard},
title = {Aid to Regional Development Agencies: Finding and Matching Research Funding Opportunities},
year = {2012},
isbn = {9781450314039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307729.2307732},
doi = {10.1145/2307729.2307732},
abstract = {Regional development agencies are confronted with a plethora of research funding programs that provide opportunities for regional research groups and SMEs. This paper describes the practical benefits and technological building blocks of an online matching service of research profiles with funding opportunities. It gives an overview of the infrastructure for data sourcing and processing based on a scalable cloud computing platform. It then describes how data is consolidated, analyzed, and interlinked so as to facilitate the finding and matching of funding opportunities. Integrating concepts and themes with those available as Linked Open Data (LOD) adds value to finding and matching by interlinking results with publicly available data resources, and to improve search performance for related content on the internet. In order to optimize matching, the paper describes the user-profiling capabilities based on statistical analyses and machine learning. The paper concludes by discussing the lessons learned so far as well as possible extensions into other application areas.},
booktitle = {Proceedings of the 13th Annual International Conference on Digital Government Research},
pages = {2–10},
numpages = {9},
keywords = {reliability, standardization, economics, measurement, design},
location = {College Park, Maryland, USA},
series = {dg.o '12}
}

@inproceedings{10.1145/2307729.2307757,
author = {Park, Joonsuk and Klingel, Sally and Cardie, Claire and Newhart, Mary and Farina, Cynthia and Vallb\'{e}, Joan-Josep},
title = {Facilitative Moderation for Online Participation in ERulemaking},
year = {2012},
isbn = {9781450314039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307729.2307757},
doi = {10.1145/2307729.2307757},
abstract = {This paper describes the use of facilitative moderation strategies in an online rulemaking public participation system. Rulemaking is one of the U. S. government's most important policymaking methods. Although broad transparency and participation rights are part of its legal structure, significant barriers prevent effective engagement by many groups of interested citizens. Regulation Room, an experimental open-government partnership between academic researchers and government agencies, is a socio-technical participation system that uses multiple methods to lower potential barriers to broader participation. To encourage effective individual comments and productive group discussion in Regulation Room, we adapt strategies for facilitative human moderation originating from social science research in deliberative democracy and alternative dispute resolution [24, 1, 18, 14] for use in the demanding online participation setting of eRulemaking. We develop a moderation protocol, deploy it in "live" Department of Transportation (DOT) rulemakings, and provide an initial analysis of its use through a manual coding of all moderator interventions with respect to the protocol. We then investigate the feasibility of automating the moderation protocol: we employ annotated data from the coding project to train machine learning-based classifiers to identify places in the online discussion where human moderator intervention is required. Though the trained classifiers only marginally outperform the baseline, the improvement is statistically significant in spite of limited data and a very basic feature set, which is a promising result.},
booktitle = {Proceedings of the 13th Annual International Conference on Digital Government Research},
pages = {173–182},
numpages = {10},
location = {College Park, Maryland, USA},
series = {dg.o '12}
}

@inproceedings{10.1145/2307729.2307742,
author = {Wilson, Susan Copeland},
title = {E-Government Legislation Meets the Poverty Threshold: Issues for the Economically Disadvantaged},
year = {2012},
isbn = {9781450314039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307729.2307742},
doi = {10.1145/2307729.2307742},
abstract = {E-government has promised to deliver government services faster and with greater efficiency and transparency, reduce costs, engage the public as government partners, and serve as a democratizing agent. But rather than leverage e-government to examine the assumptions and presumptions that are part of the existing manual processes to deliver services to the poor, those processes have been automated without deep consideration of the recipients' needs and barriers to access, privacy or civil rights, or building in solid data points to measure their effectiveness. This paper examines the legislative framework that supports the most common public assistance programs and other e-government policies that have indirectly impacted the poor. It identifies several successes in easing living in poverty and suggests several points in the legislation that could help bring the poor to fuller engagement and reduce invisibility within the government.},
booktitle = {Proceedings of the 13th Annual International Conference on Digital Government Research},
pages = {74–83},
numpages = {10},
keywords = {disadvantaged populations, marginalization, privacy, legislation, poverty, civil rights, e-government},
location = {College Park, Maryland, USA},
series = {dg.o '12}
}

@inproceedings{10.1145/2307729.2307790,
author = {Helbig, Natalie and Nakashima, Manabu and Dawes, Sharon S.},
title = {Understanding the Value and Limits of Government Information in Policy Informatics: A Preliminary Exploration},
year = {2012},
isbn = {9781450314039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307729.2307790},
doi = {10.1145/2307729.2307790},
abstract = {Policy informatics is an emergent area of study that explores how information and communication technology can support policy making and governance. Policy informatics recognizes that more kinds, sources and volumes of information, coupled with evolving analytical and computational tools, present important opportunities to address increasingly complex social, political, and management problems. However, while new types and sources of information hold much promise for policy analysis, the specific characteristics of any particular government information resource strongly influences its fitness and usability for analytical purposes. We therefore contend that information itself should be a critical research topic in policy informatics. This poster presentation shows how different aspects of information conceptualization, management, quality, and use can affect its "fitness" for policy analysis.},
booktitle = {Proceedings of the 13th Annual International Conference on Digital Government Research},
pages = {291–293},
numpages = {3},
keywords = {policy informatics, policy analysis, fitness for use, information quality},
location = {College Park, Maryland, USA},
series = {dg.o '12}
}

@inproceedings{10.5555/2330147.2330160,
author = {Ritschel, Tobias and Templin, Krzysztof and Myszkowski, Karol and Seidel, Hans-Peter},
title = {Virtual Passepartouts},
year = {2012},
isbn = {9783905673906},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {In traditional media, such as photography and painting, a cardboard sheet with a cutout (called passepartout) is frequently placed on top of an image. One of its functions is to increase the depth impression via the "looking-through-a-window" metaphor. This paper shows how an improved 3D effect can be achieved by using a virtual passepartout: a 2D framing that selectively masks the 3D shape and leads to additional occlusion events between the virtual world and the frame. We introduce a pipeline to design virtual passepartouts interactively as a simple post-process on RGB images augmented with depth information. Additionally, an automated approach finds the optimal virtual passepartout for a given scene. Virtual passepartouts can be used to enhance depth depiction in images and videos with depth information, renderings, stereo images and the fabrication of physical passepartouts.},
booktitle = {Proceedings of the Symposium on Non-Photorealistic Animation and Rendering},
pages = {57–63},
numpages = {7},
keywords = {matting, depth, passepartout},
location = {Annecy, France},
series = {NPAR '12}
}

@inproceedings{10.5555/2343576.2343585,
author = {Kang, Sin-Hwa and Gratch, Jonathan and Sidner, Candy and Artstein, Ron and Huang, Lixing and Morency, Louis-Philippe},
title = {Towards Building a Virtual Counselor: Modeling Nonverbal Behavior during Intimate Self-Disclosure},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Nonverbal behavior is considered critical for indicating intimacy and is important when designing a social virtual agent such as a counselor. One key research question is how to properly express intimate self-disclosure. In this paper we present an extensive study of human nonverbal behavior during intimate self-disclosure. This is an important milestone in creating a virtual counselor. A study of video interactions between human participants demonstrated that people display more head tilts and pauses when they revealed highly intimate information about themselves; they presented more head nods and eye gazes during less intimate sharing. An implementation of these behaviors in a virtual agent suggests that people tend to perceive head tilts, pauses and gaze aversion by the agent as conveying intimate self-disclosure. These findings are important for future research with virtual counselors and other social agents.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {63–70},
numpages = {8},
keywords = {rapport, intimacy, self-disclosure, nonverbal behavior, affective behavior, virtual agents},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.5555/2390374.2390378,
author = {Mandel, Benjamin and Culotta, Aron and Boulahanis, John and Stark, Danielle and Lewis, Bonnie and Rodrigue, Jeremy},
title = {A Demographic Analysis of Online Sentiment during Hurricane Irene},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We examine the response to the recent natural disaster Hurricane Irene on Twitter.com. We collect over 65,000 Twitter messages relating to Hurricane Irene from August 18th to August 31st, 2011, and group them by location and gender. We train a sentiment classifier to categorize messages based on level of concern, and then use this classifier to investigate demographic differences. We report three principal findings: (1) the number of Twitter messages related to Hurricane Irene in directly affected regions peaks around the time the hurricane hits that region; (2) the level of concern in the days leading up to the hurricane's arrival is dependent on region; and (3) the level of concern is dependent on gender, with females being more likely to express concern than males. Qualitative linguistic variations further support these differences. We conclude that social media analysis provides a viable, real-time complement to traditional survey methods for understanding public perception towards an impending disaster.},
booktitle = {Proceedings of the Second Workshop on Language in Social Media},
pages = {27–36},
numpages = {10},
location = {Montreal, Canada},
series = {LSM '12}
}

@inproceedings{10.5555/2392855.2392861,
author = {Green, Nancy L. and Guinn, Curry and Smith, Ronnie W.},
title = {Assisting Social Conversation between Persons with Alzheimer's Disease and Their Conversational Partners},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The number of people with dementia of the Alzheimer's type (DAT) continues to grow. One of the significant impacts of this disease is a decline in the ability to communicate using natural language. This decline in language facility often results in decreased social interaction and life satisfaction for persons with DAT and their caregivers. One possible strategy to lessen the effects of this loss of language facility is for the unaffected conversational partner (Facilitator) to "co-construct" short autobiographical stories from the life of the DAT-affected conversational partner (Storyteller). It has been observed that a skilled conversational partner can facilitate co-constructed narrative with individuals who have mild to moderate DAT. Developing a computational model of this type of co-constructed narrative would enable assistive technology to be developed that can monitor a conversation between a Storyteller and Facilitator. This technology could provide context-sensitive suggestions to an unskilled Facilitator to help maintain the flow of conversation. This paper describes a framework in which the necessary computational model of co-constructed narrative can be developed. An analysis of the fundamental elements of such a model will be presented.},
booktitle = {Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies},
pages = {37–46},
numpages = {10},
location = {Montreal, Canada},
series = {SLPAT '12}
}

@inproceedings{10.5555/2387636.2387697,
author = {Agirre, Eneko and Diab, Mona and Cer, Daniel and Gonzalez-Agirre, Aitor},
title = {SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity},
year = {2012},
abstract = {Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation &gt;80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.},
booktitle = {Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},
pages = {385–393},
numpages = {9},
location = {Montr\'{e}al, Canada},
series = {SemEval '12}
}

@inproceedings{10.5555/2391123.2391129,
author = {Cogley, James and Stokes, Nicola and Carthy, Joe and Dunnion, John},
title = {Analyzing Patient Records to Establish If and When a Patient Suffered from a Medical Condition},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The growth of digital clinical data has raised questions as to how best to leverage this data to aid the world of healthcare. Promising application areas include Information Retrieval and Question-Answering systems. Such systems require an in-depth understanding of the texts that are processed. One aspect of this understanding is knowing if a medical condition outlined in a patient record is recent, or if it occurred in the past. As well as this, patient records often discuss other individuals such as family members. This presents a second problem - determining if a medical condition is experienced by the patient described in the report or some other individual. In this paper, we investigate the suitability of a machine learning (ML) based system for resolving these tasks on a previously unexplored collection of Patient History and Physical Examination reports. Our results show that our novel Score-based feature approach outperforms the standard Linguistic and Contextual features described in the related literature. Specifically, near-perfect performance is achieved in resolving if a patient experienced a condition. While for the task of establishing when a patient experienced a condition, our ML system significantly outperforms the ConText system (87% versus 69% f-score, respectively).},
booktitle = {Proceedings of the 2012 Workshop on Biomedical Natural Language Processing},
pages = {38–46},
numpages = {9},
location = {Montreal, Canada},
series = {BioNLP '12}
}

@inproceedings{10.5555/2663700.2663703,
author = {Dubois, Daniel J.},
title = {Toward Adopting Self-Organizing Models for the <i>Gamification</i> of Context-Aware User Applications},
year = {2012},
isbn = {9781467317689},
publisher = {IEEE Press},
abstract = {Recent developments in the area of small and smart devices led to a massive spread of them, which, in some cases are replacing traditional computers for performing common activities such as web browsing. These devices are usually equipped with specialized hardware to sense and interact with the environment. In this context the use of self-organizing techniques has been widely used to provide adaptation capabilities at the low level, such as for optimizing energy consumption, or for providing some fault-tolerance properties to the communication middleware. What we want to show in this work is how the same self-organization principles can be used at the user-experience level of context-aware applications. The approach we propose shows that self-organization can be used to model the introduction of gaming elements to motivate and simplify the use of context-aware applications, thus leading to higher quality software. This work is finally validated using a case study and empirical evidence from existing popular context-aware applications.},
booktitle = {Proceedings of the Second International Workshop on Games and Software Engineering: Realizing User Engagement with Game Engineering Techniques},
pages = {9–15},
numpages = {7},
keywords = {self-organization, human computer interaction, software engineering, gamification, context-aware systems},
location = {Zurich, Switzerland},
series = {GAS '12}
}

@inproceedings{10.5555/2337159.2337172,
author = {Demme, John and Martin, Robert and Waksman, Adam and Sethumadhavan, Simha},
title = {Side-Channel Vulnerability Factor: A Metric for Measuring Information Leakage},
year = {2012},
isbn = {9781450316422},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {There have been many attacks that exploit side-effects of program execution to expose secret information and many proposed countermeasures to protect against these attacks. However there is currently no systematic, holistic methodology for understanding information leakage. As a result, it is not well known how design decisions affect information leakage or the vulnerability of systems to side-channel attacks.In this paper, we propose a metric for measuring information leakage called the Side-channel Vulnerability Factor (SVF). SVF is based on our observation that all side-channel attacks ranging from physical to microarchitectural to software rely on recognizing leaked execution patterns. SVF quantifies patterns in attackers' observations and measures their correlation to the victim's actual execution patterns and in doing so captures systems' vulnerability to side-channel attacks.In a detailed case study of on-chip memory systems, SVF measurements help expose unexpected vulnerabilities in whole-system designs and shows how designers can make performance-security trade-offs. Thus, SVF provides a quantitative approach to secure computer architecture.},
booktitle = {Proceedings of the 39th Annual International Symposium on Computer Architecture},
pages = {106–117},
numpages = {12},
location = {Portland, Oregon},
series = {ISCA '12}
}

@article{10.1145/2366231.2337172,
author = {Demme, John and Martin, Robert and Waksman, Adam and Sethumadhavan, Simha},
title = {Side-Channel Vulnerability Factor: A Metric for Measuring Information Leakage},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2366231.2337172},
doi = {10.1145/2366231.2337172},
abstract = {There have been many attacks that exploit side-effects of program execution to expose secret information and many proposed countermeasures to protect against these attacks. However there is currently no systematic, holistic methodology for understanding information leakage. As a result, it is not well known how design decisions affect information leakage or the vulnerability of systems to side-channel attacks.In this paper, we propose a metric for measuring information leakage called the Side-channel Vulnerability Factor (SVF). SVF is based on our observation that all side-channel attacks ranging from physical to microarchitectural to software rely on recognizing leaked execution patterns. SVF quantifies patterns in attackers' observations and measures their correlation to the victim's actual execution patterns and in doing so captures systems' vulnerability to side-channel attacks.In a detailed case study of on-chip memory systems, SVF measurements help expose unexpected vulnerabilities in whole-system designs and shows how designers can make performance-security trade-offs. Thus, SVF provides a quantitative approach to secure computer architecture.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {106–117},
numpages = {12}
}

@inproceedings{10.1145/2232817.2232847,
author = {Bogen, Paul L. and Furuta, Richard and Shipman, Frank},
title = {A Quantitative Evaluation of Techniques for Detection of Abnormal Change Events in Blogs.},
year = {2012},
isbn = {9781450311540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2232817.2232847},
doi = {10.1145/2232817.2232847},
abstract = {While most digital collections have limited forms of change--primarily creation and deletion of additional resources--there exists a class of digital collections that undergoes additional kinds of change. These collections are made up of resources that are distributed across the Internet and brought together into a collection via hyperlinking. Resources in these collections can be expected to change as time goes on. Part of the difficulty in maintaining these collections is determining if a changed page is still a valid member of the collection. Others have tried to address this problem by measuring change and defining a maximum allowed threshold of change, however, these methods treat all change as a potential problem and treat web content as a static document despite its intrinsically dynamic nature. Instead, we approach the significance of change on the web as a normal part of a web document's life-cycle and determine the difference between what a maintainer expects a page to do and what it actually does. In this work we evaluate the different options for extractors and analyzers in order to determine the best options from a suite of techniques. The evaluation used a human-generated ground-truth set of blog changes. The results of this work showed a statistically significant improvement over a range of traditional threshold techniques when applied to our collection of tagged blog changes.},
booktitle = {Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {157–166},
numpages = {10},
keywords = {ensemble, distributed collectionmanager, pattern recognition, kalman filters},
location = {Washington, DC, USA},
series = {JCDL '12}
}

@inproceedings{10.1145/2232817.2232857,
author = {Crane, Gregory and Almas, Bridget and Babeu, Alison and Cerrato, Lisa and Harrington, Matthew and Bamman, David and Diakoff, Harry},
title = {Student Researchers, Citizen Scholars and the Trillion Word Library},
year = {2012},
isbn = {9781450311540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2232817.2232857},
doi = {10.1145/2232817.2232857},
abstract = {The surviving corpora of Greek and Latin are relatively compact but the shift from books and written objects to digitized texts has already challenged students of these languages to move away from books as organizing metaphors and to ask, instead, what do you do with a billion, or even a trillion, words? We need a new culture of intellectual production in which student researchers and citizen scholars play a central role. And we need as a consequence to reorganize the education that we provide in the humanities, stressing participatory learning, and supporting a virtuous cycle where students contribute data as they learn and learn in order to contribute knowledge. We report on five strategies that we have implemented to further this virtuous cycle: (1) reading environments by which learners can work with languages that they have not studied, (2) feedback for those who choose to internalize knowledge about a particular language, (3) methods whereby those with knowledge of different languages can collaborate to develop interpretations and to produce new annotations, (4) dynamic reading lists that allow learners to assess and to document what they have mastered, and (5) general e-portfolios in which learners can track what they have accomplished and document what they have contributed and learned to the public or to particular groups.},
booktitle = {Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {213–222},
numpages = {10},
keywords = {human learning},
location = {Washington, DC, USA},
series = {JCDL '12}
}

@inproceedings{10.1145/2254756.2254791,
author = {Yoo, Wucherl and Larson, Kevin and Baugh, Lee and Kim, Sangkyum and Campbell, Roy H.},
title = {ADP: Automated Diagnosis of Performance Pathologies Using Hardware Events},
year = {2012},
isbn = {9781450310970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254756.2254791},
doi = {10.1145/2254756.2254791},
abstract = {Performance characterization of applications' hardware behavior is essential for making the best use of available hardware resources. Modern architectures offer access to many hardware events that are capable of providing information to reveal architectural performance bottlenecks throughout the core and memory hierarchy. These events can provide programmers with unique and powerful insights into the causes of the resource bottlenecks in their applications. However, interpreting these events has been a significant challenge. We present an automated system that uses machine learning to identify an application's performance problems. Our system provides programmers with insights about the performance of their applications while shielding them from the onerous task of digesting hardware events. It uses a decision tree algorithm, random forests on our micro-benchmarks to fingerprint the performance problems. Our system divides a profiled application into functions and automatically classifies each function by the dominant hardware resource bottlenecks. Using the classifications from the hotspot functions, we were able to achieve an average speedup of 1.73 from three applications in the PARSEC benchmark suite. Our system provides programmers with a guideline of where, what, and how to fix the detected performance problems in applications, which would have otherwise required considerable architectural knowledge.},
booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {283–294},
numpages = {12},
keywords = {hardware event, resource bottleneck, fingerprint, performance analysis, micro-benchmark, machine learning},
location = {London, England, UK},
series = {SIGMETRICS '12}
}

@article{10.1145/2318857.2254791,
author = {Yoo, Wucherl and Larson, Kevin and Baugh, Lee and Kim, Sangkyum and Campbell, Roy H.},
title = {ADP: Automated Diagnosis of Performance Pathologies Using Hardware Events},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2318857.2254791},
doi = {10.1145/2318857.2254791},
abstract = {Performance characterization of applications' hardware behavior is essential for making the best use of available hardware resources. Modern architectures offer access to many hardware events that are capable of providing information to reveal architectural performance bottlenecks throughout the core and memory hierarchy. These events can provide programmers with unique and powerful insights into the causes of the resource bottlenecks in their applications. However, interpreting these events has been a significant challenge. We present an automated system that uses machine learning to identify an application's performance problems. Our system provides programmers with insights about the performance of their applications while shielding them from the onerous task of digesting hardware events. It uses a decision tree algorithm, random forests on our micro-benchmarks to fingerprint the performance problems. Our system divides a profiled application into functions and automatically classifies each function by the dominant hardware resource bottlenecks. Using the classifications from the hotspot functions, we were able to achieve an average speedup of 1.73 from three applications in the PARSEC benchmark suite. Our system provides programmers with a guideline of where, what, and how to fix the detected performance problems in applications, which would have otherwise required considerable architectural knowledge.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {283–294},
numpages = {12},
keywords = {hardware event, micro-benchmark, performance analysis, fingerprint, resource bottleneck, machine learning}
}

@inproceedings{10.1145/2248341.2248346,
author = {Kawsar, Ferdaus A. and Haque, Md Munirul and Adibuzzaman, Mohammad and Ahamed, Sheikh Iqbal and Uddin, Md and Love, Richard and Roe, David and Dowla, Rumana and Ferdousy, T. and Selim, Reza and Hossain, Syed},
title = {E-ESAS: Improving Quality of Life for Breast Cancer Patients in Developing Countries},
year = {2012},
isbn = {9781450312929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2248341.2248346},
doi = {10.1145/2248341.2248346},
abstract = {In this paper, we present e-ESAS, a mobile phone based remote monitoring tool for patients with palliative care need, carefully designed for developing countries. Most of the current remote monitoring systems are complex, obtrusive and expensive resulting in a system unsuitable to deploy in low-income countries. We here describe evolution and performance of e-ESAS within the contexts of breast cancer patients as these patients require management of pain and other symptoms. Edmonton Symptom Assessment Scale (ESAS) was developed to capture the important parameters where patients themselves report their level of ten symptoms. Our e-ESAS improves the current system by reducing visits by patients to clinics, providing more flexibility to both doctors and patients, improving the quality of data, accommodating doctors to fine tune interventions, and providing a convenient representation of data to doctors. User interface was designed according to feedbacks from users resulting in a UI with better performance. The system is intended to provide a platform for future research as large amount of real data is being accumulated from the deployment. The system demonstrates the feasibility of accessing quality health care through cell phones by rural, poor patients in developing countries. The system enables doctors to serve more patients as it saves time for doctors, requiring less time to view patient information.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Pervasive Wireless Healthcare},
pages = {9–14},
numpages = {6},
keywords = {breast cancer, mobile computing, remote monitoring},
location = {Hilton Head, South Carolina, USA},
series = {MobileHealth '12}
}

@inproceedings{10.1145/2254064.2254083,
author = {Tripp, Omer and Manevich, Roman and Field, John and Sagiv, Mooly},
title = {JANUS: Exploiting Parallelism via Hindsight},
year = {2012},
isbn = {9781450312059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254064.2254083},
doi = {10.1145/2254064.2254083},
abstract = {This paper addresses the problem of reducing unnecessary conflicts in optimistic synchronization. Optimistic synchronization must ensure that any two concurrently executing transactions that commit are properly synchronized. Conflict detection is an approximate check for this condition. For efficiency, the traditional approach to conflict detection conservatively checks that the memory locations mutually accessed by two concurrent transactions are accessed only for reading.We present JANUS, a parallelization system that performs conflict detection by considering sequences of operations and their composite effect on the system's state. This is done efficiently, such that the runtime overhead due to conflict detection is on a par with that of write-conflict-based detection. In certain common scenarios, this mode of refinement dramatically improves the precision of conflict detection, thereby reducing the number of false conflicts.Our empirical evaluation of JANUS shows that this precision gain reduces the abort rate by an order of magnitude (22x on average), and achieves a speedup of up to 2.5x, on a suite of real-world benchmarks where no parallelism is exploited by the standard approach.},
booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {145–156},
numpages = {12},
keywords = {conflict detection, parallelization, speculative execution, transactional memory},
location = {Beijing, China},
series = {PLDI '12}
}

@article{10.1145/2345156.2254083,
author = {Tripp, Omer and Manevich, Roman and Field, John and Sagiv, Mooly},
title = {JANUS: Exploiting Parallelism via Hindsight},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2345156.2254083},
doi = {10.1145/2345156.2254083},
abstract = {This paper addresses the problem of reducing unnecessary conflicts in optimistic synchronization. Optimistic synchronization must ensure that any two concurrently executing transactions that commit are properly synchronized. Conflict detection is an approximate check for this condition. For efficiency, the traditional approach to conflict detection conservatively checks that the memory locations mutually accessed by two concurrent transactions are accessed only for reading.We present JANUS, a parallelization system that performs conflict detection by considering sequences of operations and their composite effect on the system's state. This is done efficiently, such that the runtime overhead due to conflict detection is on a par with that of write-conflict-based detection. In certain common scenarios, this mode of refinement dramatically improves the precision of conflict detection, thereby reducing the number of false conflicts.Our empirical evaluation of JANUS shows that this precision gain reduces the abort rate by an order of magnitude (22x on average), and achieves a speedup of up to 2.5x, on a suite of real-world benchmarks where no parallelism is exploited by the standard approach.},
journal = {SIGPLAN Not.},
month = jun,
pages = {145–156},
numpages = {12},
keywords = {conflict detection, parallelization, speculative execution, transactional memory}
}

@inproceedings{10.1145/2317956.2318010,
author = {Hornecker, Eva and Nicol, Emma},
title = {What Do Lab-Based User Studies Tell Us about in-the-Wild Behavior? Insights from a Study of Museum Interactives},
year = {2012},
isbn = {9781450312103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2317956.2318010},
doi = {10.1145/2317956.2318010},
abstract = {We contribute to an understanding of how well lab-based user studies can help us to anticipate how a system will be used in 'the wild'. We analyze and compare data from lab-based user studies of prototype museum installations and the subsequent deployment of these systems in a museum. While the user study was successful in identifying usability issues, social behavior patterns in the museum, in particular between caregivers and children, differed in several aspects between the settings. Our analysis highlights influences on usage and behavior patterns: the physical and structural setup, the user study creating a focused activity, and the demand characteristics of a user study.},
booktitle = {Proceedings of the Designing Interactive Systems Conference},
pages = {358–367},
numpages = {10},
keywords = {museum, CSCW, family, social behavior, user study},
location = {Newcastle Upon Tyne, United Kingdom},
series = {DIS '12}
}

@inproceedings{10.1145/2317956.2317981,
author = {Lee, Young S. and Chaysinh, Shirley and Basapur, Santosh and Metcalf, Crysta J. and Mandalia, Hiren},
title = {Active Aging in Community Centers and ICT Design Implications},
year = {2012},
isbn = {9781450312103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2317956.2317981},
doi = {10.1145/2317956.2317981},
abstract = {In recent years, the wellness of seniors has become more important than ever before in our rapidly aging society. In the U. S., approximately 11,000 community senior centers provide a broad spectrum of programs for seniors to improve their overall health and wellness in their community. Although numerous studies have reported on the various benefits of participation in such programs, little is known about how information and communications technology (ICT) can support seniors' participation in this practice. We describe findings from a two-phase qualitative study using semi-structured interviews, site visits, and focus groups with seniors and staff of senior centers located in urban and suburban areas of Chicago, IL and Tampa, FL. Based on the results, we discuss design implications for technologies that could facilitate seniors' engagement with their local community including senior centers.},
booktitle = {Proceedings of the Designing Interactive Systems Conference},
pages = {156–165},
numpages = {10},
keywords = {older adults, community engagement, health and wellness, community senior centers},
location = {Newcastle Upon Tyne, United Kingdom},
series = {DIS '12}
}

@inproceedings{10.1145/2317956.2317984,
author = {Melvin, Roberta M. and Bunt, Andrea},
title = {Designed for Work, but Not from Here: Rural and Remote Perspectives on Networked Technology},
year = {2012},
isbn = {9781450312103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2317956.2317984},
doi = {10.1145/2317956.2317984},
abstract = {While workers in an urban environment typically enjoy full speed, always available, broadband access, those in rural and remote environments do not necessarily have access to the same level of service. In this paper we describe insights from a qualitative study examining the benefits and continued challenges of using networked technologies for work purposes in rural and remote communities. Our findings indicate that work in these areas increasingly depends on networked technology to support in-situ and geographically distributed work practices, and to ameliorate health and safety issues, but that participants experience significant challenges in obtaining signal access and stability. We discuss implications for design and future research that arise from our findings.},
booktitle = {Proceedings of the Designing Interactive Systems Conference},
pages = {176–185},
numpages = {10},
keywords = {qualitative studies, networked technologies, rural and remote environments},
location = {Newcastle Upon Tyne, United Kingdom},
series = {DIS '12}
}

@inproceedings{10.1145/2317956.2318004,
author = {Wiethoff, Alexander and Gehring, Sven},
title = {Designing Interaction with Media Fa\c{c}Ades: A Case Study},
year = {2012},
isbn = {9781450312103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2317956.2318004},
doi = {10.1145/2317956.2318004},
abstract = {Media fa\c{c}ades are one prominent example of how new technologies currently augment urban spaces. At the same time, they offer new, ubiquitous opportunities for novel applications. To achieve a usable and enjoyable outcome, however, designing interaction with media fa\c{c}ades demands a structured design process. In this paper, we present our experiences designing iRiS, a system for remote interaction with media fa\c{c}ades. We approached the development following a user-centered design approach and addressing the process at two points with additional means: (1) using a purpose-built prototyping toolkit testing and exploring both, content and hardware before deploying the system on the actual fa\c{c}ade and (2) experimental use and adaptation of user experience (UX) evaluation methods to investigate the users actions and emotions more holistically in this context.},
booktitle = {Proceedings of the Designing Interactive Systems Conference},
pages = {308–317},
numpages = {10},
keywords = {interaction design, media Fa\c{c}ades, interactive lighting design},
location = {Newcastle Upon Tyne, United Kingdom},
series = {DIS '12}
}

@inproceedings{10.1145/2254129.2254149,
author = {Papadakis, George and Kawase, Ricardo and Herder, Eelco},
title = {Client- and Server-Side Revisitation Prediction with SUPRA},
year = {2012},
isbn = {9781450309158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254129.2254149},
doi = {10.1145/2254129.2254149},
abstract = {Users of collaborative applications as well as individual users in their private environment return to previously visited Web pages for various reasons; apart from pages visited due to backtracking, they typically have a number of favorite or important pages that they monitor or tasks that reoccur on an infrequent basis. In this paper, we introduce a library of methods that facilitate revisitation through the effective prediction of the next page request. It is based on a generic framework that inherently incorporates contextual information, handling uniformly both server- and the client-side applications. Unlike other existing approaches, the methods it encompasses are real-time, since they do not rely on training data or machine learning algorithms. We evaluate them over two large, real-world datasets, with the outcomes suggesting a significant improvement over methods typically used in this context. We have also made our implementation and data publicly available, thus encouraging other researchers to use it as a benchmark and to extend it with new techniques for supporting user's navigational activity.},
booktitle = {Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {12},
keywords = {contextual support, web behavior, revisitation prediction},
location = {Craiova, Romania},
series = {WIMS '12}
}

@inproceedings{10.1145/2287016.2287025,
author = {Deligiannis, Pantazis and Loidl, Hans-Wolfgang and Kouidi, Evangelia},
title = {Improving the Diagnosis of Mild Hypertrophic Cardiomyopathy with MapReduce},
year = {2012},
isbn = {9781450313438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2287016.2287025},
doi = {10.1145/2287016.2287025},
abstract = {Hypertrophic Cardiomyopathy (HCM), an inherited heart disease, is the most common cause of sudden cardiac death in young athletes. Successful diagnosis of mild HCM presents a major medical challenge, especially in athletes with exercise-induced hypertrophy that overlaps with HCM. This is due to a wide spectrum of non-specific clinical parameters and their complex dependencies. Recently, medical researchers proposed multidisciplinary strategies, defining differential diagnostic scoring algorithms, with the goal of identifying which parameters correlate with HCM in order to achieve faster and more accurate diagnosis. These algorithms require extensive testing against large medical datasets in order to identify potential correlations, and assess the overall algorithmic quality and diagnostic accuracy.We present a prototype data-parallel algorithm for improving the diagnosis of mild HCM, by refining the set of parameters contributing to the main diagnostic function. To this end, we employ a rule-based, machine-learning approach and develop an iterative MapReduce application for applying the diagnostic function on large data-sets. The core component of the algorithm, including the diagnostic function, has been implemented in Java, Pig and Hive in order to identify potential productivity gains by using a high-level MapReduce language specifically for medical applications. Finally, we assess the algorithmic performance on up to 64 cores of our Hadoop (version 0.20.1) enabled Beowulf cluster, managing to achieve near-linear speedups while reducing the overall runtime from over 9 hours to a couple of minutes for a realistic dataset of 10,000 medical records.},
booktitle = {Proceedings of Third International Workshop on MapReduce and Its Applications Date},
pages = {41–48},
numpages = {8},
keywords = {machine learning, medical diagnosis, hypertrophic cardiomyopathy, Hadoop, MapReduce},
location = {Delft, The Netherlands},
series = {MapReduce '12}
}

@inproceedings{10.1145/2287076.2287108,
author = {Dinu, Florin and Ng, T.S. Eugene},
title = {Understanding the Effects and Implications of Compute Node Related Failures in Hadoop},
year = {2012},
isbn = {9781450308052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2287076.2287108},
doi = {10.1145/2287076.2287108},
abstract = {Hadoop has become a critical component in today's cloud environment. Ensuring good performance for Hadoop is paramount for the wide-range of applications built on top of it. In this paper we analyze Hadoop's behavior under failures involving compute nodes. We find that even a single failure can result in inflated, variable and unpredictable job running times, all undesirable properties in a distributed system. We systematically track the causes underlying this distressing behavior. First, we find that Hadoop makes unrealistic assumptions about task progress rates. These assumptions can be easily invalidated by the cloud environment and, more surprisingly, by Hadoop's own design decisions. The result are significant inefficiencies in Hadoop's speculative execution algorithm. Second, failures are re-discovered individually by each task at the cost of great degradation in job running time. The reason is that Hadoop focuses on extreme scalability and thus trades off possible improvements resulting from sharing failure information between tasks. Third, Hadoop does not consider the causes of connection failures between its tasks. We show that the resulting overloading of connection failure semantics unnecessarily causes an otherwise localized failure to propagate to healthy tasks. We also discuss the implications of our findings and draw attention to new ways of improving Hadoop-like frameworks.},
booktitle = {Proceedings of the 21st International Symposium on High-Performance Parallel and Distributed Computing},
pages = {187–198},
numpages = {12},
keywords = {hadoop, failures, speculative execution},
location = {Delft, The Netherlands},
series = {HPDC '12}
}

@inproceedings{10.1145/2483954.2483962,
author = {Ruan, Yang and Guo, Zhenhua and Zhou, Yuduo and Qiu, Judy and Fox, Geoffrey},
title = {HyMR: A Hybrid MapReduce Workflow System},
year = {2012},
isbn = {9781450313391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483954.2483962},
doi = {10.1145/2483954.2483962},
abstract = {Many distributed computing models have been developed for high performance processing of large scale scientific data. Among them, MapReduce is a popular and widely used fine grain parallel runtime. Workflows integrate and coordinate distributed and heterogeneous components to solve the computation problem which may contain several MapReduce jobs. However, existing workflow solutions have limited supports for important features such as fault tolerance and efficient execution for iterative applications. In this paper, we propose HyMR: a hybrid MapReduce workflow system based on two different MapReduce frameworks. HyMR optimizes scheduling for individual jobs and supports fault tolerance for the entire workflow pipeline. A distributed file system is used for fast data sharing between jobs. We compare a pipeline using HyMR with the workflow model based on a single MapReduce framework. Our results show that the hybrid model achieves a higher efficiency.},
booktitle = {Proceedings of the 3rd International Workshop on Emerging Computational Methods for the Life Sciences},
pages = {39–48},
numpages = {10},
keywords = {MapReduce, iterative algorithms, workflow management},
location = {Delft, The Netherlands},
series = {ECMLS '12}
}

@inproceedings{10.1145/2287036.2287042,
author = {Zhao, Zhou and Hwang, Kai and Villeta, Jose},
title = {Game Cloud Design with Virtualized CPU/GPU Servers and Initial Performance Results},
year = {2012},
isbn = {9781450313407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2287036.2287042},
doi = {10.1145/2287036.2287042},
abstract = {Cloud gaming provides game-on-demand (GoD) services over the Internet cloud. The goal is to achieve faster response time and higher QoS. The video game is rendered remotely on the game cloud and decoded on thin client devices such as a tablet computer or a smartphone. We design a game cloud with a virtualized cluster of CPU/GPU servers at USC GamePipe Laboratory. We enable interactive gaming by taking full advantage of the cloud and local resources for high quality of experience (QoE) gaming.We report some preliminary performance results on game latency and frame rate. We find 109 ~ 131 ms latency in using the game cloud, which is 14 ~ 38% lower than 200 ms latency often experienced on a thin local computer. Moreover, the frame rate from the cloud is 25 ~ 35% higher than that of using a client computer alone. Base on these results, we anticipate game cloud has a performance gain or QoS improvement of 14 ~ 38% against that of using a thin client mobile device. Potential applications of the game cloud for high-performance scientific computing are also discussed in the paper.},
booktitle = {Proceedings of the 3rd Workshop on Scientific Cloud Computing},
pages = {23–30},
numpages = {8},
keywords = {and scientific clouds, visualization, remote rendering, virtual machine, game cloud, game benchmarking},
location = {Delft, The Netherlands},
series = {ScienceCloud '12}
}

@inproceedings{10.1145/2380718.2380724,
author = {Bhattacharya, Sanmitra and Tran, Hung and Srinivasan, Padmini and Suls, Jerry},
title = {Belief Surveillance with Twitter},
year = {2012},
isbn = {9781450312288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380718.2380724},
doi = {10.1145/2380718.2380724},
abstract = {Data from social media systems are being actively mined for trends and patterns of interests. Problems such as sentiment and opinion mining, and prediction of election outcomes have become tremendously popular due to the unprecedented availability of social interactivity data of different types. An important angle that has not yet been explored is to estimate beliefs from posts made on social media. We propose that social media can be used to monitor the level of belief, disbelief and doubt related to specific propositions. Inspired by efforts in disease surveillance using social media we coin the term belief surveillance for this function. We propose a novel methodological framework for belief surveillance using Twitter. Our method may be used to gauge belief on any proposition as long as it is specifiable in a form that we call probes. We present our belief estimates for 32 probes some of which represent factual information, others represent false information and the remaining represent debatable propositions. Finally, we provide preliminary evidence suggesting that off-the-shelf classifiers may be used to automatically estimate belief.},
booktitle = {Proceedings of the 4th Annual ACM Web Science Conference},
pages = {43–46},
numpages = {4},
keywords = {microblogs, public belief analysis, large-scale data collection},
location = {Evanston, Illinois},
series = {WebSci '12}
}

@inproceedings{10.1145/2380718.2380747,
author = {O'Hara, Kieron},
title = {Transparency, Open Data and Trust in Government: Shaping the Infosphere},
year = {2012},
isbn = {9781450312288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380718.2380747},
doi = {10.1145/2380718.2380747},
abstract = {This paper examines the ways in which using the World Wide Web to promote transparency and to disseminate open data will affect warranted and unwarranted trust in politics and within societies. It is argued that transparency and open data will be damaging for unwarranted trust, but this will open up a space for warranted trust to flourish. Three types of theory about trust and decision-making in politics are discussed: social capital theories, rational choice theories and deliberative democracy theories. Using the UK government's transparency programme in crime and criminal justice as an example, it is argued that mechanisms being pioneered to disseminate open data online, such as sites like data.gov and data.gov.uk, promote trust on each theory, although the supply of data is necessary but not sufficient. It is also necessary to consider the wider infosphere, putting deliberative processes in place to foster trust.},
booktitle = {Proceedings of the 4th Annual ACM Web Science Conference},
pages = {223–232},
numpages = {10},
keywords = {trust, criminal justice data, crime data, transparency, social capital, rational choice, open data, deliberative democracy},
location = {Evanston, Illinois},
series = {WebSci '12}
}

@inproceedings{10.1145/2380718.2380739,
author = {Kosinski, Michal and Bachrach, Yoram and Kasneci, Gjergji and Van-Gael, Jurgen and Graepel, Thore},
title = {Crowd IQ: Measuring the Intelligence of Crowdsourcing Platforms},
year = {2012},
isbn = {9781450312288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380718.2380739},
doi = {10.1145/2380718.2380739},
abstract = {We measure crowdsourcing performance based on a standard IQ questionnaire, and examine Amazon's Mechanical Turk (AMT) performance under different conditions. These include variations of the payment amount offered, the way incorrect responses affect workers' reputations, threshold reputation scores of participating AMT workers, and the number of workers per task. We show that crowds composed of workers of high reputation achieve higher performance than low reputation crowds, and the effect of the amount of payment is non-monotone---both paying too much and too little affects performance. Furthermore, higher performance is achieved when the task is designed such that incorrect responses can decrease workers' reputation scores. Using majority vote to aggregate multiple responses to the same task can significantly improve performance, which can be further boosted by dynamically allocating workers to tasks in order to break ties.},
booktitle = {Proceedings of the 4th Annual ACM Web Science Conference},
pages = {151–160},
numpages = {10},
keywords = {incentive schemes, psychometrics, crowdsourcing},
location = {Evanston, Illinois},
series = {WebSci '12}
}

@inproceedings{10.1145/2380718.2380742,
author = {Lu, Xuesong and Cheliotis, Giorgos and Cao, Xiyue and Song, Yi and Bressan, St\'{e}phane},
title = {The Configuration of Networked Publics on the Web: Evidence from the Greek Indignados Movement},
year = {2012},
isbn = {9781450312288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380718.2380742},
doi = {10.1145/2380718.2380742},
abstract = {The Internet, and social media in particular, are frequently credited in public discourse with being instrumental for the development and coordination of various contemporary social movements. We examine the evolution of Facebook activity in relation to the movement of the Greek Indignados of 2011, by collecting the electronic traces of their public communications on Facebook pages for a period of 8 months. We analyze the resulting bipartite graphs consisting of users posting to pages, using social network analysis. We reveal some of the dynamics of structural properties of the network over time and explain what these mean for the configuration of networked publics on social network sites. We conclude that the very early stages of activity are essential in determining this configuration, because users converge quickly and exclusively on a small number of pages. When gradually activity is reduced, the reduction is strongest in the most active users and the most popular pages, but when activity resumes, users return to the same pages. We discuss implications for the organization of collective action on social network sites.},
booktitle = {Proceedings of the 4th Annual ACM Web Science Conference},
pages = {185–194},
numpages = {10},
location = {Evanston, Illinois},
series = {WebSci '12}
}

@inproceedings{10.1145/2380718.2380746,
author = {Nguyen, Nam P. and Yan, Guanhua and Thai, My T. and Eidenbenz, Stephan},
title = {Containment of Misinformation Spread in Online Social Networks},
year = {2012},
isbn = {9781450312288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380718.2380746},
doi = {10.1145/2380718.2380746},
abstract = {With their blistering expansions in recent years, popular on-line social sites such as Twitter, Facebook and Bebo, have become some of the major news sources as well as the most effective channels for viral marketing nowadays. However, alongside these promising features comes the threat of misinformation propagation which can lead to undesirable effects, such as the widespread panic in the general public due to faulty swine flu tweets on Twitter in 2009. Due to the huge magnitude of online social network (OSN) users and the highly clustered structures commonly observed in these kinds of networks, it poses a substantial challenge to efficiently contain viral spread of misinformation in large-scale social networks.In this paper, we focus on how to limit viral propagation of misinformation in OSNs. Particularly, we study a set of problems, namely the β1T -- Node Protectors, which aims to find the smallest set of highly influential nodes whose decontamination with good information helps to contain the viral spread of misinformation, initiated from the set I, to a desired ratio (1 − β) in T time steps. In this family set, we analyze and present solutions including inapproximability result, greedy algorithms that provide better lower bounds on the number of selected nodes, and a community-based heuristic method for the Node Protector problems. To verify our suggested solutions, we conduct experiments on real world traces including NetHEPT, NetHEPT_WC and Facebook networks. Empirical results indicate that our methods are among the best ones for hinting out those important nodes in comparison with other available methods.},
booktitle = {Proceedings of the 4th Annual ACM Web Science Conference},
pages = {213–222},
numpages = {10},
keywords = {misinformation containment, online social networks},
location = {Evanston, Illinois},
series = {WebSci '12}
}

@inproceedings{10.1145/2305484.2305524,
author = {de S\'{a}, Marco and Carri\c{c}o, Lu\'{\i}s},
title = {Fear Therapy for Children: A Mobile Approach},
year = {2012},
isbn = {9781450311687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2305484.2305524},
doi = {10.1145/2305484.2305524},
abstract = {Mobile devices have shown to be useful tools in supporting various procedures and therapy approaches for different purposes. However, when applied to children, particular care has to be taken, considering both their abilities and their acceptance towards the used approaches. In this paper we present mobile applications, designed specifically for children and young patients, aiming at supporting fear therapy procedures. The software was developed following a user centered design approach and offers users an intuitive and metaphor based interaction paradigm that overcomes the paper-based counterpart's limitations. We describe the design process, the software and the results that we have obtained during an exploratory trial study.},
booktitle = {Proceedings of the 4th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {237–246},
numpages = {10},
keywords = {mobile devices, user centered design, fear and anxiety therapy},
location = {Copenhagen, Denmark},
series = {EICS '12}
}

@inproceedings{10.1145/2307863.2307866,
author = {Wiese, Jason and Hong, Jason I. and Zimmerman, John},
title = {Building a Dynamic and Computational Understanding of Personal Social Networks},
year = {2012},
isbn = {9781450313247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307863.2307866},
doi = {10.1145/2307863.2307866},
abstract = {While individuals' personal social networks are extremely important in their day-to-day lives, computational systems lack meaningful representations of them. We argue that recent trends in computer-mediated communication, the ubiquity of smartphones, usage of online services, and new approaches to real-world social science experimentation have created an opportunity to dynamically generate representations of personal social networks that will be useful in a variety of application areas. We describe several preliminary steps we have taken to investigate this vision, which demonstrate that the approach appears to be feasible and seems likely to produce useful results. While there are significant privacy concerns in this space, we outline two approaches for dealing with them. Finally, we close with a discussion of several application areas that might benefit from this new process for generating representations of personal social networks.},
booktitle = {Proceedings of the 1st ACM Workshop on Mobile Systems for Computational Social Science},
pages = {5–10},
numpages = {6},
keywords = {personal social network, privacy, smartphone, data mining},
location = {Low Wood Bay, Lake District, UK},
series = {MCSS '12}
}

@inproceedings{10.1145/2307636.2307641,
author = {Lee, Youngki and Ju, Younghyun and Min, Chulhong and Kang, Seungwoo and Hwang, Inseok and Song, Junehwa},
title = {CoMon: Cooperative Ambience Monitoring Platform with Continuity and Benefit Awareness},
year = {2012},
isbn = {9781450313018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307636.2307641},
doi = {10.1145/2307636.2307641},
abstract = {Mobile applications that sense continuously, such as location monitoring, are emerging. Despite their usefulness, their adoption in real-world deployment situations has been extremely slow. Many smartphone users are turned away by the drastic battery drain caused by continuous sensing and processing. Also, the extractable contexts from the phone are quite limited due to its position and sensing modalities. In this paper, we propose CoMon, a novel cooperative ambience monitoring platform, which newly addresses the energy problem through opportunistic cooperation among nearby mobile users. To maximize the benefit of cooperation, we develop two key techniques, (1) continuity-aware cooperator detection and (2) benefit-aware negotiation. The former employs heuristics to detect cooperators who will remain in the vicinity for a long period of time, while the latter automatically devises a cooperation plan that provides mutual benefit to cooperators, while considering running applications, available devices, and user policies. Through continuity- and benefit-aware operation, CoMon enables applications to monitor the environment at much lower energy consumption. We implement and deploy a CoMon prototype and show that it provides significant benefit for mobile sensing applications.},
booktitle = {Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services},
pages = {43–56},
numpages = {14},
keywords = {sensing, context, cooperation, ambience, energy, comon},
location = {Low Wood Bay, Lake District, UK},
series = {MobiSys '12}
}

@inproceedings{10.1145/2305484.2305521,
author = {Pereira, Lucas and Quintal, Filipe and Nunes, Nuno and Berg\'{e}s, Mario},
title = {The Design of a Hardware-Software Platform for Long-Term Energy Eco-Feedback Research},
year = {2012},
isbn = {9781450311687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2305484.2305521},
doi = {10.1145/2305484.2305521},
abstract = {Researchers often face engineering problems, such as optimizing prototype costs and ensuring easy access to the collected data, which are not directly related to the research problems being studied. This is especially true when dealing with long-term studies in real world scenarios. This paper describes the engineering perspective of the design, development and deployment of a long-term real word study on energy eco-feedback, where a non-intrusive home energy monitor was deployed in 30 houses for 18 months. Here we report on the efforts required to implement a cost-effective non-intrusive energy monitor and, in particular, the construction of a local network to allow remote access to multiple monitors and the creation of a RESTful web-service to enable the integration of these monitors with social media and mobile software applications. We conclude with initial results from a few eco-feedback studies that were performed using this platform.},
booktitle = {Proceedings of the 4th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {221–230},
numpages = {10},
keywords = {sustainability, eco-feedback, hardware-software platform, non-intrusive load monitoring},
location = {Copenhagen, Denmark},
series = {EICS '12}
}

@inproceedings{10.1145/2307636.2307665,
author = {Sorber, Jacob M. and Shin, Minho and Peterson, Ron and Kotz, David},
title = {Plug-n-Trust: Practical Trusted Sensing for Mhealth},
year = {2012},
isbn = {9781450313018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307636.2307665},
doi = {10.1145/2307636.2307665},
abstract = {Mobile computing and sensing technologies present exciting opportunities for healthcare. Prescription wireless sensors worn by patients can automatically deliver medical data to care providers, dramatically improving their ability to diagnose, monitor, and manage a range of medical conditions. Using the mobile phones that patients already carry to provide connectivity between sensors and providers is essential to keeping costs low and deployments simple. Unfortunately, software-based attacks against phones are also on the rise, and successful attacks on privacy-sensitive and safety-critical applications can have significant consequences for patients.In this paper, we describe Plug-n-Trust (PnT), a novel approach to protecting both the confidentiality and integrity of safety-critical medical sensing and data processing on vulnerable mobile phones. With PnT, a plug-in smart card provides a trusted computing environment, keeping data safe even on a compromised mobile phone. By design, PnT is simple to use and deploy, while providing a flexible programming interface amenable to a wide range of applications. We describe our implementation, designed for Java-based smart cards and Android phones, in which we use a split-computation model with a novel path hashing technique to verify proper behavior without exposing confidential data. Our experimental evaluation demonstrates that PnT achieves its security goals while incurring acceptable overhead.},
booktitle = {Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services},
pages = {309–322},
numpages = {14},
keywords = {smartcards, smartphones, trusted computing, mhealth},
location = {Low Wood Bay, Lake District, UK},
series = {MobiSys '12}
}

@inproceedings{10.1145/2305484.2305490,
author = {Dumas, Bruno and Signer, Beat and Lalanne, Denis},
title = {Fusion in Multimodal Interactive Systems: An HMM-Based Algorithm for User-Induced Adaptation},
year = {2012},
isbn = {9781450311687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2305484.2305490},
doi = {10.1145/2305484.2305490},
abstract = {Multimodal interfaces have shown to be ideal candidates for interactive systems that adapt to a user either automatically or based on user-defined rules. However, user-based adaptation demands for the corresponding advanced software architectures and algorithms. We present a novel multimodal fusion algorithm for the development of adaptive interactive systems which is based on hidden Markov models (HMM). In order to select relevant modalities at the semantic level, the algorithm is linked to temporal relationship properties. The presented algorithm has been evaluated in three use cases from which we were able to identify the main challenges involved in developing adaptive multimodal interfaces.},
booktitle = {Proceedings of the 4th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {15–24},
numpages = {10},
keywords = {user interface adaptation, hmm-based fusion, multimodal fusion, multimodal interaction},
location = {Copenhagen, Denmark},
series = {EICS '12}
}

@inproceedings{10.1145/2307849.2307862,
author = {Bahl, Paramvir and Philipose, Matthai and Zhong, Lin},
title = {VISION: Cloud-Powered Sight for All: Showing the Cloud What You See},
year = {2012},
isbn = {9781450313193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307849.2307862},
doi = {10.1145/2307849.2307862},
abstract = {We argue that for computers to do more for us, we need to show the cloud what we see and embrace cloud-powered sight for mobile users. We present sample applications that will be empowered by this vision, discuss why the timing is right to tackle it, and offer our initial thoughts on some of the important research challenges.},
booktitle = {Proceedings of the Third ACM Workshop on Mobile Cloud Computing and Services},
pages = {53–60},
numpages = {8},
keywords = {wearable computing, cloud, computer vision, camera, mobile computing},
location = {Low Wood Bay, Lake District, UK},
series = {MCS '12}
}

@inproceedings{10.1145/2304696.2304702,
author = {Ali, Nour and Rosik, Jacek and Buckley, Jim},
title = {Characterizing Real-Time Reflexion-Based Architecture Recovery: An in-Vivo Multi-Case Study},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304702},
doi = {10.1145/2304696.2304702},
abstract = {Architecting software systems is an integral part of the software development lifecycle. However, often the implementation of the resultant software ends up diverging from the designed architecture due to factors such as time pressures on the development team during implementation/evolution, or the lack of architectural awareness on the part of (possibly new) programmers. In such circumstances, the quality requirements addressed by the as-designed architecture are likely to be unaddressed by the as-implemented system.This paper reports on in-vivo case studies of the ACTool, a tool which supports real-time Reflexion Modeling for architecture recovery and on-going consistency. It describes our experience conducting architectural recovery sessions on three deployed, commercial software systems in two companies with the tool, as a first step towards ongoing architecture consistency in these systems. Our findings provide the first in-depth characterization of real-time Reflexion-based architectural recovery in practice, highlighting the architectural recovery agendas at play, the modeling approaches employed, the mapping approaches employed and characterizing the inconsistencies encountered. Our findings also discuss the usefulness of the ACTool for these companies.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {architecture evolution, actool, architecture consistency, in-vivo multi case study, architecture recovery, reflexion modeling, architectural conformance},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@inproceedings{10.1145/2304576.2304611,
author = {Joubert, Wayne and Su, Shi-Quan},
title = {An Analysis of Computational Workloads for the ORNL Jaguar System},
year = {2012},
isbn = {9781450313162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304576.2304611},
doi = {10.1145/2304576.2304611},
abstract = {This study presents an analysis of science application workloads for the Jaguar Cray XT5 system during its tenure as a 2.3 petaflop supercomputer at Oak Ridge National Laboratory. Jaguar was the first petascale system to be deployed for open science and has been one of the world's top three supercomputers for six releases of the TOP500 list. Its workload is investigated here as a representative of the growing worldwide install base of petascale systems and also as a foreshadowing of science workloads to be expected for future systems. The study considers characteristics of the Jaguar workload such as resource utilization, typical job characteristics, most heavily used applications, application scaling and application usage patterns. Implications of these findings are considered for current petascale workflows and for exascale systems to be deployed later this decade.},
booktitle = {Proceedings of the 26th ACM International Conference on Supercomputing},
pages = {247–256},
numpages = {10},
keywords = {exascale, scaling, metrics, cray, hpc, workload, applications, petascale, science, ornl},
location = {San Servolo Island, Venice, Italy},
series = {ICS '12}
}

@article{10.1145/2317307.2317313,
author = {claffy, kc},
title = {Border Gateway Protocol (BGP) and Traceroute Data Workshop Report},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2317307.2317313},
doi = {10.1145/2317307.2317313},
abstract = {On Monday, 22 August 2011, CAIDA hosted a one-day workshop to discuss scalable measurement and analysis of BGP and traceroute topology data, and practical applications of such data analysis including tracking of macroscopic censorship and filtering activities on the Internet. Discussion topics included: the surprisingly stability in the number of BGP updates over time; techniques for improving measurement and analysis of inter-domain routing policies; an update on Colorado State's BGPMon instrumentation; using BGP data to improve the interpretation of traceroute data, both for real-time diagnostics (e.g., AS traceroute) and for large-scale topology mapping; using both BGP and traceroute data to support detection and mapping infrastructure integrity, including different types of of filtering and censorship; and use of BGP data to analyze existing and proposed approaches to securing the interdomain routing system. This report briefly summarizes the presentations and discussions that followed.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = jun,
pages = {28–31},
numpages = {4},
keywords = {data analysis, internet measurement techniques, topology, censorship, filtering, routing, validation}
}

@article{10.14778/2350229.2350244,
author = {Hueske, Fabian and Peters, Mathias and Sax, Matthias J. and Rheinl\"{a}nder, Astrid and Bergmann, Rico and Krettek, Aljoscha and Tzoumas, Kostas},
title = {Opening the Black Boxes in Data Flow Optimization},
year = {2012},
issue_date = {July 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2350229.2350244},
doi = {10.14778/2350229.2350244},
abstract = {Many systems for big data analytics employ a data flow abstraction to define parallel data processing tasks. In this setting, custom operations expressed as user-defined functions are very common. We address the problem of performing data flow optimization at this level of abstraction, where the semantics of operators are not known. Traditionally, query optimization is applied to queries with known algebraic semantics. In this work, we find that a handful of properties, rather than a full algebraic specification, suffice to establish reordering conditions for data processing operators. We show that these properties can be accurately estimated for black box operators by statically analyzing the general-purpose code of their user-defined functions.We design and implement an optimizer for parallel data flows that does not assume knowledge of semantics or algebraic properties of operators. Our evaluation confirms that the optimizer can apply common rewritings such as selection reordering, bushy join-order enumeration, and limited forms of aggregation push-down, hence yielding similar rewriting power as modern relational DBMS optimizers. Moreover, it can optimize the operator order of nonrelational data flows, a unique feature among today's systems.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1256–1267},
numpages = {12}
}

@article{10.14778/2350229.2350272,
author = {Dittrich, Jens and Quian\'{e}-Ruiz, Jorge-Arnulfo and Richter, Stefan and Schuh, Stefan and Jindal, Alekh and Schad, J\"{o}rg},
title = {Only Aggressive Elephants Are Fast Elephants},
year = {2012},
issue_date = {July 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2350229.2350272},
doi = {10.14778/2350229.2350272},
abstract = {Yellow elephants are slow. A major reason is that they consume their inputs entirely before responding to an elephant rider's orders. Some clever riders have trained their yellow elephants to only consume parts of the inputs before responding. However, the teaching time to make an elephant do that is high. So high that the teaching lessons often do not pay off. We take a different approach. We make elephants aggressive; only this will make them very fast.We propose HAIL (Hadoop Aggressive Indexing Library), an enhancement of HDFS and Hadoop MapReduce that dramatically improves runtimes of several classes of MapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create different clustered indexes on each data block replica. An interesting feature of HAIL is that we typically create a win-win situation: we improve both data upload to HDFS and the runtime of the actual Hadoop MapReduce job. In terms of data upload, HAIL improves over HDFS by up to 60% with the default replication factor of three. In terms of query execution, we demonstrate that HAIL runs up to 68x faster than Hadoop. In our experiments, we use six clusters including physical and EC2 clusters of up to 100 nodes. A series of scalability experiments also demonstrates the superiority of HAIL.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1591–1602},
numpages = {12}
}

@article{10.1145/2240156.2240163,
author = {Black, Rolf and Waller, Annalu and Turner, Ross and Reiter, Ehud},
title = {Supporting Personal Narrative for Children with Complex Communication Needs},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/2240156.2240163},
doi = {10.1145/2240156.2240163},
abstract = {Children with complex communication needs who use voice output communication aids seldom engage in extended conversation. The “How was School today...?” system has been designed to enable such children to talk about their school day. The system uses data-to-text technology to generate narratives from sensor data. Observations, interviews and prototyping were used to ensure that stakeholders were involved in the design of the system. Evaluations with three children showed that the prototype system, which automatically generates utterances, has the potential to support disabled individuals to participate better in interactive conversation. Analysis of a conversational transcript and observations indicate that the children were able to access relevant conversation and had more control in the conversation in comparison to their usual interactions where control lay mainly with the speaking partner. Further research to develop an improved, more rugged system that supports users with different levels of language ability is now underway.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jul,
articleno = {15},
numpages = {35},
keywords = {scanning selection, language development, disability, accessibility, cerebral palsy, single-switch input, personal narrative, Augmentative and alternative communication, assistive technology}
}

@inproceedings{10.1145/2379256.2379260,
author = {Dhillon, Jaspaljeet Singh and W\"{u}nsche, Burkhard C. and Lutteroth, Christof},
title = {Evaluation of a Web-Based Telehealth System: A Preliminary Investigation with Seniors in New Zealand},
year = {2012},
isbn = {9781450314749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2379256.2379260},
doi = {10.1145/2379256.2379260},
abstract = {Home telehealth systems are gaining popularity among seniors, but they are mostly doctor centric, focused on managing diseases instead of preventing them, and do not take into account the social needs of the users. Increasing numbers of seniors going online opens up opportunities to address the shortcomings of current telehealth systems. We have developed a patient-centric, web-based telehealth system which uses Web 2.0 technologies to add social support and user defined content. In this study, we have evaluated the functionalities, usability and user interface of the functional prototype with eight seniors of age range 67 to 90 by using a multi-method approach involving individual walkthrough, system usability scale (SUS), protocol analysis and interviews. Overall, users were satisfied with the usability of the system and functionalities promoting exercises and supporting weight management were in most demand. The evaluation of our prototype demonstrates that combining telehealth functionalities with social component and user-generated content is a promising way to enable users to proactively manage and improve their health.},
booktitle = {Proceedings of the 13th International Conference of the NZ Chapter of the ACM's Special Interest Group on Human-Computer Interaction},
pages = {17–24},
numpages = {8},
keywords = {seniors, telehealth, human computer interfaces},
location = {Dunedin, New Zealand},
series = {CHINZ '12}
}

@inproceedings{10.5555/2392800.2392844,
author = {Ha, Eun Young and Grafsgaard, Joseph F. and Mitchell, Christopher M. and Boyer, Kristy Elizabeth and Lester, James C.},
title = {Combining Verbal and Nonverbal Features to Overcome the 'information Gap' in Task-Oriented Dialogue},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Dialogue act modeling in task-oriented dialogue poses significant challenges. It is particularly challenging for corpora consisting of two interleaved communication streams: a dialogue stream and a task stream. In such corpora, information can be conveyed implicitly by the task stream, yielding a dialogue stream with seemingly missing information. A promising approach leverages rich resources from both the dialog and the task streams, combining verbal and non-verbal features. This paper presents work on dialogue act modeling that leverages body posture, which may be indicative of particular dialogue acts. Combining three information sources (dialogue exchanges, task context, and users' posture), three types of machine learning frameworks were compared. The results indicate that some models better preserve the structure of task-oriented dialogue than others, and that automatically recognized postural features may help to disambiguate user dialogue moves.},
booktitle = {Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
pages = {247–256},
numpages = {10},
location = {Seoul, South Korea},
series = {SIGDIAL '12}
}

@inproceedings{10.5555/2392800.2392837,
author = {Ward, Nigel G. and Vega, Alejandro},
title = {A Bottom-up Exploration of the Dimensions of Dialog State in Spoken Interaction},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Models of dialog state are important, both scientifically and practically, but today's best build strongly on tradition. This paper presents a new way to identify the important dimensions of dialog state, more bottom-up and empirical than previous approaches. Specifically, we applied Principal Component Analysis to a large number of low-level prosodic features to find the most important dimensions of variation. The top 20 out of 76 dimensions accounted for 81% of the variance, and each of these dimensions clearly related to dialog states and activities, including turn taking, topic structure, grounding, empathy, cognitive processes, attitude and rhetorical structure.},
booktitle = {Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
pages = {198–206},
numpages = {9},
location = {Seoul, South Korea},
series = {SIGDIAL '12}
}

@inproceedings{10.1145/2330163.2330316,
author = {Kowaliw, Taras and Banzhaf, Wolfgang},
title = {The Unconstrained Automated Generation of Cell Image Features for Medical Diagnosis},
year = {2012},
isbn = {9781450311779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330163.2330316},
doi = {10.1145/2330163.2330316},
abstract = {An extension to a non-linear offline method for generating features for image recognition is introduced. It aims at generating low-level features automatically when provided with some arbitrary image database. First, a general representation of prioritized pixel-neighbourhoods is described. Next, genetic programming is used to specify functions on those representations. The result is a set of transformations on the space of grayscale images. These transforms are utilized as a step in a classification process, and evolved in an evolutionary algorithm. The technique is shown to match the efficiency of the state-of-the-art on a medical image classification task. Further, the approach is shown to self-select an appropriate solution structure and complexity. Finally, we show that competitive co-evolution is a viable means of combating over-fitting. It is concluded that the technique generally shows good promise for the creation of novel image features in situations where pixel-level features are complex or unknown, such as medical images.},
booktitle = {Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation},
pages = {1103–1110},
numpages = {8},
keywords = {genetic programming, image processing, tef, co-evolution, muscular dystrophy, pattern recognition},
location = {Philadelphia, Pennsylvania, USA},
series = {GECCO '12}
}

@inproceedings{10.5555/2390524.2390640,
author = {Zhang, Ce and Niu, Feng and R\'{e}, Christopher and Shavlik, Jude},
title = {Big Data versus the Crowd: Looking for Relationships in All the Right Places},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1},
pages = {825–834},
numpages = {10},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@inproceedings{10.5555/2390524.2390609,
author = {Zweig, Geoffrey and Platt, John C. and Meek, Christopher and Burges, Christopher J. C. and Yessenalina, Ainur and Liu, Qiang},
title = {Computational Approaches to Sentence Completion},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper studies the problem of sentence-level semantic coherence by answering SAT-style sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1},
pages = {601–610},
numpages = {10},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@inproceedings{10.5555/2390524.2390526,
author = {Duh, Kevin and Sudoh, Katsuhito and Wu, Xianchao and Tsukada, Hajime and Nagata, Masaaki},
title = {Learning to Translate with Multiple Objectives},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality.Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1},
pages = {1–10},
numpages = {10},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@inproceedings{10.5555/2390507.2390510,
author = {Anderson, Ashton and McFarland, Dan and Jurafsky, Dan},
title = {Towards a Computational History of the ACL: 1980-2008},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We develop a people-centered computational history of science that tracks authors over topics and apply it to the history of computational linguistics. We present four findings in this paper. First, we identify the topical subfields authors work on by assigning automatically generated topics to each paper in the ACL Anthology from 1980 to 2008. Next, we identify four distinct research epochs where the pattern of topical overlaps are stable and different from other eras: an early NLP period from 1980 to 1988, the period of US government-sponsored MUC and ATIS evaluations from 1989 to 1994, a transitory period until 2001, and a modern integration period from 2002 onwards. Third, we analyze the flow of authors across topics to discern how some subfields flow into the next, forming different stages of ACL research. We find that the government-sponsored bakeoffs brought new researchers to the field, and bridged early topics to modern probabilistic approaches. Last, we identify steep increases in author retention during the bakeoff era and the modern era, suggesting two points at which the field became more integrated.},
booktitle = {Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries},
pages = {13–21},
numpages = {9},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@inproceedings{10.1145/2335356.2335358,
author = {Chin, Erika and Felt, Adrienne Porter and Sekar, Vyas and Wagner, David},
title = {Measuring User Confidence in Smartphone Security and Privacy},
year = {2012},
isbn = {9781450315326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335356.2335358},
doi = {10.1145/2335356.2335358},
abstract = {In order to direct and build an effective, secure mobile ecosystem, we must first understand user attitudes toward security and privacy for smartphones and how they may differ from attitudes toward more traditional computing systems. What are users' comfort levels in performing different tasks? How do users select applications? What are their overall perceptions of the platform? This understanding will help inform the design of more secure smartphones that will enable users to safely and confidently benefit from the potential and convenience offered by mobile platforms.To gain insight into user perceptions of smartphone security and installation habits, we conduct a user study involving 60 smartphone users. First, we interview users about their willingness to perform certain tasks on their smartphones to test the hypothesis that people currently avoid using their phones due to privacy and security concerns. Second, we analyze why and how they select applications, which provides information about how users decide to trust applications. Based on our findings, we present recommendations and opportunities for services that will help users safely and confidently use mobile applications and platforms.},
booktitle = {Proceedings of the Eighth Symposium on Usable Privacy and Security},
articleno = {1},
numpages = {16},
keywords = {application installation, mobile phone usage, laptop usage, smartphones},
location = {Washington, D.C.},
series = {SOUPS '12}
}

@inproceedings{10.1145/2335356.2335369,
author = {Johnson, Maritza and Egelman, Serge and Bellovin, Steven M.},
title = {Facebook and Privacy: It's Complicated},
year = {2012},
isbn = {9781450315326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335356.2335369},
doi = {10.1145/2335356.2335369},
abstract = {We measure users' attitudes toward interpersonal privacy concerns on Facebook and measure users' strategies for reconciling their concerns with their desire to share content online. To do this, we recruited 260 Facebook users to install a Facebook application that surveyed their privacy concerns, their friend network compositions, the sensitivity of posted content, and their privacy-preserving strategies. By asking participants targeted questions about people randomly selected from their friend network and posts shared on their profiles, we were able to quantify the extent to which users trust their "friends" and the likelihood that their content was being viewed by unintended audiences. We found that while strangers are the most concerning audience, almost 95% of our participants had taken steps to mitigate those concerns. At the same time, we observed that 16.5% of participants had at least one post that they were uncomfortable sharing with a specific friend---someone who likely already had the ability to view it---and that 37% raised more general concerns with sharing their content with friends. We conclude that the current privacy controls allow users to effectively manage the outsider threat, but that they are unsuitable for mitigating concerns over the insider threat---members of the friend network who dynamically become inappropriate audiences based on the context of a post.},
booktitle = {Proceedings of the Eighth Symposium on Usable Privacy and Security},
articleno = {9},
numpages = {15},
keywords = {access control, social networking, privacy},
location = {Washington, D.C.},
series = {SOUPS '12}
}

@inproceedings{10.1145/2335356.2335371,
author = {Fahl, Sascha and Harbach, Marian and Muders, Thomas and Smith, Matthew and Sander, Uwe},
title = {Helping Johnny 2.0 to Encrypt His Facebook Conversations},
year = {2012},
isbn = {9781450315326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335356.2335371},
doi = {10.1145/2335356.2335371},
abstract = {Several billion Facebook messages are sent every day. While there are many solutions to email security whose usability has been extensively studied, little work has been done in the area of message security for Facebook and even less on the usability aspects in this area. To evaluate the need for such a mechanism, we conducted a screening study with 514 participants, which showed a clear desire to protect private messages on Facebook. We therefore proceeded to analyse the usability of existing approaches and extracted key design decisions for further evaluation. Based on this analysis, we conducted a laboratory study with 96 participants to analyse different usability aspects and requirements of a Facebook message encryption mechanism. Two key findings of our study are that automatic key management and key recovery capabilities are important features for such a mechanism. Following on from these studies, we designed and implemented a usable service-based encryption mechanism for Facebook conversations. In a final study with 15 participants, we analysed the usability of our solution. All participants were capable of successfully encrypting their Facebook conversations without error when using our service, and the mechanism was perceived as usable and useful. The results of our work suggest that in the context of the social web, new security/usability trade-offs can be explored to protect users more effectively.},
booktitle = {Proceedings of the Eighth Symposium on Usable Privacy and Security},
articleno = {11},
numpages = {17},
keywords = {message encryption, usable security, social networks},
location = {Washington, D.C.},
series = {SOUPS '12}
}

@inproceedings{10.5555/2390948.2390954,
author = {Hong, Kai and Kohler, Christian G. and March, Mary E. and Parker, Amber A. and Nenkova, Ani},
title = {Lexical Differences in Autobiographical Narratives from Schizophrenic Patients and Healthy Controls},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a system for automatic identification of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classifier can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction.},
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
pages = {37–47},
numpages = {11},
location = {Jeju Island, Korea},
series = {EMNLP-CoNLL '12}
}

@inproceedings{10.5555/2390948.2391015,
author = {Mihalcea, Rada and Strapparava, Carlo},
title = {Lyrics, Music, and Emotions},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we explore the classification of emotions in songs, using the music and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. We show that textual and musical features can both be successfully used for emotion recognition in songs. Moreover, through comparative experiments, we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers, with error rate reductions of up to 31%.},
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
pages = {590–599},
numpages = {10},
location = {Jeju Island, Korea},
series = {EMNLP-CoNLL '12}
}

@inproceedings{10.5555/2390948.2391032,
author = {Rahman, Altaf and Ng, Vincent},
title = {Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We examine the task of resolving complex cases of definite pronouns, specifically those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artificial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset.},
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
pages = {777–789},
numpages = {13},
location = {Jeju Island, Korea},
series = {EMNLP-CoNLL '12}
}

@inproceedings{10.5555/2390948.2391004,
author = {Fran\c{c}ois, Thomas and Fairon, C\'{e}drick},
title = {An "AI Readability" Formula for French as a Foreign Language},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language.},
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
pages = {466–477},
numpages = {12},
location = {Jeju Island, Korea},
series = {EMNLP-CoNLL '12}
}

@inproceedings{10.1145/2335755.2335835,
author = {Simmerman, Scott and Wang, Jingyuan and Osborne, James and Shook, Kimberly and Huang, Jian and Godsoe, William and Simons, Theodore},
title = {Exploring Similarities among Many Species Distributions},
year = {2012},
isbn = {9781450316026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335755.2335835},
doi = {10.1145/2335755.2335835},
abstract = {Collecting species presence data and then building models to predict species distribution has been long practiced in the field of ecology for the purpose of improving our understanding of species relationships with each other and with the environment. Due to limitations of computing power as well as limited means of using modeling software on HPC facilities, past species distribution studies have been unable to fully explore diverse data sets. We build a system that can, for the first time to our knowledge, leverage HPC to support effective exploration of species similarities in distribution as well as their dependencies on common environmental conditions. Our system can also compute and reveal uncertainties in the modeling results enabling domain experts to make informed judgments about the data. Our work was motivated by and centered around data collection efforts within the Great Smoky Mountains National Park that date back to the 1940s. Our findings present new research opportunities in ecology and produce actionable field-work items for biodiversity management personnel to include in their planning of daily management activities.},
booktitle = {Proceedings of the 1st Conference of the Extreme Science and Engineering Discovery Environment: Bridging from the EXtreme to the Campus and Beyond},
articleno = {38},
numpages = {9},
keywords = {HPC, parallel processing, species distribution modeling},
location = {Chicago, Illinois, USA},
series = {XSEDE '12}
}

@inproceedings{10.1145/2335484.2335505,
author = {Freudenreich, Tobias and Appel, Stefan and Frischbier, Sebastian and Buchmann, Alejandro P.},
title = {ACTrESS: Automatic Context Transformation in Event-Based Software Systems},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335505},
doi = {10.1145/2335484.2335505},
abstract = {Event-based systems (EBS) enable companies to respond to changes in their environment in a timely manner. To interpret event notifications, knowledge about their context is essential. The matching mechanisms of publish/subscribe systems depend on a common interpretation of event notifications and subscriptions that may span organisational boundaries. To mediate between such semantic contexts, we developed ACTrESS, a distributed middleware addon for automatic context transformation in event-based software systems and message-oriented middleware (MOM) in general. Transformations are substitutable at runtime and transparent to the user. ACTrESS is built on top of a production strength open source MOM extending the Java Message Service API. In this paper we present the challenges arising from differing contexts in event-based systems. We introduce ACTrESS and evaluate our solution using workloads derived from findings from research projects dealing with real-world applications of EBS.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {179–190},
numpages = {12},
keywords = {contextualization, context mediation, middleware, event semantics, transformation, event-based systems},
location = {Berlin, Germany},
series = {DEBS '12}
}

@article{10.1145/2331576.2331584,
author = {Bessani, Alysson and Kapitza, R\"{u}diger and Petcu, Dana and Romano, Paolo and Gogouvitis, Spyridon V. and Kyriazis, Dimosthenis and Cascella, Roberto G.},
title = {A Look to the Old-World_sky: EU-Funded Dependability Cloud Computing Research},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0163-5980},
url = {https://doi.org/10.1145/2331576.2331584},
doi = {10.1145/2331576.2331584},
abstract = {Cloud computing is currently the most important trend in the Information and Communication Technology (ICT) industry, and it has still not fully realized its potential. Reasons for its popularity are the opportunities to rapidly allocate vast amounts of computing resources and the fact that resources are accounted per use. While cloud computing was initiated by major industry players, academia has rapidly caught up; currently we see a vast number of cloud computing-related research efforts. However, since industry pushes development and many research aspects of cloud computing demand for large compute resources and real workloads, pure academic efforts have difficulties to address the most important issues and to have a major impact. On the other hand, academia usually tends to explore disruptive ideas that would not be addressed by industry alone. This paper summarizes the approaches and methods of five EU-funded research projects that focus on cloud computing in general and address important issues such as security, dependability, and interoperability. These aspects have received limited attention by the industry so far. The key to success of these large joint efforts is the close collaboration between partners from academia and industry spread all over Europe. The specific projects are Cloud-TM, Contrail, mOASIC, TClouds and VISION Cloud. Besides presenting the individual projects and their key contributions, we provide a perspective on future ICT research in Europe.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jul,
pages = {43–56},
numpages = {14},
keywords = {clouds, Europe, research projects, dependability}
}

@inproceedings{10.1145/2335755.2335789,
author = {Strande, Shawn M. and Cicotti, Pietro and Sinkovits, Robert S. and Young, William S. and Wagner, Rick and Tatineni, Mahidhar and Hocks, Eva and Snavely, Allan and Norman, Mike},
title = {Gordon: Design, Performance, and Experiences Deploying and Supporting a Data Intensive Supercomputer},
year = {2012},
isbn = {9781450316026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335755.2335789},
doi = {10.1145/2335755.2335789},
abstract = {The Gordon data intensive supercomputer entered service in 2012 as an allocable computing system in the NSF Extreme Science and Engineering Discovery Environment (XSEDE) program. Gordon has several innovative features that make it ideal for data intensive computing including: 1,024, compute nodes based on Intel's Sandy Bridge (Xeon E5) processor; 64 I/O nodes with an aggregate of 300 TB of high performance flash (SSD); large, virtual SMP "supernodes" of up to 2 TB DRAM; a dual-rail, QDR InfiniBand, 3D torus network based on commodity hardware and open source software; and a 100 GB/s Lustre based parallel file system, with over 4 PB of disk space. In this paper we present the motivation, design, and performance of Gordon. We provide: low level micro-benchmark results to demonstrate processor, memory, I/O, and network performance; standard HPC benchmarks; and performance on data intensive applications to demonstrate Gordon's performance on typical workloads. We highlight the inherent risks in, and describe mitigation strategies for, deploying a data intensive supercomputer like Gordon which embodies significant innovative technologies. Finally we present our experiences thus far in supporting users and managing Gordon.},
booktitle = {Proceedings of the 1st Conference of the Extreme Science and Engineering Discovery Environment: Bridging from the EXtreme to the Campus and Beyond},
articleno = {3},
numpages = {8},
keywords = {flash memory, data intensive computing, SSD, 3D torus, symmetric multiprocessing, interconnect},
location = {Chicago, Illinois, USA},
series = {XSEDE '12}
}

@inproceedings{10.1145/2335484.2335508,
author = {Tariq, Muhammad Adnan and Koldehofe, Boris and Koch, Gerald G. and Rothermel, Kurt},
title = {Distributed Spectral Cluster Management: A Method for Building Dynamic Publish/Subscribe Systems},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335508},
doi = {10.1145/2335484.2335508},
abstract = {In recent years peer-to-peer (P2P) networking has gained high popularity for large-scale content distribution. Prominent systems expect a large user base with rather diversified demands. Yet it is highly challenging to achieve scalability without sacrificing the expressiveness of queries in such systems. This paper proposes distributed spectral cluster management, an approach which adapts the techniques from spectral graph theory to work in distributed settings. The proposed approach is applied to content-based publish/subscribe to i) significantly reduce the cost for event dissemination by clustering subscribers exploiting the similarity of events, ii) preserve the expressiveness of the subscription language, and iii) perform robustly in the presence of workload variations. The evaluations analyze the accuracy of the proposed distributed spectral mechanisms and show their effectiveness to significantly reduce the efforts to disseminate events under many practical workloads.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {213–224},
numpages = {12},
keywords = {broker-less, spectral clustering, P2P, publish/subscribe, content-based},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/2335484.2335516,
author = {Akram, Shoaib and Marazakis, Manolis and Bilas, Angelos},
title = {Understanding and Improving the Cost of Scaling Distributed Event Processing},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335516},
doi = {10.1145/2335484.2335516},
abstract = {Building scalable back-end infrastructures for data-centric applications is becoming important. Applications used in data-centres have complex, multilayer software stacks and are required to scale to a large number of nodes. Today, there is increased interest in improving the efficiency of such software stacks. In this paper, we examine the efficiency of such a stack used for distributed stream processing, an important application domain. We use a specific streaming system, Borealis [10], and extensively hand-tune the end-to-end data path. We focus on parts of the stack that are related to intra- and inter-node communication and data exchange, a central component of many software stacks. We find that application-independent code in stream processing middleware employs operations for communication that consume significant amount of CPU cycles and are not strictly necessary. We first categorize these operations based on the protocol function they support. We then proceed to remove these operations by producing a functionally equivalent software stack in terms of application processing. Our results show that restructuring the data path achieves up to 5x higher throughput, reduces energy consumption by up to 60% and saves infrastructure cost by up to 40%. Finally, we project that with 1024-core processors per node, stream processing applications will demand up to 2 TBits/s/node of networking throughput.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {290–301},
numpages = {12},
keywords = {data-centric infrastructures, stream processing engines, distributed event processing},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.5555/2422356.2422372,
author = {Misztal, M. K. and Erleben, K. and Bargteil, A. and Fursund, J. and Christensen, B. Bunch and B\ae{}rentzen, J. A. and Bridson, R.},
title = {Multiphase Flow of Immiscible Fluids on Unstructured Moving Meshes},
year = {2012},
isbn = {9783905674378},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {In this paper, we present a method for animating multiphase flow of immiscible fluids using unstructured moving meshes. Our underlying discretization is an unstructured tetrahedral mesh, the deformable simplicial complex (DSC), that moves with the flow in a Lagrangian manner. Mesh optimization operations improve element quality and avoid element inversion. In the context of multiphase flow, we guarantee that every element is occupied by a single fluid and, consequently, the interface between fluids is represented by a set of faces in the simplicial complex. This approach ensures that the underlying discretization matches the physics and avoids the additional book-keeping required in grid-based methods where multiple fluids may occupy the same cell. Our Lagrangian approach naturally leads us to adopt a finite element approach to simulation, in contrast to the finite volume approaches adopted by a majority of fluid simulation techniques that use tetrahedral meshes. We characterize fluid simulation as an optimization problem allowing for full coupling of the pressure and velocity fields and the incorporation of a second-order surface energy. We introduce a preconditioner based on the diagonal Schur complement and solve our optimization on the GPU. We provide the results of parameter studies as well as a performance analysis of our method.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {97–106},
numpages = {10},
location = {Lausanne, Switzerland},
series = {SCA '12}
}

@article{10.14778/2367502.2367519,
author = {Chen, Yanpei and Alspaugh, Sara and Katz, Randy},
title = {Interactive Analytical Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367519},
doi = {10.14778/2367502.2367519},
abstract = {Within the past few years, organizations in diverse industries have adopted MapReduce-based systems for large-scale data processing. Along with these new users, important new workloads have emerged which feature many small, short, and increasingly interactive jobs in addition to the large, long-running batch jobs for which MapReduce was originally designed. As interactive, large-scale query processing is a strength of the RDBMS community, it is important that lessons from that field be carried over and applied where possible in this new domain. However, these new workloads have not yet been described in the literature. We fill this gap with an empirical analysis of MapReduce traces from six separate business-critical deployments inside Facebook and at Cloudera customers in e-commerce, telecommunications, media, and retail. Our key contribution is a characterization of new MapReduce workloads which are driven in part by interactive analysis, and which make heavy use of query-like programming frameworks on top of MapReduce. These workloads display diverse behaviors which invalidate prior assumptions about MapReduce such as uniform data access, regular diurnal patterns, and prevalence of large jobs. A secondary contribution is a first step towards creating a TPC-like data processing benchmark for MapReduce.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1802–1813},
numpages = {12}
}

@article{10.1145/2287710.2287711,
author = {Bouayad-Agha, Nadjet and Casamayor, Gerard and Mille, Simon and Wanner, Leo},
title = {Perspective-Oriented Generation of Football Match Summaries: Old Tasks, New Challenges},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1550-4875},
url = {https://doi.org/10.1145/2287710.2287711},
doi = {10.1145/2287710.2287711},
abstract = {Team sports commentaries call for techniques that are able to select content and generate wordings to reflect the affinity of the targeted reader for one of the teams. The existing works tend to have in common that they either start from knowledge sources of limited size to whose structures then different ways of realization are explicitly assigned, or they work directly with linguistic corpora, without the use of a deep knowledge source. With the increasing availability of large-scale ontologies this is no longer satisfactory: techniques are needed that are applicable to general purpose ontologies, but which still take user preferences into account. We take the best of both worlds in that we use a two-layer ontology. The first layer is composed of raw domain data modelled in an application-independent base OWL ontology. The second layer contains a rich perspective generation-motivated domain communication knowledge ontology, inferred from the base ontology. The two-layer ontology allows us to take into account user perspective-oriented criteria at different stages of generation to generate perspective-oriented commentaries. We show how content selection, discourse structuring, information structure determination, and lexicalization are driven by these criteria and how stage after stage a truly user perspective-tailored summary is generated. The viability of our proposal has been evaluated for the generation of football match summaries of the First Spanish Football League. The reported outcome of the evaluation demonstrates that we are on the right track.},
journal = {ACM Trans. Speech Lang. Process.},
month = aug,
articleno = {3},
numpages = {31},
keywords = {domain communication knowledge, discourse structuring, user perspective, lexicalization, content selection, information structuring, OWL, ontology, multilevel generation}
}

@article{10.1145/2240092.2240093,
author = {Cucuringu, Mihai and Lipman, Yaron and Singer, Amit},
title = {Sensor Network Localization by Eigenvector Synchronization over the Euclidean Group},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/2240092.2240093},
doi = {10.1145/2240092.2240093},
abstract = {We present a new approach to localization of sensors from noisy measurements of a subset of their Euclidean distances. Our algorithm starts by finding, embedding, and aligning uniquely realizable subsets of neighboring sensors called patches. In the noise-free case, each patch agrees with its global positioning up to an unknown rigid motion of translation, rotation, and possibly reflection. The reflections and rotations are estimated using the recently developed eigenvector synchronization algorithm, while the translations are estimated by solving an overdetermined linear system. The algorithm is scalable as the number of nodes increases and can be implemented in a distributed fashion. Extensive numerical experiments show that it compares favorably to other existing algorithms in terms of robustness to noise, sparse connectivity, and running time. While our approach is applicable to higher dimensions, in the current article, we focus on the two-dimensional case.},
journal = {ACM Trans. Sen. Netw.},
month = aug,
articleno = {19},
numpages = {42},
keywords = {eigenvectors, rigidity theory, Sensor networks, distance geometry, spectral graph theory, synchronization}
}

@inproceedings{10.1145/2345396.2345482,
author = {Karuppanan, Komathy and Vairasundaram, Abinaya Sree and Sigamani, Manjula},
title = {A Comprehensive Machine Learning Approach to Prognose Pulmonary Disease from Home},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345482},
doi = {10.1145/2345396.2345482},
abstract = {This paper proposes a machine learning based prognosis for rehabilitating the COPD patients to be monitored from home in real time. Wearable sensor Technology (WST) is utilized to collect the physiological status of the pulmonary patient from home dynamically and communicated to the healthcare centre. The proposed approach applies a comprehensive predictive model employing a time series forecasting using condensed polynomial neural network with swarm intelligence. Discrete particle swarm optimization (DPSO) filters out the relevant neurons and continuous particle swarm optimization (CPSO) reduces the computational overheads. The time series prediction is further strengthened by using multimodal genetic algorithm. Control measures such as sensitivity, specificity and reliability are applied meticulously to validate the predicted state of the patient.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {518–523},
numpages = {6},
keywords = {genetic algorithm, time series prediction, artificial intelligence, fuzzy c-means, condensed polynomial neural network, swarm intelligence},
location = {Chennai, India},
series = {ICACCI '12}
}

@inproceedings{10.1145/2345396.2345469,
author = {Dargha, Ramkumar},
title = {Cloud Computing: From Hype to Reality: Fast Tracking Cloud Adoption},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345469},
doi = {10.1145/2345396.2345469},
abstract = {Cloud computing is one of the most talked about and to an extent hyped technologies in the recent years. As per Gartner, cloud computing is one of the top 10 technology initiatives for this year. Cloud computing is at the "peak of inflated expectations" as per Gartner's recent cloud computing hype cycle. Cloud Computing Technology has a definitive potential. However as with any new technology, substantial amount of hype surrounds this technology. This paper starts with analyses on the background of cloud computing (including both private and public clouds) technology and arrives at a realistic prediction ('our view') on how cloud computing technology is most likely to evolve over next few years before it reaches mainstream adoption. The paper applies the theoretical frameworks like Geoffrey Moore's technology adoption life cycle and Innovation Decision Process theory &amp; Perceived Attributes Theory Frameworks to dissect issues and provide recommendations for adopting this technology.The technology adoption life cycle theory deals with the nature of adopters of a particular innovation and their characterization (innovators, early adopters, early majority, late majority and laggards). This paper analyses how cloud computing adoption could probably evolve over the years and arrive at a prediction of cloud computing adoption. We will arrive at the key chasm areas (like security, standardization of cloud APIs, reduction in overall migration cost etc) the cloud computing technology need to cross before getting into mainstream adoption. The innovation Decision Process theory (Rogers') deals with five specific stages an innovation adopter will go through. The five stages are: acquiring knowledge, persuasion, decision, implementation and confirmation. We will describe the ways a cloud vendor or system integrators can influence each of these five stages. Perceived Attributes Theory Framework deals with five key attributes on which any particular innovation will be judged upon. They are trial ability, observability, relative advantage, complexity and compatibility. This paper analyses cloud computing technology from the above aspects. Finally we will come up with key recommendations that the cloud vendors, system integrators, standard bodies and enterprises need to look at in order to leverage the applicable benefits of cloud computing technology and fast track adoption of this technology in enterprises.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {440–445},
numpages = {6},
location = {Chennai, India},
series = {ICACCI '12}
}

@inproceedings{10.1145/2351476.2351480,
author = {Costa, Jo\~{a}o Pedro and Martins, Pedro and Cecilio, Jos\'{e} and Furtado, Pedro},
title = {TEEPA: A Timely-Aware Elastic Parallel Architecture},
year = {2012},
isbn = {9781450312349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351476.2351480},
doi = {10.1145/2351476.2351480},
abstract = {Parallel Shared-Nothing architectures are frequently used to handle large star-schema Data Warehouses (DW). The continuous increase in data volume and the star-schema storage organization introduce severe limitations to scalability due to the well-known parallel join issues and the resulting need to use solutions such as on-the fly repartitioning of data or intermediate results, or massive replication of large data sets that still need to be joined locally, constraining their ability to deliver fast results. Parallelism may improve query performance, however some business decisions may require that query results be timely available which, even with additional parallelism and significant upgrade costs (both monetary and due to disturbance of normal operations), cannot be guaranteed. We propose a Timely-aware Execution Parallel Architecture (TEEPA) which balances data load and query processing among an elastic set of non-dedicated heterogeneous nodes in order to provide scale-out performance and timely query results. Data is allocated using adaptable storage models to minimize join costs (the major uncertainty factor) which best fit the nodes' capabilities, while preserving a consistent logical view of the star-schema. We present experimental evaluation of TEEPA and demonstrate its ability to provide timely results.},
booktitle = {Proceedings of the 16th International Database Engineering &amp; Applications Sysmposium},
pages = {24–31},
numpages = {8},
keywords = {timely execution, data warehouse, star-schema model, parallel shared-nothing, elastic parallel DW},
location = {Prague, Czech Republic},
series = {IDEAS '12}
}

@inproceedings{10.1145/2351476.2351491,
author = {Chen, Qiming and Hsu, Meichun},
title = {Stream-Join Revisited in the Context of Epoch-Based SQL Continuous Query},
year = {2012},
isbn = {9781450312349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351476.2351491},
doi = {10.1145/2351476.2351491},
abstract = {The current generation of stream processing systems is in general built separately from the query engine thus lacks the expressive power of SQL and causes significant overhead in data access and movement. This situation has motivated us to leverage the query engine for stream processing.Stream-join is a window operation where the key issue is how to punctuate and pair two or more correlated streams. In this work we tackle this issue in the specific context of query engine supported stream processing. We focus on the following problems: a SQL query is definable on bounded relation data but stream data are unbounded, and join multiple streams is a stateful (thus history-sensitive) operation but a SQL query only cares about the current state; further, relation join typically requires relation re-scan in a nested-loop but by nature a stream cannot be re-captured as reading a stream always gets newly incoming data.To leverage query processing for analyzing unbounded stream, we defined the Epoch-based Continuous Query (ECQ) model which allows a SQL query to be executed epoch by epoch for processing the stream data chunk by chunk. However, unlike multiple one-time queries, an ECQ is a single, continuous query instance across execution epochs for keeping the continuity of the application state as required by the history-sensitive operations such as sliding-window join.To joining multiple streams, we further developed the techniques to cache one or more consecutive data chunks falling in a sliding window across query execution epochs in the ECQ instance, to allow them to be re-delivered from the cache. In this way join multiple streams and self-join a single stream in the data chunk based window or sliding window, with various pairing schemes, are made possible.We extended the PostgreSQL engine to support the proposed approach. Our experience has demonstrated its value.},
booktitle = {Proceedings of the 16th International Database Engineering &amp; Applications Sysmposium},
pages = {130–138},
numpages = {9},
keywords = {stream join, continuous query},
location = {Prague, Czech Republic},
series = {IDEAS '12}
}

@inproceedings{10.1145/2351476.2351494,
author = {Manco, Giuseppe and Masciari, Elio},
title = {XML Class Outlier Detection},
year = {2012},
isbn = {9781450312349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351476.2351494},
doi = {10.1145/2351476.2351494},
abstract = {XML (eXtensible Markup Language) became in recent years the new standard for data representation and exchange on the WWW. This has resulted in a great need for data cleaning techniques in order to identify outlying data. In this paper, we present a technique for outlier detection that singles out anomalies with respect to a relevant group of objects. We exploit a suitable encoding of XML documents that are encoded as signals of fixed frequency that can be transformed using Fourier Transforms. Outliers are identified by simply looking at the signal spectra. The results show the effectiveness of our approach.},
booktitle = {Proceedings of the 16th International Database Engineering &amp; Applications Sysmposium},
pages = {155–164},
numpages = {10},
location = {Prague, Czech Republic},
series = {IDEAS '12}
}

@inproceedings{10.1145/2339530.2339740,
author = {Raeder, Troy and Stitelman, Ori and Dalessandro, Brian and Perlich, Claudia and Provost, Foster},
title = {Design Principles of Massive, Robust Prediction Systems},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339740},
doi = {10.1145/2339530.2339740},
abstract = {Most data mining research is concerned with building high-quality classification models in isolation. In massive production systems, however, the ability to monitor and maintain performance over time while growing in size and scope is equally important. Many external factors may degrade classification performance including changes in data distribution, noise or bias in the source data, and the evolution of the system itself. A well-functioning system must gracefully handle all of these. This paper lays out a set of design principles for large-scale autonomous data mining systems and then demonstrates our application of these principles within the m6d automated ad targeting system. We demonstrate a comprehensive set of quality control processes that allow us monitor and maintain thousands of distinct classification models automatically, and to add new models, take on new data, and correct poorly-performing models without manual intervention or system disruption.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1357–1365},
numpages = {9},
keywords = {data mining systems, quality control, data monitoring},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/2339530.2339637,
author = {Liu, Xuemei and Biagioni, James and Eriksson, Jakob and Wang, Yin and Forman, George and Zhu, Yanmin},
title = {Mining Large-Scale, Sparse GPS Traces for Map Inference: Comparison of Approaches},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339637},
doi = {10.1145/2339530.2339637},
abstract = {We address the problem of inferring road maps from large-scale GPS traces that have relatively low resolution and sampling frequency. Unlike past published work that requires high-resolution traces with dense sampling, we focus on situations with coarse granularity data, such as that obtained from thousands of taxis in Shanghai, which transmit their location as seldom as once per minute. Such data sources can be made available inexpensively as byproducts of existing processes, rather than having to drive every road with high-quality GPS instrumentation just for map building - and having to re-drive roads for periodic updates. Although the challenges in using opportunistic probe data are significant, successful mining algorithms could potentially enable the creation of continuously updated maps at very low cost.In this paper, we compare representative algorithms from two approaches: working with individual reported locations vs. segments between consecutive locations. We assess their trade-offs and effectiveness in both qualitative and quantitative comparisons for regions of Shanghai and Chicago.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {669–677},
numpages = {9},
keywords = {spatial data mining, map inference, road maps, gps},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/2339530.2339757,
author = {Roy, Pratanu and Teubner, Jens and Alonso, Gustavo},
title = {Efficient Frequent Item Counting in Multi-Core Hardware},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339757},
doi = {10.1145/2339530.2339757},
abstract = {The increasing number of cores and the rich instruction sets of modern hardware are opening up new opportunities for optimizing many traditional data mining tasks. In this paper we demonstrate how to speed up the performance of the computation of frequent items by almost one order of magnitude over the best published results by matching the algorithm to the underlying hardware architecture.We start with the observation that frequent item counting, like other data mining tasks, assumes certain amount of skew in the data. We exploit this skew to design a new algorithm that uses a pre-filtering stage that can be implemented in a highly efficient manner through SIMD instructions. Using pipelining, we then combine this pre-filtering stage with a conventional frequent item algorithm (Space-Saving) that will process the remainder of the data. The resulting operator can be parallelized with a small number of cores, leading to a parallel implementation that does not suffer any of the overheads of existing parallel solutions when querying the results and offers significantly higher throughput.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1451–1459},
numpages = {9},
keywords = {frequent items, data flow, parallelism},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/2339530.2339628,
author = {Gleich, David F. and Seshadhri, C.},
title = {Vertex Neighborhoods, Low Conductance Cuts, and Good Seeds for Local Community Methods},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339628},
doi = {10.1145/2339530.2339628},
abstract = {The communities of a social network are sets of vertices with more connections inside the set than outside. We theoretically demonstrate that two commonly observed properties of social networks, heavy-tailed degree distributions and large clustering coefficients, imply the existence of vertex neighborhoods (also known as egonets) that are themselves good communities. We evaluate these neighborhood communities on a range of graphs. What we find is that the neighborhood communities can exhibit conductance scores that are as good as the Fiedler cut. Also, the conductance of neighborhood communities shows similar behavior as the network community profile computed with a personalized PageRank community detection method. Neighborhood communities give us a simple and powerful heuristic for speeding up local partitioning methods. Since finding good seeds for the PageRank clustering method is difficult, most approaches involve an expensive sweep over a great many starting vertices. We show how to use neighborhood communities to quickly generate a small set of seeds.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {597–605},
numpages = {9},
keywords = {conductance, triangles, clustering coefficients, egonets},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/2339530.2339537,
author = {Matsubara, Yasuko and Sakurai, Yasushi and Prakash, B. Aditya and Li, Lei and Faloutsos, Christos},
title = {Rise and Fall Patterns of Information Diffusion: Model and Implications},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339537},
doi = {10.1145/2339530.2339537},
abstract = {The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated faster propagation of news and rumors. How quickly does a piece of news spread over these media? How does its popularity diminish over time? Does the rising and falling pattern follow a simple universal law?In this paper, we propose SpikeM, a concise yet flexible analytical model for the rise and fall patterns of influence propagation. Our model has the following advantages: (a) unification power: it generalizes and explains earlier theoretical models and empirical observations; (b) practicality: it matches the observed behavior of diverse sets of real data; (c) parsimony: it requires only a handful of parameters; and (d) usefulness: it enables further analytics tasks such as fore- casting, spotting anomalies, and interpretation by reverse- engineering the system parameters of interest (e.g. quality of news, count of interested bloggers, etc.).Using SpikeM, we analyzed 7.2GB of real data, most of which were collected from the public domain. We have shown that our SpikeM model accurately and succinctly describes all the patterns of the rise-and-fall spikes in these real datasets.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {6–14},
numpages = {9},
keywords = {information diffusion, social networks},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/2339530.2339706,
author = {Shahaf, Dafna and Guestrin, Carlos and Horvitz, Eric},
title = {Metro Maps of Science},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339706},
doi = {10.1145/2339530.2339706},
abstract = {As the number of scientific publications soars, even the most enthusiastic reader can have trouble staying on top of the evolving literature. It is easy to focus on a narrow aspect of one's field and lose track of the big picture. Information overload is indeed a major challenge for scientists today, and is especially daunting for new investigators attempting to master a discipline and scientists who seek to cross disciplinary borders. In this paper, we propose metrics of influence, coverage and connectivity for scientific literature. We use these metrics to create structured summaries of information, which we call metro maps. Most importantly, metro maps explicitly show the relations between papers in a way which captures developments in the field. Pilot user studies demonstrate that our method helps researchers acquire new knowledge efficiently: map users achieved better precision and recall scores and found more seminal papers while performing fewer searches.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1122–1130},
numpages = {9},
keywords = {information, metro maps, summarization},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/2351356.2351360,
author = {Hill, Shawndra and Nalavade, Aman and Benton, Adrian},
title = {Social TV: Real-Time Social Media Response to TV Advertising},
year = {2012},
isbn = {9781450315456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351356.2351360},
doi = {10.1145/2351356.2351360},
abstract = {We link the content of real-time social media response to characteristics of the Super Bowl 2012 TV advertisements, including their advertisers' social media strategies and the categories of the products being advertised. We analyze millions of social media posts about approximately forty-five advertisements during the 2012 Super Bowl. While there are many studies that focus on the popularity of and sentiment of response to Super Bowl ads, we believe ours is the first to go deeper and analyze the textual "content" to understand better the factors contributing to specific types of social media responses among a number of TV advertising dimensions. In this study, we show that the level of online consumer engagement, measured by attracting new followers on Twitter during the Superbowl, can be linked to whether the brand that advertised had a social media strategy or not. In addition, we show that sentiment response, a measure often use to quantify the effectiveness of TV ads, varies by demographic category.},
booktitle = {Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy},
articleno = {4},
numpages = {9},
keywords = {social TV, advertising, social media, television},
location = {Beijing, China},
series = {ADKDD '12}
}

@inproceedings{10.1145/2392622.2392624,
author = {La Gala, Massimiliano and Arnaboldi, Valerio and Conti, Marco and Passarella, Andrea},
title = {Ego-Net Digger: A New Way to Study Ego Networks in Online Social Networks},
year = {2012},
isbn = {9781450315494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2392622.2392624},
doi = {10.1145/2392622.2392624},
abstract = {The vast proliferation of Online Social Networks (OSN) is generating many new ways to interact and create social relationships with others. While substantial results have been obtained in anthropology literature describing the properties of human social networks, a clear understanding of the properties of social networks built using OSN is still to be achieved. The presence of a huge amount of records containing users' communication history, provided by OSN, represents a new opportunity to analyse and better understand the human social behaviour. In this paper we present ego-net digger, a novel Facebook application for the analysis of ego networks in OSN. Ego-net digger collects users' social data and gives a representation of their personal social networks according to the Dunbar's circles model. To show the potential of our application we analyse a sample data set collected during a data acquisition campaign, finding interesting similarities between the structure of Facebook ego networks and the properties found in the anthropology literature. Specifically, we find that, in our sample, there is a clear evidence of the presence of the same ego network structure - i.e., the Dunbar's circles - as found in human social networks formed offline.},
booktitle = {Proceedings of the First ACM International Workshop on Hot Topics on Interdisciplinary Social Networks Research},
pages = {9–16},
numpages = {8},
keywords = {tie strength, online social networks, ego networks},
location = {Beijing, China},
series = {HotSocial '12}
}

@inproceedings{10.1145/2346676.2346677,
author = {He, Yulan},
title = {A Bayesian Modeling Approach to Multi-Dimensional Sentiment Distributions Prediction},
year = {2012},
isbn = {9781450315432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2346676.2346677},
doi = {10.1145/2346676.2346677},
abstract = {Sentiment analysis has long focused on binary classification of text as either positive or negative. There has been few work on mapping sentiments or emotions into multiple dimensions. This paper studies a Bayesian modeling approach to multi-class sentiment classification and multidimensional sentiment distributions prediction. It proposes effective mechanisms to incorporate supervised information such as labeled feature constraints and document-level sentiment distributions derived from the training data into model learning. We have evaluated our approach on the datasets collected from the confession section of the Experience Project website where people share their life experiences and personal stories. Our results show that using the latent representation of the training documents derived from our approach as features to build a maximum entropy classifier outperforms other approaches on multi-class sentiment classification. In the more difficult task of multi-dimensional sentiment distributions prediction, our approach gives superior performance compared to a few competitive baselines.},
booktitle = {Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining},
articleno = {1},
numpages = {8},
keywords = {sentiment analysis, latent dirichlet allocation (LDA), opinion mining, joint sentiment/topic model (JST)},
location = {Beijing, China},
series = {WISDOM '12}
}

@inproceedings{10.1145/2348283.2348405,
author = {Efron, Miles and Organisciak, Peter and Fenlon, Katrina},
title = {Improving Retrieval of Short Texts through Document Expansion},
year = {2012},
isbn = {9781450314725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348283.2348405},
doi = {10.1145/2348283.2348405},
abstract = {Collections containing a large number of short documents are becoming increasingly common. As these collections grow in number and size, providing effective retrieval of brief texts presents a significant research problem. We propose a novel approach to improving information retrieval (IR) for short texts based on aggressive document expansion. Starting from the hypothesis that short documents tend to be about a single topic, we submit documents as pseudo-queries and analyze the results to learn about the documents themselves. Document expansion helps in this context because short documents yield little in the way of term frequency information. However, as we show, the proposed technique helps us model not only lexical properties, but also temporal properties of documents. We present experimental results using a corpus of microblog (Twitter) data and a corpus of metadata records from a federated digital library. With respect to established baselines, results of these experiments show that applying our proposed document expansion method yields significant improvements in effectiveness. Specifically, our method improves the lexical representation of documents and the ability to let time influence retrieval.},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {911–920},
numpages = {10},
keywords = {document expansion, twitter, dublin core, temporal IR, microblogs, language models, information retrieval},
location = {Portland, Oregon, USA},
series = {SIGIR '12}
}

@inproceedings{10.1145/2347635.2347646,
author = {Johnson, Mikael and Hyysalo, Sampsa},
title = {Lessons for Participatory Designers of Social Media: Long-Term User Involvement Strategies in Industry},
year = {2012},
isbn = {9781450308465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2347635.2347646},
doi = {10.1145/2347635.2347646},
abstract = {Social media changes the conditions for user participation in service development. Active user communities, fast paced iterative development, considerable development after market launch, developer access to users' digital trails, peer production, and low cost feature distribution are well known facets that bring substantial changes. In this paper we distil lessons for participatory designers from an in-depth case study of an over decade-long service development in industry, Habbo Hotel by Sulake Corporation. We argue that the range of core issues that shape user participation in social media can be captured by three interrelated issues: 1) shifts in developer--user social distance, 2) cumulated user knowledge beyond one project, and 3) user-generated content and user-owned services. We then consider what insight these provide for a design initiative we are involved in: the Finnish national public service broadcasting company's teacher resource.},
booktitle = {Proceedings of the 12th Participatory Design Conference: Research Papers - Volume 1},
pages = {71–80},
numpages = {10},
keywords = {social media, long-term user involvement, community participation, industry contexts},
location = {Roskilde, Denmark},
series = {PDC '12}
}

@inproceedings{10.1145/2347635.2347653,
author = {Davis, Janet},
title = {Early Experiences with Participation in Persuasive Technology Design},
year = {2012},
isbn = {9781450308465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2347635.2347653},
doi = {10.1145/2347635.2347653},
abstract = {Persuasive technology, designed to change behaviors and attitudes, stands on uneasy moral ground. A key concern is the appropriateness of the means of persuasion and the intent to persuade. Engaging with those who will use the persuasive technology can ensure that it aligns with their own desires for change. This paper presents an early case study applying participatory design methods to persuasive technology in the context of a college EcoHouse. After presenting the methods and results, I synthesize lessons learned for the intersection of participatory design and persuasive technology design: begin with participants who want change, attend to power relations, promote reflection, start with simple behaviors, use examples to educate and inspire, explore designs in parallel, and be open to not designing technology. Finally, I identify challenges for future work: designing an effective design process, negotiating tensions between effectiveness and reflectiveness, and evaluating the impact of participation.},
booktitle = {Proceedings of the 12th Participatory Design Conference: Research Papers - Volume 1},
pages = {119–128},
numpages = {10},
keywords = {participatory design, persuasive technology, sustainability},
location = {Roskilde, Denmark},
series = {PDC '12}
}

@inproceedings{10.1145/2342509.2342513,
author = {Bonomi, Flavio and Milito, Rodolfo and Zhu, Jiang and Addepalli, Sateesh},
title = {Fog Computing and Its Role in the Internet of Things},
year = {2012},
isbn = {9781450315197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342509.2342513},
doi = {10.1145/2342509.2342513},
abstract = {Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. Defining characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things (IoT) services and applications, namely, Connected Vehicle, Smart Grid, Smart Cities, and, in general, Wireless Sensors and Actuators Networks (WSANs).},
booktitle = {Proceedings of the First Edition of the MCC Workshop on Mobile Cloud Computing},
pages = {13–16},
numpages = {4},
keywords = {fog computing, software defined networks, wsan, iot, analytics, cloud computing, real time systems},
location = {Helsinki, Finland},
series = {MCC '12}
}

