@inproceedings{10.5555/1862242.1862251,
author = {Shang, Shuo and Deng, Ke and Zheng, Kai},
title = {Efficient Best Path Monitoring in Road Networks for Instant Local Traffic Information},
year = {2010},
isbn = {9781920682859},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {The shortest path problem has been well studied previously. To improve the utility, the traffic conditions can be modeled to associate a weight to each road segment. The recent trend is to apply data mining techniques over use history which usually covers a long period of time such as months. However, this method fails to reflect the instant (i.e. temporary) traffic conditions change such as traffic accident or road work. Due to the temporary nature, the local instant traffic conditions makes more sense when an object is moving in road networks. In this work, we investigate the shortest path monitoring problem while the instant traffic conditions in local region update repeatedly around a moving object to a given destination. A simple way is to apply A* algorithm repeatedly. However, the weakness is obvious. Because only a small fraction, i.e. the local area, of the whole networks have changed and the other parts keep intact. That means, for many vertices, that their paths (or the lower bounds of their paths) to the destination are still valid. This motivates us to maintain these information and reuse in the following computations. Our method is based on two tree structures where one records the previous computing results and the other aims to reduce the search space of subsequent processing. The experiments over real data set demonstrate an improvement of processing efficiency by one degree of magnitude at a small memory cost. In addition, the tree can be shared when monitoring the shortest paths for several moving objects to the same destination.},
booktitle = {Proceedings of the Twenty-First Australasian Conference on Database Technologies - Volume 104},
pages = {47–56},
numpages = {10},
keywords = {instant local traffic information, shortest path monitoring},
location = {Brisbane, Australia},
series = {ADC '10}
}

@inproceedings{10.5555/1862242.1862257,
author = {Sharaf, Mohamed A. and Chrysanthis, Panos K. and Labrinidis, Alexandros},
title = {Tuning QoD in Stream Processing Engines},
year = {2010},
isbn = {9781920682859},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Quality of Service (QoS) and Quality of Data (QoD) are the two major dimensions for evaluating any query processing system. In the context of data stream management systems (DSMSs), multi-query scheduling has been exploited to improve QoS. In this paper, we are proposing to exploit query scheduling to improve QoD in DSMSs. Specifically, we are presenting a new policy for scheduling multiple continuous queries with the objective of maximizing the freshness of the output data streams and hence the QoD of such outputs. The proposed Freshness-Aware Scheduling of Multiple Continuous Queries (FAS-MCQ) policy decides the execution order of continuous queries based on each query's properties (i.e., cost and selectivity) as well the properties of the input update streams (i.e., variability of updates). Our experimental results have shown that FAS-MCQ can improve QoD by up to 50% compared to existing scheduling policies used in DSMSs. Finally, we propose and evaluate a parametrized version of our FAS-MCQ scheduler that is able to balance the trade-off between freshness and response time according to the application's requirements.},
booktitle = {Proceedings of the Twenty-First Australasian Conference on Database Technologies - Volume 104},
pages = {103–112},
numpages = {10},
keywords = {operator scheduling, continuous queries, quality of data (QoD), data freshness, data stream management systems, quality of service (QoS)},
location = {Brisbane, Australia},
series = {ADC '10}
}

@article{10.1145/1656255.1656259,
author = {Meerbaum--Salant, Orni and Hazzan, Orit},
title = {An Agile Constructionist Mentoring Methodology for Software Projects in the High School},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/1656255.1656259},
doi = {10.1145/1656255.1656259},
abstract = {This article describes the construction process and evaluation of the Agile Constructionist Mentoring Methodology (ACMM), a mentoring method for guiding software development projects in the high school. The need for such a methodology has arisen due to the complexity of mentoring software project development in the high school. We introduce the ACMM and suggest a year-long mentoring template that includes the practices required for the actual mentoring process. The evaluation of the ACMM reveals that the methodology addresses each of the challenges teachers cope with during the mentoring process, which were identified in the first phase of the research.},
journal = {ACM Trans. Comput. Educ.},
month = jan,
articleno = {21},
numpages = {29},
keywords = {agile software development, Computer science education, constructionism, Shulman’s teacher knowledge base model}
}

@article{10.1145/1653760.1653766,
author = {Eisenman, Shane B. and Miluzzo, Emiliano and Lane, Nicholas D. and Peterson, Ronald A. and Ahn, Gahng-Seop and Campbell, Andrew T.},
title = {BikeNet: A Mobile Sensing System for Cyclist Experience Mapping},
year = {2010},
issue_date = {December 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/1653760.1653766},
doi = {10.1145/1653760.1653766},
abstract = {We present BikeNet, a mobile sensing system for mapping the cyclist experience. Built leveraging the MetroSense architecture to provide insight into the real-world challenges of people-centric sensing, BikeNet uses a number of sensors embedded into a cyclist's bicycle to gather quantitative data about the cyclist's rides. BikeNet uses a dual-mode operation for data collection, using opportunistically encountered wireless access points in a delay-tolerant fashion by default, and leveraging the cellular data channel of the cyclist's mobile phone for real-time communication as required. BikeNet also provides a Web-based portal for each cyclist to access various representations of her data, and to allow for the sharing of cycling-related data (for example, favorite cycling routes) within cycling interest groups, and data of more general interest (for example, pollution data) with the broader community. We present: a description and prototype implementation of the system architecture based on customized Moteiv Tmote Invent motes and sensor-enabled Nokia N80 mobile phones; an evaluation of sensing and inference that quantifies cyclist performance and the cyclist environment; a report on networking performance in an environment characterized by bicycle mobility and human unpredictability; and a description of BikeNet system user interfaces.},
journal = {ACM Trans. Sen. Netw.},
month = jan,
articleno = {6},
numpages = {39},
keywords = {recreation, systems, bicycling, Applications}
}

@inproceedings{10.1145/1693453.1693482,
author = {Zhang, Eddy Z. and Jiang, Yunlian and Shen, Xipeng},
title = {Does Cache Sharing on Modern CMP Matter to the Performance of Contemporary Multithreaded Programs?},
year = {2010},
isbn = {9781605588773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1693453.1693482},
doi = {10.1145/1693453.1693482},
abstract = {Most modern Chip Multiprocessors (CMP) feature shared cache on chip. For multithreaded applications, the sharing reduces communication latency among co-running threads, but also results in cache contention.A number of studies have examined the influence of cache sharing on multithreaded applications, but most of them have concentrated on the design or management of shared cache, rather than a systematic measurement of the influence. Consequently, prior measurements have been constrained by the reliance on simulators, the use of out-of-date benchmarks, and the limited coverage of deciding factors. The influence of CMP cache sharing on contemporary multithreaded applications remains preliminarily understood.In this work, we conduct a systematic measurement of the influence on two kinds of commodity CMP machines, using a recently released CMP benchmark suite, PARSEC, with a number of potentially important factors on program, OS, and architecture levels considered. The measurement shows some surprising results. Contrary to commonly perceived importance of cache sharing, neither positive nor negative effects from the cache sharing are significant for most of the program executions, regardless of the types of parallelism, input datasets, architectures, numbers of threads, and assignments of threads to cores. After a detailed analysis, we find that the main reason is the mismatch of current development and compilation of multithreaded applications and CMP architectures. By transforming the programs in a cache-sharing-aware manner, we observe up to 36% performance increase when the threads are placed on cores appropriately.},
booktitle = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {203–212},
numpages = {10},
keywords = {chip multiprocessors, parallel program optimizations, thread scheduling, shared cache},
location = {Bangalore, India},
series = {PPoPP '10}
}

@article{10.1145/1837853.1693482,
author = {Zhang, Eddy Z. and Jiang, Yunlian and Shen, Xipeng},
title = {Does Cache Sharing on Modern CMP Matter to the Performance of Contemporary Multithreaded Programs?},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/1837853.1693482},
doi = {10.1145/1837853.1693482},
abstract = {Most modern Chip Multiprocessors (CMP) feature shared cache on chip. For multithreaded applications, the sharing reduces communication latency among co-running threads, but also results in cache contention.A number of studies have examined the influence of cache sharing on multithreaded applications, but most of them have concentrated on the design or management of shared cache, rather than a systematic measurement of the influence. Consequently, prior measurements have been constrained by the reliance on simulators, the use of out-of-date benchmarks, and the limited coverage of deciding factors. The influence of CMP cache sharing on contemporary multithreaded applications remains preliminarily understood.In this work, we conduct a systematic measurement of the influence on two kinds of commodity CMP machines, using a recently released CMP benchmark suite, PARSEC, with a number of potentially important factors on program, OS, and architecture levels considered. The measurement shows some surprising results. Contrary to commonly perceived importance of cache sharing, neither positive nor negative effects from the cache sharing are significant for most of the program executions, regardless of the types of parallelism, input datasets, architectures, numbers of threads, and assignments of threads to cores. After a detailed analysis, we find that the main reason is the mismatch of current development and compilation of multithreaded applications and CMP architectures. By transforming the programs in a cache-sharing-aware manner, we observe up to 36% performance increase when the threads are placed on cores appropriately.},
journal = {SIGPLAN Not.},
month = jan,
pages = {203–212},
numpages = {10},
keywords = {chip multiprocessors, thread scheduling, shared cache, parallel program optimizations}
}

@inproceedings{10.1145/2108616.2108641,
author = {Liu, Ying Chieh and Lin, Chad and Feng-Chia, Li},
title = {A Model of Transiting Individual Efforts to the Outcomes of Virtual Team},
year = {2010},
isbn = {9781605588933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2108616.2108641},
doi = {10.1145/2108616.2108641},
abstract = {Few studies have formulated how individuals contribute to the team process and bring the success of virtual teams. The researcher attempted to integrate Technology-Task fit (TTF), Self-disclosure and Social Networks to build a framework to formulate how individual efforts are transited to the outcome of virtual teams. This framework was validated by an experiment which was engaged in a Wiki system. The results revealed that virtual team members interact by computer-mediated communication (CMC), the social relations were formed and gradually the team was tied together as a group. The efforts of each member are integrated by social ties in order to accomplish the goals. Three suggestions were proposed: (a) an appropriate technology which fits to the tasks should be provided; (b) an adequate training of operating the technology and disclose is needed to help members convey social cues and information to accomplish the tasks; (c) a mechanism to ensure the social ties go on the right track is crucial such as the frequency of communication and the way of solving conflicts.},
booktitle = {Proceedings of the 4th International Conference on Uniquitous Information Management and Communication},
articleno = {20},
numpages = {8},
keywords = {self-disclosure, virtual teams, transit, SEM, TTF, social networks},
location = {Suwon, Republic of Korea},
series = {ICUIMC '10}
}

@inproceedings{10.5555/1873601.1873682,
author = {Christodoulou, George and Kov\'{a}cs, Annam\'{a}ria},
title = {A Deterministic Truthful PTAS for Scheduling Related Machines},
year = {2010},
isbn = {9780898716986},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {Scheduling on related machines (Q||Cmax) is one of the most important problems in the field of Algorithmic Mechanism Design. Each machine is controlled by a selffish agent and her valuation can be expressed via a single parameter, her speed. Archer and Tardos [4] showed that, in contrast to other similar problems, a (non-polynomial) allocation that minimizes the makespan can be truthfully implemented. On the other hand, if we leave out the game-theoretic issues, the complexity of the problem has been completely settled --- the problem is strongly NP-hard, while there exists a PTAS [9, 8].This problem is the most well-studied in single-parameter Algorithmic Mechanism Design. It gives an excellent ground to explore the boundary between truthfulness and efficient computation. Since the work of Archer and Tardos, quite a lot of deterministic and randomized mechanisms have been suggested. Recently, a breakthrough result [7] showed that a randomized, truthful-in-expectation PTAS exists. On the other hand, for the deterministic case, the best known approximation factor is 2.8 [10, 11].It has been a major open question whether there exists a deterministic truthful PTAS, or whether truthfulness has an essential, negative impact on the computational complexity of the problem. In this paper we give a deffinitive answer to this important question by providing a truthful deterministic PTAS.},
booktitle = {Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1005–1016},
numpages = {12},
location = {Austin, Texas},
series = {SODA '10}
}

@inproceedings{10.5555/1873601.1873677,
author = {Karloff, Howard and Suri, Siddharth and Vassilvitskii, Sergei},
title = {A Model of Computation for MapReduce},
year = {2010},
isbn = {9780898716986},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {In recent years the MapReduce framework has emerged as one of the most widely used parallel computing platforms for processing data on terabyte and petabyte scales. Used daily at companies such as Yahoo!, Google, Amazon, and Facebook, and adopted more recently by several universities, it allows for easy parallelization of data intensive computations over many machines. One key feature of MapReduce that differentiates it from previous models of parallel computation is that it interleaves sequential and parallel computation. We propose a model of efficient computation using the MapReduce paradigm. Since MapReduce is designed for computations over massive data sets, our model limits the number of machines and the memory per machine to be substantially sublinear in the size of the input. On the other hand, we place very loose restrictions on the computational power of of any individual machine---our model allows each machine to perform sequential computations in time polynomial in the size of the original input.We compare MapReduce to the PRAM model of computation. We prove a simulation lemma showing that a large class of PRAM algorithms can be efficiently simulated via MapReduce. The strength of MapReduce, however, lies in the fact that it uses both sequential and parallel computation. We demonstrate how algorithms can take advantage of this fact to compute an MST of a dense graph in only two rounds, as opposed to Ω(log(n)) rounds needed in the standard PRAM model. We show how to evaluate a wide class of functions using the MapReduce framework. We conclude by applying this result to show how to compute some basic algorithmic problems such as undirected s-t connectivity in the MapReduce framework.},
booktitle = {Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {938–948},
numpages = {11},
location = {Austin, Texas},
series = {SODA '10}
}

@article{10.1145/1644873.1644875,
author = {Syed, Zeeshan and Stultz, Collin and Kellis, Manolis and Indyk, Piotr and Guttag, John},
title = {Motif Discovery in Physiological Datasets: A Methodology for Inferring Predictive Elements},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1644873.1644875},
doi = {10.1145/1644873.1644875},
abstract = {In this article, we propose a methodology for identifying predictive physiological patterns in the absence of prior knowledge. We use the principle of conservation to identify activity that consistently precedes an outcome in patients, and describe a two-stage process that allows us to efficiently search for such patterns in large datasets. This involves first transforming continuous physiological signals from patients into symbolic sequences, and then searching for patterns in these reduced representations that are strongly associated with an outcome.Our strategy of identifying conserved activity that is unlikely to have occurred purely by chance in symbolic data is analogous to the discovery of regulatory motifs in genomic datasets. We build upon existing work in this area, generalizing the notion of a regulatory motif and enhancing current techniques to operate robustly on non-genomic data. We also address two significant considerations associated with motif discovery in general: computational efficiency and robustness in the presence of degeneracy and noise. To deal with these issues, we introduce the concept of active regions and new subset-based techniques such as a two-layer Gibbs sampling algorithm. These extensions allow for a framework for information inference, where precursors are identified as approximately conserved activity of arbitrary complexity preceding multiple occurrences of an event.We evaluated our solution on a population of patients who experienced sudden cardiac death and attempted to discover electrocardiographic activity that may be associated with the endpoint of death. To assess the predictive patterns discovered, we compared likelihood scores for motifs in the sudden death population against control populations of normal individuals and those with non-fatal supraventricular arrhythmias. Our results suggest that predictive motif discovery may be able to identify clinically relevant information even in the absence of significant prior knowledge.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {2},
numpages = {23},
keywords = {Gibbs sampling, inference, physiological signals, motifs, data mining, knowledge discovery}
}

@inproceedings{10.1145/1935701.1935759,
author = {Portocarrero, Edwina and Cranor, David and Bove, V. Michael},
title = {Pillow-Talk: Seamless Interface for Dream Priming, Recalling and Playback},
year = {2010},
isbn = {9781450304788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1935701.1935759},
doi = {10.1145/1935701.1935759},
abstract = {Pillow-Talk is the first of a series of objects designed to aid creative endeavors through the unobtrusive acquisition of unconscious self-generated content to permit reflexive self-knowledge. Composed of a seamless recording device embedded in a pillow and a playback and visualization system in a jar, Pillow-Talk crystallizes that which we normally forget. This allows users to capture their dreams in a less mediated way, aiding recollection by priming the experience and providing no distraction for recall and capture through embodied interaction.},
booktitle = {Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {269–272},
numpages = {4},
keywords = {memory, dream, design, visualization, seamless interface},
location = {Funchal, Portugal},
series = {TEI '11}
}

@inproceedings{10.1145/1935701.1935730,
author = {Helmes, John and Taylor, Alex S. and Cao, Xiang and H\"{o}\"{o}k, Kristina and Schmitt, Peter and Villar, Nicolas},
title = {Rudiments 1, 2 &amp; 3: Design Speculations on Autonomy},
year = {2010},
isbn = {9781450304788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1935701.1935730},
doi = {10.1145/1935701.1935730},
abstract = {This work describes the design process and installation of three speculative, rudimentary machines, or rudiments. Through careful iterations in their design, the rudiments are intended to provoke curiosity and discussion around the possibility of autonomy in interactive systems. The design of the rudiments is described in detail, alongside the design decisions that were made to suggest a machine autonomy and to provoke discussion. Some preliminary reflections from installing the rudiments in two separate households are also reported. Widely divergent opinions of the rudiments from the two households are used to discuss a number of themes for thinking about autonomy and interactive systems design. Overall, the presented work adopts a perspective strongly oriented towards guiding future research, but, importantly, aims to do so by opening up and exposing the design possibilities rather than constraining them.},
booktitle = {Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {145–152},
numpages = {8},
keywords = {autonomy, speculative design, social robots},
location = {Funchal, Portugal},
series = {TEI '11}
}

@inproceedings{10.1145/1935701.1935745,
author = {Wimmer, Raphael},
title = {Grasp Sensing for Human-Computer Interaction},
year = {2010},
isbn = {9781450304788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1935701.1935745},
doi = {10.1145/1935701.1935745},
abstract = {The way we grasp an object depends on several factors, e.g. the intended goal or the hand's anatomy. Therefore, a grasp can convey meaningful information about its context. Inferring these factors from a grasp allows us to enhance interaction with grasp-sensitive objects. This paper highlights an grasp as an important source of meaningful context for human-computer interaction and gives an overview of prior work from other disciplines. This paper offers a basis and framework for further research and discussion by proposing a descriptive model of meaning in grasps. The GRASP model combines five factors that determine how an object is grasped: goal, relationship between user and object, anatomy, setting, and properties of the object. The model is validated both from an epistemological perspective and by applying it to scenarios from related work.},
booktitle = {Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {221–228},
numpages = {8},
keywords = {grasp, meaning, tangible user interfaces, grasp recognition},
location = {Funchal, Portugal},
series = {TEI '11}
}

@inproceedings{10.1145/1709886.1709898,
author = {Berlin, Eugen and Liu, Jun and van Laerhoven, Kristof and Schiele, Bernt},
title = {Coming to Grips with the Objects We Grasp: Detecting Interactions with Efficient Wrist-Worn Sensors},
year = {2010},
isbn = {9781605588414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1709886.1709898},
doi = {10.1145/1709886.1709898},
abstract = {The use of a wrist-worn sensor that is able to read nearby RFID tags and the wearer's gestures has been suggested frequently as a way to both detect the objects we interact with and to identify the interaction. Making such a prototype feasible for longer-term deployments is far from solved however, as plenty of challenges remain in the hardware, embedded algorithms, and the overall design of such a bracelet-like device. This paper presents several of the challenges that emerged during the development of a functioning prototype that is able to sense interaction data for several days. We focus in particular on RFID tag reading range optimization, efficient data logging methods, meaningful evaluation techniques, and long-term deployments.},
booktitle = {Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction},
pages = {57–64},
numpages = {8},
keywords = {wearable interaction, gesture detection, wrist-worn RFID},
location = {Cambridge, Massachusetts, USA},
series = {TEI '10}
}

@article{10.1145/1668862.1668863,
author = {Alvaro, Alexandre and Santana de Almeida, Eduardo and Romero de Lemos Meira, Silvio},
title = {A Software Component Quality Framework},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1668862.1668863},
doi = {10.1145/1668862.1668863},
abstract = {One of the major problems with Component-Based Software Engineering (CBSE) is the quality of the components used in a system. The reliability of a component-based software system depends on the reliability of the components that is made of. In CBSE, the proper search, selection and evaluation process of components is considered the cornerstone for the development of any effective component-based system. So far the software industry was concentrated on the functional aspects of components, leaving aside the difficult task of assessing their quality. In this way, we propose a software component quality framework to evaluate the quality of software components in an efficient way. Moreover, an experimental study was accomplished in order to evaluate the viability of the proposed framework.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–18},
numpages = {18}
}

@article{10.1145/1658377.1658380,
author = {Tagarelli, Andrea and Greco, Sergio},
title = {Semantic Clustering of XML Documents},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/1658377.1658380},
doi = {10.1145/1658377.1658380},
abstract = {Dealing with structure and content semantics underlying semistructured documents is challenging for any task of document management and knowledge discovery conceived for such data. In this work we address the novel problem of clustering semantically related XML documents according to their structure and content features. XML features are generated by enriching syntactic with semantic information based on a lexical knowledge base. The backbone of the proposed framework for the semantic clustering of XML documents is a data representation model that exploits the notion of tree tuple to identify semantically cohesive substructures in XML documents and represent them as transactional data. This framework is equipped with two clustering algorithms based on different paradigms, namely centroid-based partitional clustering and frequent-itemset-based hierarchical clustering. An extensive experimental evaluation was conducted on real data sets from various domains, showing the significance of our approach as a solution for the semantic clustering of XML documents.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {3},
numpages = {56},
keywords = {XML structure and content mining, XML transactional modeling, XML tree tuples, similarity measures, XML document clustering}
}

@article{10.1145/1731888.1731889,
author = {Kordon, Arthur},
title = {Issues in Applying Computational Intelligence},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/1731888.1731889},
doi = {10.1145/1731888.1731889},
abstract = {Applying any emerging technology is not trivial and requires some level of risk-taking even when the competitive advantage is clear. In the case of computational intelligence the process is even harder due to the different nature of the comprising methods, the lack of marketing, affordable professional tools and application methodology. Another important factor slowing down computational intelligence applications is the wrong perception of the technology. To many potential users it looks like it's either too expensive or it's rocket science. The pendulum of expectations also swings from one extreme of anticipating a silver bullet to all problems to the other extreme of awaiting the next technology fiasco.},
journal = {SIGEVOlution},
month = feb,
pages = {2–14},
numpages = {13}
}

@article{10.1145/1716383.1731902,
author = {Creeger, Mache},
title = {CTO Roundtable: Malware Defense: The Battle is Bigger than Most of Us Realize.},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1542-7730},
url = {https://doi.org/10.1145/1716383.1731902},
doi = {10.1145/1716383.1731902},
abstract = {As all manner of information assets migrate online, malware has kept on track to become a huge source of individual threats. In a continuously evolving game of cat and mouse, as security professionals close off points of access, attackers develop more sophisticated attacks. Today profit models from malware are comparable to any seen in the legitimate world.},
journal = {Queue},
month = feb,
pages = {40–51},
numpages = {12}
}

@inproceedings{10.1145/1718918.1718975,
author = {Mentis, Helena M. and Reddy, Madhu and Rosson, Mary Beth},
title = {Invisible Emotion: Information and Interaction in an Emergency Room},
year = {2010},
isbn = {9781605587950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718918.1718975},
doi = {10.1145/1718918.1718975},
abstract = {Emotions are an often overlooked aspect of work since they are not included in formal work models. However, they continue provide critical information as well as be part of a rich social context for action. The following study focuses on the expression of emotions within the context of a particular work environment -- an emergency room -- and high-lights how it is used, why it is invisible in the work, and how it continues to persist through workarounds. These workarounds provide indications towards the design of so-ciotechnical systems to continue to support the expression of invisible emotions.},
booktitle = {Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work},
pages = {311–320},
numpages = {10},
keywords = {er, articulation work, healthcare, documentation, emotion expressions},
location = {Savannah, Georgia, USA},
series = {CSCW '10}
}

@inproceedings{10.1145/1718918.1718938,
author = {Wang, Hao-Chuan and Cosley, Dan and Fussell, Susan R.},
title = {Idea Expander: Supporting Group Brainstorming with Conversationally Triggered Visual Thinking Stimuli},
year = {2010},
isbn = {9781605587950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718918.1718938},
doi = {10.1145/1718918.1718938},
abstract = {Creativity is central to much human problem solving and innovation. Brainstorming processes attempt to leverage group creativity, but group dynamics sometimes limit their utility. We present IdeaExpander, a tool to support group brainstorming by intelligently selecting pictorial stimuli based on the group's conversation The design is based on theories of how perception, thinking, and communication interact; a pilot study (N=16) suggests that it increases individuals' idea production and that people value it.},
booktitle = {Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work},
pages = {103–106},
numpages = {4},
keywords = {creativity support tool, group brainstorming},
location = {Savannah, Georgia, USA},
series = {CSCW '10}
}

@inproceedings{10.1145/1718918.1718965,
author = {Starbird, Kate and Palen, Leysia and Hughes, Amanda L. and Vieweg, Sarah},
title = {Chatter on the Red: What Hazards Threat Reveals about the Social Life of Microblogged Information},
year = {2010},
isbn = {9781605587950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718918.1718965},
doi = {10.1145/1718918.1718965},
abstract = {This paper considers a subset of the computer-mediated communication (CMC) that took place during the flooding of the Red River Valley in the US and Canada in March and April 2009. Focusing on the use of Twitter, a microblogging service, we identified mechanisms of information production, distribution, and organization. The Red River event resulted in a rapid generation of Twitter communications by numerous sources using a variety of communications forms, including autobiographical and mainstream media reporting, among other types. We examine the social life of microblogged information, identifying generative, synthetic, derivative and innovative properties that sustain the broader system of interaction. The landscape of Twitter is such that the production of new information is supported through derivative activities of directing, relaying, synthesizing, and redistributing, and is additionally complemented by socio-technical innovation. These activities comprise self-organization of information.},
booktitle = {Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work},
pages = {241–250},
numpages = {10},
keywords = {emergency, disaster, microblogging, risk communication, crisis informatics, computer-mediated communication},
location = {Savannah, Georgia, USA},
series = {CSCW '10}
}

@inproceedings{10.1145/1718918.1718941,
author = {Geiger, R. Stuart and Ribes, David},
title = {The Work of Sustaining Order in Wikipedia: The Banning of a Vandal},
year = {2010},
isbn = {9781605587950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1718918.1718941},
doi = {10.1145/1718918.1718941},
abstract = {In this paper, we examine the social roles of software tools in the English-language Wikipedia, specifically focusing on autonomous editing programs and assisted editing tools. This qualitative research builds on recent research in which we quantitatively demonstrate the growing prevalence of such software in recent years. Using trace ethnography, we show how these often-unofficial technologies have fundamentally transformed the nature of editing and administration in Wikipedia. Specifically, we analyze "vandal fighting" as an epistemic process of distributed cognition, highlighting the role of non-human actors in enabling a decentralized activity of collective intelligence. In all, this case shows that software programs are used for more than enforcing policies and standards. These tools enable coordinated yet decentralized action, independent of the specific norms currently in force.},
booktitle = {Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work},
pages = {117–126},
numpages = {10},
keywords = {wiki, trace ethnography, collaboration, social, bots, qualitative, ethnography, distributed cognition, wikipedia},
location = {Savannah, Georgia, USA},
series = {CSCW '10}
}

@inproceedings{10.1145/2002333.2002349,
author = {Giovannella, C. and Canale, M.},
title = {Observing an Image, Storing an Image},
year = {2010},
isbn = {9781605589992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002333.2002349},
doi = {10.1145/2002333.2002349},
abstract = {By means of an eye-tracker and a home-made software of analysis, we have investigated the relationship among the ability of the humans to memorize and recognize images (and their modifications), the characteristics of the testers (strategy of exploration, cultural background, sex, etc...) and the construction of the image.},
booktitle = {Proceedings of the 2010 Workshop on Eye Gaze in Intelligent Human Machine Interaction},
pages = {102–107},
numpages = {6},
keywords = {human communication, visual memory, eye-tracking, human factors, visual communication channel},
location = {Hong Kong, China},
series = {EGIHMI '10}
}

@inproceedings{10.1145/2002333.2002350,
author = {Giovannella, C. and Galli, G.},
title = {Visual Perception, Awareness and Self-Control: The Brentano-M\"{u}Ller-Lyer Illusion},
year = {2010},
isbn = {9781605589992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002333.2002350},
doi = {10.1145/2002333.2002350},
abstract = {In this paper we present a study on visual perception, awareness and self-control, based on the visual exploration of the classical Brentano-M\"{u}ller Lyer illusion and of some of its variants. The study has been realized with the help of self-made software designed to analyze eye-tracker recordings. In the case of subjects that have a certain familiarity with visual illusions, although not with all its variants, it is possible to observe, after some hundreds of milliseconds, the occurrence of a transition toward an aware exploration of the images. Moreover, it has been possible to show that the awareness might or might not help - it depends on the subject - to get out from the "trap" of the illusion.},
booktitle = {Proceedings of the 2010 Workshop on Eye Gaze in Intelligent Human Machine Interaction},
pages = {108–113},
numpages = {6},
keywords = {visual communication channel, human factors, visual awareness, perception of illusions, human communication, eye-tracking, brentano-m?ller lyer illusion},
location = {Hong Kong, China},
series = {EGIHMI '10}
}

@inproceedings{10.1145/1734583.1734599,
author = {Beach, Aaron and Gartrell, Mike and Xing, Xinyu and Han, Richard and Lv, Qin and Mishra, Shivakant and Seada, Karim},
title = {Fusing Mobile, Sensor, and Social Data to Fully Enable Context-Aware Computing},
year = {2010},
isbn = {9781450300056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1734583.1734599},
doi = {10.1145/1734583.1734599},
abstract = {In this paper, we identify mobile social networks as an important new direction of research in mobile computing, and show how an expanded definition of mobile social networks that includes sensor networks can enable exciting new context-aware applications, such as context-aware video screens, music jukeboxes, and mobile health applications. We offer SocialFusion as a system capable of systematically integrating such diverse mobile, social, and sensing input streams and effectuating the appropriate context-aware output action. We explain some of the major challenges that SocialFusion must overcome. We describe some preliminary results that we have obtained in implementing the SocialFusion vision.},
booktitle = {Proceedings of the Eleventh Workshop on Mobile Computing Systems &amp; Applications},
pages = {60–65},
numpages = {6},
location = {Annapolis, Maryland},
series = {HotMobile '10}
}

@inproceedings{10.1145/1755743.1755767,
author = {D\'{o}ra, L\'{a}szl\'{o} and Holczer, Tam\'{a}s},
title = {Hide-and-Lie: Enhancing Application-Level Privacy in Opportunistic Networks},
year = {2010},
isbn = {9781605589251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1755743.1755767},
doi = {10.1145/1755743.1755767},
abstract = {A delay-tolerant network is a mobile ad hoc network where the message dissemination is based on the store-carry-and-forward principle. This principle raises new aspects of the privacy problem. In particular, an attacker can build a user profile and trace the nodes based on this profile even if the message exchange protocol provides anonymity. In this paper, an attacker model is presented and some proposed attackers are implemented. We analyze the efficiency of both the attacks and the proposed defense mechanism, called Hide-and-Lie Strategy. We show that without any defense mechanism, the nodes are traceable, but with the Hide-and-Lie Strategy, the success probability of an attacker can be made equal to the success probability of the simple guessing. Furthermore, in some scenarios, the Hide-and-Lie Strategy increases the message delivery ratio. The number of downloaded messages and the maximal memory size required to apply the proposed privacy defense mechanism is also investigated.},
booktitle = {Proceedings of the Second International Workshop on Mobile Opportunistic Networking},
pages = {135–142},
numpages = {8},
keywords = {privacy in data forwarding, opportunistic networks},
location = {Pisa, Italy},
series = {MobiOpp '10}
}

@article{10.1145/1731035.1731038,
author = {Tepper, Michael and Xia, Fei},
title = {Inducing Morphemes Using Light Knowledge},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1530-0226},
url = {https://doi.org/10.1145/1731035.1731038},
doi = {10.1145/1731035.1731038},
abstract = {Allomorphic variation, or form variation among morphs with the same meaning, is a stumbling block to morphological induction (MI). To address this problem, we present a hybrid approach that uses a small amount of linguistic knowledge in the form of orthographic rewrite rules to help refine an existing MI-produced segmentation. Using rules, we derive underlying analyses of morphs---generalized with respect to contextual spelling differences---from an existing surface morph segmentation, and from these we learn a morpheme-level segmentation. To learn morphemes, we have extended the Morfessor segmentation algorithm [Creutz and Lagus 2004; 2005; 2006] by using rules to infer possible underlying analyses from surface segmentations. A segmentation produced by Morfessor Categories-MAP Software v. 0.9.2 is used as input to our procedure and as a baseline that we evaluate against. To suggest analyses for our procedure, a set of language-specific orthographic rules is needed. Our procedure has yielded promising improvements for English and Turkish over the baseline approach when tested on the Morpho Challenge 2005 and 2007 style evaluations. On the Morpho Challenge 2007 test evaluation, we report gains over the current best unsupervised contestant for Turkish, where our technique shows a 2.5% absolute F-score improvement.},
journal = {ACM Transactions on Asian Language Information Processing},
month = mar,
articleno = {3},
numpages = {38},
keywords = {computational linguistics, allomorphy, machine learning, Morphological induction}
}

@article{10.1145/1731041.1731045,
author = {Baldwin, D. and Brady, A. and Danyluk, A. and Adams, J. and Lawrence, A.},
title = {Case Studies of Liberal Arts Computer Science Programs},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
url = {https://doi.org/10.1145/1731041.1731045},
doi = {10.1145/1731041.1731045},
abstract = {Many undergraduate liberal arts institutions offer computer science majors. This article illustrates how quality computer science programs can be realized in a wide variety of liberal arts settings by describing and contrasting the actual programs at five liberal arts colleges: Williams College, Kalamazoo College, the State University of New York at Geneseo, Spelman College, and Calvin College. While the example programs differ in size, mission, and the nature of their home institutions, all take advantage of their liberal arts setting to offer rich computer science educations. Comparing these programs to each other and to the latest ACM/IEEE Computer Society computer science curriculum shows that the liberal arts programs are distinguishable from the ACM/Computer Society recommendations, but at the same time are strong undergraduate majors.},
journal = {ACM Trans. Comput. Educ.},
month = mar,
articleno = {4},
numpages = {30},
keywords = {Liberal arts}
}

@article{10.1145/1689239.1689247,
author = {Ingelrest, Fran\c{c}ois and Barrenetxea, Guillermo and Schaefer, Gunnar and Vetterli, Martin and Couach, Olivier and Parlange, Marc},
title = {SensorScope: Application-Specific Sensor Network for Environmental Monitoring},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/1689239.1689247},
doi = {10.1145/1689239.1689247},
abstract = {SensorScope is a turnkey solution for environmental monitoring systems, based on a wireless sensor network and resulting from a collaboration between environmental and network researchers. Given the interest in climate change, environmental monitoring is a domain where sensor networks will have great impact by providing high resolution spatio-temporal data for long periods of time. SensorScope is such a system, which has already been successfully deployed multiple times in various environments (e.g., mountainous, urban). Here, we describe the overall hardware and software architectures and especially focus on the sensor network itself. We also describe one of our most prominent deployments, on top of a rock glacier in Switzerland, which resulted in the description of a micro-climate phenomenon leading to cold air release from a rock-covered glacier in a region of high alpine risks. Another focus of this paper is the description of what happened behind the scenes to turn SensorScope from a laboratory experiment into successful outdoor deployments in harsh environments. Illustrated by various examples, we point out many lessons learned while working on the project. We indicate the importance of simple code, well suited to the application, as well as the value of close interaction with end-users in planning and running the network and finally exploiting the data.},
journal = {ACM Trans. Sen. Netw.},
month = mar,
articleno = {17},
numpages = {32},
keywords = {implementation, wireless sensor network, deployment, environmental monitoring, Architecture}
}

@inproceedings{10.5555/1734454.1734561,
author = {Yu, Chen and Scheutz, Matthias and Schermerhorn, Paul},
title = {Investigating Multimodal Real-Time Patterns of Joint Attention in an Hri Word Learning Task},
year = {2010},
isbn = {9781424448937},
publisher = {IEEE Press},
abstract = {Joint attention - the idea that humans make inferences from observable behaviors of other humans by attending to the objects and events that these others humans attend to - has been recognized as a critical component in human-robot interactions. While various HRI studies showed that having robots to behave in ways that support human recognition of joint attention leads to better behavioral outcomes on the human side, there are no studies that investigate the detailed time course of interactive joint attention processes.In this paper, we present the results from an HRI study that investigates the exact time course of human multi-modal attentional processes during an HRI word learning task in an unprecedented way. Using novel data analysis techniques, we are able to demonstrate that the temporal details of human attentional behavior are critical for understanding human expectations of joint attention in HRI and that failing to do so can force humans into assuming unnatural behaviors.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {309–316},
numpages = {8},
keywords = {joint attention, human-robot interaction},
location = {Osaka, Japan},
series = {HRI '10}
}

@inproceedings{10.5555/1734454.1734468,
author = {Weiss, Astrid and Igelsb\"{o}ck, Judith and Tscheligi, Manfred and Bauer, Andrea and K\"{u}hnlenz, Kolja and Wollherr, Dirk and Buss, Martin},
title = {Robots Asking for Directions: The Willingness of Passers-by to Support Robots},
year = {2010},
isbn = {9781424448937},
publisher = {IEEE Press},
abstract = {This paper reports about a human-robot interaction field trial conducted with the autonomous mobile robot ACE (Autonomous City Explorer) in a public place, where the ACE robot needs the support of human passers-by to find its way to a target location. Since the robot does not possess any prior map knowledge or GPS support, it has to acquire missing information through interaction with humans. The robot thus has to initiate communication by asking for the way, and retrieves information from passers-by showing the way by gestures (pointing) and marking goal positions on a still image on the touch screen of the robot. The aims of the field trial where threefold: (1) Investigating the aptitude of the navigation architecture, (2) Evaluating the intuitiveness of the interaction concept for the passers-by, (3) Assessing people's willingness to support the ACE robot in its task, i.e. assessing the social acceptability. The field trial demonstrates that the architecture enables successful autonomous path finding without any prior map knowledge just by route directions given by passers-by. An additional street survey and observational data moreover attests the intuitiveness of the interaction paradigm and the high acceptability of the ACE robot in the public place.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {23–30},
numpages = {8},
keywords = {field trial, human-robot interaction, social acceptance, autonomous mobile robot},
location = {Osaka, Japan},
series = {HRI '10}
}

@inproceedings{10.5555/1870926.1871064,
author = {Mintarno, Evelyn and Skaf, Jo\"{e}lle and Zheng, Rui and Velamala, Jyothi and Cao, Yu and Boyd, Stephen and Dutton, Robert W. and Mitra, Subhasish},
title = {Optimized Self-Tuning for Circuit Aging},
year = {2010},
isbn = {9783981080162},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {We present a framework and control policies for optimizing dynamic control of various self-tuning parameters over lifetime in the presence of circuit aging. Our framework introduces dynamic cooling as one of the self-tuning parameters, in addition to supply voltage and clock frequency. Our optimized self-tuning satisfies performance constraints at all times and maximizes a lifetime computational power efficiency (LCPE) metric, which is defined as the total number of clock cycles achieved over lifetime divided by the total energy consumed over lifetime. Our framework features three control policies: 1. Progressive-worst-case-aging (PWCA), which assumes worst-case aging at all times; 2. Progressive-on-state-aging (POSA), which estimates aging by tracking active/sleep mode, and then assumes worst-case aging in active mode and long recovery effects in sleep mode; 3. Progressive-real-time-aging-assisted (PRTA), which estimates the actual amount of aging and initiates optimized control action. Simulation results on benchmark circuits, using aging models validated by 45nm CMOS stress measurements, demonstrate the practicality and effectiveness of our approach. We also analyze design constraints and derive system design guidelines to maximize self-tuning benefits.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {586–591},
numpages = {6},
location = {Dresden, Germany},
series = {DATE '10}
}

@inproceedings{10.5555/1870926.1871057,
author = {Marinissen, Erik Jan and Singh, Adit and Glotter, Dan and Esposito, Marco and Carulli, John M. and Nahar, Amit and Butler, Kenneth M. and Appello, Davide and Portelli, Chris},
title = {Adapting to Adaptive Testing},
year = {2010},
isbn = {9783981080162},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Adaptive testing is a generic term for a number of techniques which aim at improving the test quality and/or reducing the test application costs. In adaptive tests, the test content or pass/fail limits are not fixed as in conventional tests, but dependent on other test results of the currently or previously tested chips. Part-average testing, outlier detection, and neighborhood screening are just a few examples of adaptive testing. With this Embedded Tutorial, we are offering an introduction to this topic, which is hot in the test community, to the wider DATE audience.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {556–561},
numpages = {6},
location = {Dresden, Germany},
series = {DATE '10}
}

@inproceedings{10.1145/1734263.1734279,
author = {Sahami, Mehran and Aiken, Alex and Zelenski, Julie},
title = {Expanding the Frontiers of Computer Science: Designing a Curriculum to Reflect a Diverse Field},
year = {2010},
isbn = {9781450300063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1734263.1734279},
doi = {10.1145/1734263.1734279},
abstract = {While the discipline of computing has evolved significantly in the past 30 years, Computer Science curricula have not as readily adapted to these changes. In response, we have recently completely redesigned the undergraduate CS curriculum at Stanford University, both modernizing the program as well as highlighting new directions in the field and its multi-disciplinary nature. As we explain in this paper, our restructured major features a streamlined core of foundation courses followed by a depth concentration in a track area as well as additional elective courses. Since its deployment this past year, the new program has proven to be very attractive to students, contributing to an increase of over 40% in the number of CS major declarations. We analyze feedback we received on the program from students, as well as commentary from industrial affiliates and other universities, providing further evidence of the promise this new curriculum holds.},
booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
pages = {47–51},
numpages = {5},
keywords = {concentrations, tracks, curriculum, multi-disciplinary},
location = {Milwaukee, Wisconsin, USA},
series = {SIGCSE '10}
}

@inproceedings{10.1145/1734263.1734388,
author = {Thomas, Stan J. and Whitener, Paul M.},
title = {In the Zone: Virtual Computing on a Budget},
year = {2010},
isbn = {9781450300063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1734263.1734388},
doi = {10.1145/1734263.1734388},
abstract = {In this paper, we report on our efforts, extending over several years, to provide computer science students experience with a variety of operating system and computing environments. We describe our explorations into the use of virtual machine environments for instructional purposes, explorations that have led to the current multifaceted approach to virtualization. We also demonstrate that implementing a diverse, sophisticated virtual computing environment does not require a large investment in computer hardware, in fact it can lead to a cost saving by extending the useful life of systems and reducing the complexity of system administration.},
booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
pages = {366–370},
numpages = {5},
keywords = {operating systems, virtualization, computing environments},
location = {Milwaukee, Wisconsin, USA},
series = {SIGCSE '10}
}

@inproceedings{10.1145/1734263.1734299,
author = {Kaczmarczyk, Lisa C. and Petrick, Elizabeth R. and East, J. Philip and Herman, Geoffrey L.},
title = {Identifying Student Misconceptions of Programming},
year = {2010},
isbn = {9781450300063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1734263.1734299},
doi = {10.1145/1734263.1734299},
abstract = {Computing educators are often baffled by the misconceptions that their CS1 students hold. We need to understand these misconceptions more clearly in order to help students form correct conceptions. This paper describes one stage in the development of a concept inventory for Computing Fundamentals: investigation of student misconceptions in a series of core CS1 topics previously identified as both important and difficult. Formal interviews with students revealed four distinct themes, each containing many interesting misconceptions. Three of those misconceptions are detailed in this paper: two misconceptions about memory models, and data assignment when primitives are declared. Individual misconceptions are related, but vary widely, thus providing excellent material to use in the development of the CI. In addition, CS1 instructors are provided immediate usable material for helping their students understand some difficult introductory concepts.},
booktitle = {Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
pages = {107–111},
numpages = {5},
keywords = {programming, misconceptions, pedagogy, curriculum, concept inventory, cs1},
location = {Milwaukee, Wisconsin, USA},
series = {SIGCSE '10}
}

@inproceedings{10.1145/1735688.1735707,
author = {Alshawabkeh, Malak and Jang, Byunghyun and Kaeli, David},
title = {Accelerating the Local Outlier Factor Algorithm on a GPU for Intrusion Detection Systems},
year = {2010},
isbn = {9781605589350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1735688.1735707},
doi = {10.1145/1735688.1735707},
abstract = {The Local Outlier Factor (LOF) is a very powerful anomaly detection method available in machine learning and classification. The algorithm defines the notion of local outlier in which the degree to which an object is outlying is dependent on the density of its local neighborhood, and each object can be assigned an LOF which represents the likelihood of that object being an outlier. Although this concept of a local outlier is a useful one, the computation of LOF values for every data object requires a large number of k-nearest neighbor queries -- this overhead can limit the use of LOF due to the computational overhead involved.Due to the growing popularity of Graphics Processing Units (GPU) in general-purpose computing domains, and equipped with a high-level programming language designed specifically for general-purpose applications (e.g., CUDA), we look to apply this parallel computing approach to accelerate LOF. In this paper we explore how to utilize a CUDA-based GPU implementation of the k-nearest neighbor algorithm to accelerate LOF classification. We achieve more than a 100X speedup over a multi-threaded dual-core CPU implementation. We also consider the impact of input data set size, the neighborhood size (i.e., the value of k) and the feature space dimension, and report on their impact on execution time.},
booktitle = {Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units},
pages = {104–110},
numpages = {7},
keywords = {LOF, intrusion detection system, parallelization, GPU},
location = {Pittsburgh, Pennsylvania, USA},
series = {GPGPU-3}
}

@inproceedings{10.1145/1743666.1743694,
author = {Moshnyaga, Vasily G.},
title = {The Use of Eye Tracking for PC Energy Management},
year = {2010},
isbn = {9781605589947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1743666.1743694},
doi = {10.1145/1743666.1743694},
abstract = {This paper discusses a new application of eye-tracking, namely power management, and outlines its implementation in personal computer system. Unlike existing power management technology, which "senses" a PC user through keyboard and/or mouse, our technology "watches" the user through a single camera. The technology tracks the user's eyes keeping the display active only if the user looks at the screen. Otherwise it dims the display down or even switches it off to save energy. We implemented the technology in hardware and present the results of its experimental evaluation.},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications},
pages = {113–116},
numpages = {4},
keywords = {applications, energy reduction, eye tracking},
location = {Austin, Texas},
series = {ETRA '10}
}

@inproceedings{10.1145/1743666.1743718,
author = {Jarodzka, Halszka and Holmqvist, Kenneth and Nystr\"{o}m, Marcus},
title = {A Vector-Based, Multidimensional Scanpath Similarity Measure},
year = {2010},
isbn = {9781605589947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1743666.1743718},
doi = {10.1145/1743666.1743718},
abstract = {A great need exists in many fields of eye-tracking research for a robust and general method for scanpath comparisons. Current measures either quantize scanpaths in space (string editing measures like the Levenshtein distance) or in time (measures based on attention maps). This paper proposes a new pairwise scanpath similarity measure. Unlike previous measures that either use AOI sequences or forgo temporal order, the new measure defines scanpaths as a series of geometric vectors and compares temporally aligned scanpaths across several dimensions: shape, fixation position, length, direction, and fixation duration. This approach offers more multifaceted insights to how similar two scanpaths are. Eight fictitious scanpath pairs are tested to elucidate the strengths of the new measure, both in itself and compared to two of the currently most popular measures - the Levenshtein distance and attention map correlation.},
booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications},
pages = {211–218},
numpages = {8},
keywords = {sequence analysis, string edit, Levenshtein distance, scanpath, vector},
location = {Austin, Texas},
series = {ETRA '10}
}

@inproceedings{10.1145/1739041.1739076,
author = {Fang, Lujun and LeFevre, Kristen},
title = {Splash: Ad-Hoc Querying of Data and Statistical Models},
year = {2010},
isbn = {9781605589459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1739041.1739076},
doi = {10.1145/1739041.1739076},
abstract = {Data mining is increasingly performed by people who are not computer scientists or professional programmers. It is often done as an iterative process involving multiple ad-hoc tasks, as well as data pre- and post-processing, all of which must be executed over large databases. In order to make data mining more accessible, it is critical to provide a simple, easy-to-use language that allows the user to specify ad-hoc data processing, model construction, and model manipulation. Simultaneously, it is necessary for the underlying system to scale up to large datasets. Unfortunately, while each of these requirements can be satisfied, individually, by existing systems, no system fully satisfies all criteria.In this paper, we present a system called Splash to fill this void. Splash supports an extended relational data model and SQL query language, which allows for the natural integration of statistical modeling and ad-hoc data processing. It also supports a novel representatives operator to help explain models using a limited number of examples. We have developed a prototype implementation of Splash. Our experimental study indicates that it scales well to large input datasets. Further, to demonstrate the simplicity of the language, we conducted a case study using Splash to perform a series of exploratory analyses using network log data. Our study indicates that the query-based interface is simpler than a common data mining software package, and it often requires less programming effort to use.},
booktitle = {Proceedings of the 13th International Conference on Extending Database Technology},
pages = {275–286},
numpages = {12},
location = {Lausanne, Switzerland},
series = {EDBT '10}
}

@inproceedings{10.1145/1774088.1774246,
author = {Doman, Marguerite and Payton, Jamie and Dahlberg, Teresa},
title = {Leveraging Fuzzy Query Processing to Support Applications in Wireless Sensor Networks},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774246},
doi = {10.1145/1774088.1774246},
abstract = {In this paper, we describe a fuzzy query processing approach to support application development in sensor networks. Using a fuzzy query, an application programmer can provide a linguistic and semantic specification of the desired data, eliminating the need to specify explicit and exact thresholds as part of a query. The returned fuzzy query results are each associated with a degree of membership measurement that indicates how closely each returned data value matches the semantic intent of the fuzzy query, providing applications with additional information that can be used to reason about the query result. Our approach to in-network fuzzy query processing allows for each sensor node to tailor its evaluation of a fuzzy query; this feature allows for consideration of micro-environments embedded within the sensor network that can impact how individual sensor data values should be interpreted with respect to the semantic intent of the query. To demonstrate that a fuzzy query processing approach is feasible, we use an application scenario to evaluate the implementation of our fuzzy query processing system in a simulated sensor network environment; results show that precision and overhead for our approach are comparable to traditional query processing.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {764–771},
numpages = {8},
keywords = {fuzzy query processing, wireless sensor networks, data management},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/1754239.1754276,
author = {Ichim, Daniela},
title = {Quantile-Based Bootstrap Methods to Generate Continuous Synthetic Data},
year = {2010},
isbn = {9781605589909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754239.1754276},
doi = {10.1145/1754239.1754276},
abstract = {To face the increasing demand from users, National Statistical Institutes (NSI) release different information products. The dissemination of this information should be performed in full compliance with the regulations pertaining to the privacy of respondents. One product that could belong to a dissemination portfolio is represented by synthetic data. In this paper a very brief review of several methods to generate synthetic data is given. The emphasis is put on bootstrap methods that might be used in complex surveys. A quantile-based bootstrap method is proposed, avoiding any model assumption. Different bootstrap strategies were empirically compared from the point of view of some univariate statistics and in a linear regression framework. The Italian Structure of Earnings Survey 2006 data were used in these preliminary experiments.},
booktitle = {Proceedings of the 2010 EDBT/ICDT Workshops},
articleno = {33},
numpages = {10},
keywords = {microdata dissemination, data utility, synthetic data, statistical disclosure control, bootstrap, privacy},
location = {Lausanne, Switzerland},
series = {EDBT '10}
}

@inproceedings{10.1145/1774088.1774505,
author = {Lee, Jusuk and Jeong, Kyoochang and Lee, Heejo},
title = {Detecting Metamorphic Malwares Using Code Graphs},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774505},
doi = {10.1145/1774088.1774505},
abstract = {Malware writers and detectors have been running an endless battle. Self-defense is the weapon most malware writers prepare against malware detectors. Malware writers have tried to evade the improved detection techniques of anti-virus(AV) products. Packing and code obfuscation are two popular evasion techniques. When these techniques are applied to malwares, they are able to change their instruction sequence while maintaining their intended function. We propose a detection mechanism defeating these self-defense techniques to improve malware detection. Since an obfuscated malware is able to change the syntax of its code while preserving its semantics, the proposed mechanism uses the semantic invariant. We convert the API call sequence of the malware into a graph, commonly known as a call graph, to extract the semantic of the malware. The call graph can be reduced to a code graph used for semantic signatures of the proposed mechanism. We show that the code graph can represent the characteristics of a program exactly and uniquely. Next, we evaluate the proposed mechanism by experiment. The mechanism has an 91% detection ratio of real-world malwares and detects 300 metamorphic malwares that can evade AV scanners. In this paper, we show how to analyze malwares by extracting program semantics using static analysis. It is shown that the proposed mechanism provides a high possibility of detecting malwares even when they attempt self-protection.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {1970–1977},
numpages = {8},
keywords = {code graph, static analysis, metamorphic malware, code obfuscation},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/1739041.1739095,
author = {Barbieri, Davide Francesco and Braga, Daniele and Ceri, Stefano and Grossniklaus, Michael},
title = {An Execution Environment for C-SPARQL Queries},
year = {2010},
isbn = {9781605589459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1739041.1739095},
doi = {10.1145/1739041.1739095},
abstract = {Continuous SPARQL (C-SPARQL) is proposed as new language for continuous queries over streams of RDF data. It covers a gap in the Semantic Web abstractions which is needed for many emerging applications, including our focus on Urban Computing. In this domain, sensor-based information on roads must be processed to deduce localized traffic conditions and then produce traffic management strategies. Executing C-SPARQL queries requires the effective integration of SPARQL and streaming technologies, which capitalize over a decade of research and development; such integration poses several nontrivial challenges.In this paper we (a) show the syntax and semantics of the C-SPARQL language together with some examples; (b) introduce a query graph model which is an intermediate representation of queries devoted to optimization; (c) discuss the features of an execution environment that leverages existing technologies; (d) introduce optimizations in terms of rewriting rules applied to the query graph model, so as to efficiently exploit the execution environment; and (e) show evidence of the effectiveness of our optimizations on a prototype of execution environment.},
booktitle = {Proceedings of the 13th International Conference on Extending Database Technology},
pages = {441–452},
numpages = {12},
location = {Lausanne, Switzerland},
series = {EDBT '10}
}

@inproceedings{10.1145/1774088.1774344,
author = {de Santana, Vagner Figuer\^{e}do and Baranauskas, M. Cec\'{\i}lia C.},
title = {Summarizing Observational Client-Side Data to Reveal Web Usage Patterns},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774344},
doi = {10.1145/1774088.1774344},
abstract = {Client-side event logs may reveal patterns of usage of Web pages. Nevertheless, extracting useful and novel information from this voluminous data set is a challenge for evaluation tools, since a few minutes simple task may result in a sequence of hundreds of events. This work contributes with a technique to process these logs and build a Web page's usage graph summarizing statistical information of the Web page usage concerning one or more sessions. This graph reveals patterns of real usage data, which Human-Computer Interaction specialists may find useful for inspecting accessibility and usability issues. Moreover, Web usage miners can reuse the usage graph to apply other techniques to discover other patterns or rules.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {1219–1223},
numpages = {5},
keywords = {websites evaluation tool, usage patterns, client-side event logs},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/1774088.1774449,
author = {Malkowski, Simon and Jayasinghe, Deepal and Hedwig, Markus and Park, Junhee and Kanemasa, Yasuhiko and Pu, Calton},
title = {Empirical Analysis of Database Server Scalability Using an N-Tier Benchmark with Read-Intensive Workload},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774449},
doi = {10.1145/1774088.1774449},
abstract = {The performance evaluation of database servers in N-tier applications is a serious challenge due to requirements such as non-stationary complex workloads and global consistency management when replicating database servers. We conducted an experimental evaluation of database server scalability and bottleneck identification in N-tier applications using the RUBBoS benchmark. Our experiments are comprised of a full scale-out mesh with up to nine database servers and three application servers. Additionally, the fourtier system was run in a variety of configurations, including two database management systems (MySQL and PostgreSQL), two hardware node types (normal and low-cost), and two database replication techniques (C-JDBC and MySQL Cluster). In this paper we present the analysis of results generated with a read-intensive interaction pattern (browse-only workload) in the client emulator. These empirical data can be divided into two kinds. First, for a relatively small number of servers, we find simple hardware resource bottlenecks. Consequently, system throughput increases with an increasing number of database (and application) servers. Second, when sufficient hardware resources are available, non-obvious database related bottlenecks have been found that limit system throughput. While the first kind of bottlenecks shows that there are similarities between database and application/web server scalability, the second kind of bottlenecks shows that database servers have significantly higher sophistication and complexity that require in-depth evaluation and analysis.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {1680–1687},
numpages = {8},
keywords = {bottleneck, database replication, distributed systems, middleware, N-tier applications, RUBBoS},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@article{10.1145/1670679.1670680,
author = {Salfner, Felix and Lenk, Maren and Malek, Miroslaw},
title = {A Survey of Online Failure Prediction Methods},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/1670679.1670680},
doi = {10.1145/1670679.1670680},
abstract = {With the ever-growing complexity and dynamicity of computer systems, proactive fault management is an effective approach to enhancing availability. Online failure prediction is the key to such techniques. In contrast to classical reliability methods, online failure prediction is based on runtime monitoring and a variety of models and methods that use the current state of a system and, frequently, the past experience as well. This survey describes these methods. To capture the wide spectrum of approaches concerning this area, a taxonomy has been developed, whose different approaches are explained and major concepts are described in detail.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {10},
numpages = {42},
keywords = {Error, prediction metrics, runtime monitoring, failure prediction, fault}
}

@inproceedings{10.1145/1953611.1953624,
author = {Monteiro, Pedro and Monteiro, Miguel P.},
title = {A Pattern Language for Parallelizing Irregular Algorithms},
year = {2010},
isbn = {9781450301275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1953611.1953624},
doi = {10.1145/1953611.1953624},
abstract = {In irregular algorithms, data set's dependences and distributions cannot be statically predicted. This class of algorithms tends to organize computations in terms of data locality instead of parallelizing control in multiple threads. Thus, opportunities for exploiting parallelism vary dynamically, according to how the algorithm changes data dependences. This paper presents the first part of a pattern language for creating parallel implementations of irregular algorithms and applications. Four patterns are proposed: Amorphous Data-Parallelism, Data-Parallel Graph, Optimistic Iteration and In-Order Iteration.},
booktitle = {Proceedings of the 2010 Workshop on Parallel Programming Patterns},
articleno = {13},
numpages = {14},
location = {Carefree, Arizona, USA},
series = {ParaPLoP '10}
}

@article{10.1145/1721654.1721670,
author = {Creeger, Mache},
title = {CTO Roundtable: Malware Defense},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1721654.1721670},
doi = {10.1145/1721654.1721670},
abstract = {The battle is bigger than most of us realize.},
journal = {Commun. ACM},
month = apr,
pages = {43–49},
numpages = {7}
}

@article{10.1145/1721654.1721672,
author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
title = {A View of Cloud Computing},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1721654.1721672},
doi = {10.1145/1721654.1721672},
abstract = {Clearing the clouds away from the true potential and obstacles posed by this computing capability.},
journal = {Commun. ACM},
month = apr,
pages = {50–58},
numpages = {9}
}

@inproceedings{10.1145/1785455.1785462,
author = {Mikhail, Mina and El-Ayat, Khaled and El Kaliouby, Rana and Coan, James and Allen, John J. B.},
title = {Emotion Detection Using Noisy EEG Data},
year = {2010},
isbn = {9781605588254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1785455.1785462},
doi = {10.1145/1785455.1785462},
abstract = {Emotion is an important aspect in the interaction between humans. It is fundamental to human experience and rational decision-making. There is a great interest for detecting emotions automatically. A number of techniques have been employed for this purpose using channels such as voice and facial expressions. However, these channels are not very accurate because they can be affected by users' intentions. Other techniques use physiological signals along with electroencephalography (EEG) for emotion detection. However, these approaches are not very practical for real time applications because they either ask the participants to reduce any motion and facial muscle movement or reject EEG data contaminated with artifacts. In this paper, we propose an approach that analyzes highly contaminated EEG data produced from a new emotion elicitation technique. We also use a feature selection mechanism to extract features that are relevant to the emotion detection task based on neuroscience findings. We reached an average accuracy of 51% for joy emotion, 53% for anger, 58% for fear and 61% for sadness.},
booktitle = {Proceedings of the 1st Augmented Human International Conference},
articleno = {7},
numpages = {7},
keywords = {feature extraction, support vector machines, brain signals, affective computing},
location = {Meg\`{e}ve, France},
series = {AH '10}
}

@article{10.1145/1721831.1721833,
author = {Kostakos, Vassilis and O'Neill, Eamonn and Penn, Alan and Roussos, George and Papadongonas, Dikaios},
title = {Brief Encounters: Sensing, Modeling and Visualizing Urban Mobility and Copresence Networks},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/1721831.1721833},
doi = {10.1145/1721831.1721833},
abstract = {Moving human-computer interaction off the desktop and into our cities requires new approaches to understanding people and technologies in the built environment. We approach the city as a system, with human, physical and digital components and behaviours. In creating effective and usable urban pervasive computing systems, we need to take into account the patterns of movement and encounter amongst people, locations, and mobile and fixed devices in the city. Advances in mobile and wireless communications have enabled us to detect and record the presence and movement of devices through cities. This article makes a number of methodological and empirical contributions. We present a toolkit of algorithms and visualization techniques that we have developed to model and make sense of spatial and temporal patterns of mobility, presence, and encounter. Applying this toolkit, we provide an analysis of urban Bluetooth data based on a longitudinal dataset containing millions of records associated with more than 70000 unique devices in the city of Bath, UK. Through a novel application of established complex network analysis techniques, we demonstrate a significant finding on the relationship between temporal factors and network structure. Finally, we suggest how our understanding and exploitation of these data may begin to inform the design and use of urban pervasive systems.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {2},
numpages = {38},
keywords = {visualisation, ubiquitous, social network, copresence, brief encounter, urban computing, mobility, Pervasive, trail, sensing, information diffusion, mobile interaction, Bluetooth, encounter, complex network, virus, epidemic}
}

@article{10.1145/1764873.1764883,
author = {claffy, k. c.},
title = {Workshop on Internet Economics (WIE2009) Report},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/1764873.1764883},
doi = {10.1145/1764873.1764883},
abstract = {On September 23, 2009, CAIDA hosted a virtual Workshop on Internet Economics [3] to bring together network technology and policy researchers, commercial Internet facilities and service providers, and communications regulators to explore a common goal: framing a concrete agenda for the emerging but empirically stunted field of Internet infrastructure economics. With participants stretching from Washington D.C. to Queensland, Australia, we used the electronic conference hosting facilities supported by the California In stitute of Technology (CalTech) EVO Collaboration Network. This report describes the workshop discussions and presents relevant open research questions identified by participants.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = apr,
pages = {56–59},
numpages = {4},
keywords = {economics, network management, internet}
}

@article{10.1145/1764873.1764884,
author = {Mahajan, Ratul},
title = {How to Build a Research System in Your Spare Time},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/1764873.1764884},
doi = {10.1145/1764873.1764884},
abstract = {This paper is based on a talk that I gave at CoNEXT 2009. Inspired by Hal Varian's paper on building economic models, it describes are search method for building computer systems. I find this method useful in my work and hope that some readers will find it helpful as well.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = apr,
pages = {60–65},
numpages = {6},
keywords = {computer system, research method}
}

@inproceedings{10.1145/1753326.1753646,
author = {Kern, Dagmar and Marshall, Paul and Schmidt, Albrecht},
title = {Gazemarks: Gaze-Based Visual Placeholders to Ease Attention Switching},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753646},
doi = {10.1145/1753326.1753646},
abstract = {Many tasks require attention switching. For example, searching for information on one sheet of paper and then entering this information onto another one. With paper we see that people use fingers or objects as placeholders. Using these simple aids, the process of switching attention between displays can be simplified and speeded up. With large or multiple visual displays we have many tasks where both attention areas are on the screen and where using a finger as a placeholder is not suitable. One way users deal with this is to use the mouse and highlight their current focus. However, this also has its limitations -- in particular in environments where there is no pointing device. Our approach is to utilize the user's gaze position to provide a visual placeholder. The last area where a user fixated on the screen (before moving their attention away) is highlighted; we call this visual reminder a Gazemark. Gazemarks ease orientation and the resumption of the interrupted task when coming back to this display. In this paper we report on a study where the effectiveness of using Gazemarks was investigated, in particular we show how they can ease attention switching. Our results show faster completion times for a resumed simple visual search task when using this technique. The paper analyzes relevant parameters for the implementation of Gazemarks and discusses some further application areas for this approach.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2093–2102},
numpages = {10},
keywords = {eye-gaze interaction, gazemarks, attention switching},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753650,
author = {Wilcox, Lauren and Morris, Dan and Tan, Desney and Gatewood, Justin},
title = {Designing Patient-Centric Information Displays for Hospitals},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753650},
doi = {10.1145/1753326.1753650},
abstract = {Electronic medical records are increasingly comprehensive, and this vast repository of information has already contri-buted to medical efficiency and hospital procedure. However, this information is not typically accessible to patients, who are frequently under-informed and unclear about their own hospital courses. In this paper, we propose a design for in-room, patient-centric information displays, based on iterative design with physicians. We use this as the basis for a Wizard-of-Oz study in an emergency department, to assess patient and provider responses to in-room information displays. 18 patients were presented with real-time information displays based on their medical records. Semi-structured interviews with patients, family members, and hospital staff reveal that subjective response to in-room displays was overwhelmingly positive, and through these interviews we elicited guidelines regarding specific information types, privacy, use cases, and information presentation techniques. We describe these findings, and we discuss the feasibility of a fully-automatic implementation of our design.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2123–2132},
numpages = {10},
keywords = {electronic medical records, patient awareness},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753605,
author = {Kim, Sunyoung and Paulos, Eric},
title = {InAir: Sharing Indoor Air Quality Measurements and Visualizations},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753605},
doi = {10.1145/1753326.1753605},
abstract = {This paper describes inAir, a tool for sharing measurements and visualizations of indoor air quality within one's social network. Poor indoor air quality is difficult for humans to detect through sight and smell alone and can contribute to the development of chronic diseases. Through a four-week long study of fourteen households as six groups, we found that inAir (1) increased awareness of, and reflection on air quality, (2) promoted behavioral changes that resulted in improved indoor air quality, and (3) demonstrated the persuasive power of sharing for furthering improvements to indoor air quality in terms of fostering new social awareness and behavior changes as well as strengthening social bonds and prompting collaborative efforts across social networks to improve human health and well being.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1861–1870},
numpages = {10},
keywords = {iphone, persuasive technology, sensors, air quality, health, sustainability, domestic technology, environment},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753569,
author = {Hirano, Sen H. and Yeganyan, Michael T. and Marcu, Gabriela and Nguyen, David H. and Boyd, Lou Anne and Hayes, Gillian R.},
title = {VSked: Evaluation of a System to Support Classroom Activities for Children with Autism},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753569},
doi = {10.1145/1753326.1753569},
abstract = {Visual schedules--the use of symbols to represent a series of activities or steps--have been successfully used by caregivers to help children with autism to understand, structure, and predict activities in their daily lives. Building from in-depth fieldwork and participatory design sessions, we developed vSked, an interactive and collaborative visual scheduling system designed for elementary school classrooms. We evaluated vSked in situ in one autism-specific classroom over three weeks. In this paper, we present the design principles, technical solution, and results from this successful deployment. Use of vSked resulted in reductions in staff effort required to use visual supports. vSked also resulted in improvements in the perceived quality and quantity of communication and social interactions in the classroom.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1633–1642},
numpages = {10},
keywords = {education, autism, assistive technology, visual supports},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753349,
author = {Birnholtz, Jeremy and Jones-Rounds, McKenzie},
title = {Independence and Interaction: Understanding Seniors' Privacy and Awareness Needs for Aging in Place},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753349},
doi = {10.1145/1753326.1753349},
abstract = {As America's baby boom population gets older, aging in place -- the idea that seniors can remain independent in a comfortable home environment while being monitored and receiving care from family and caregivers living elsewhere -- has received significant attention. Fostering a sense of independence while simultaneously enabling monitoring and frequent interaction can seem paradoxical, however. This raises questions of how we can design technologies that help seniors retain their independence and a sense of comfort, while still interacting with and being monitored regularly by others. We present results from an interview study of 30 seniors, caregivers and relatives in which we sought to understand how they managed their interactions, availability, privacy and independence. Results suggest that they rely on attributes of the physical environment, temporal structures such as routine conversations and activities, and technological mediation.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {143–152},
numpages = {10},
keywords = {seniors, awareness, home, aging in place, privacy},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753470,
author = {Bernstein, Michael S. and Marcus, Adam and Karger, David R. and Miller, Robert C.},
title = {Enhancing Directed Content Sharing on the Web},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753470},
doi = {10.1145/1753326.1753470},
abstract = {To find interesting, personally relevant web content, people rely on friends and colleagues to pass links along as they encounter them. In this paper, we study and augment link-sharing via e-mail, the most popular means of sharing web content today. Armed with survey data indicating that active sharers of novel web content are often those that actively seek it out, we developed FeedMe, a plug-in for Google Reader that makes directed sharing of content a more salient part of the user experience. FeedMe recommends friends who may be interested in seeing content that the user is viewing, provides information on what the recipient has seen and how many emails they have received recently, and gives recipients the opportunity to provide lightweight feedback when they appreciate shared content. FeedMe introduces a novel design space within mixed-initiative social recommenders: friends who know the user voluntarily vet the material on the user's behalf. We performed a two-week field experiment (N=60) and found that FeedMe made it easier and more enjoyable to share content that recipients appreciated and would not have found otherwise.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {971–980},
numpages = {10},
keywords = {social link sharing, rss, blogs, friendsourcing},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753408,
author = {Dow, Steven P. and Mehta, Manish and MacIntyre, Blair and Mateas, Michael},
title = {Eliza Meets the Wizard-of-Oz: Blending Machine and Human Control of Embodied Characters},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753408},
doi = {10.1145/1753326.1753408},
abstract = {What authoring possibilities arise by blending machine and human control of live embodied character experiences? This paper explores two different "behind-the-scenes" roles for human operators during a three-month gallery installation of an embodied character experience. In the Transcription role, human operators type players' spoken utterances; then, algorithms interpret the player's intention, choose from pre-authored dialogue based on local and global narrative contexts, and procedurally animate two embodied characters. In the Discourse role, human operators select from semantic categories to interpret player intention; algorithms use this "discourse act" to automate character dialogue and animation. We compare these two methods of blending control using game logs and interviews, and document how the amateur operators initially resisted having to learn the Discourse version, but eventually preferred having the authorial control it afforded. This paper also outlines a design space for blending machine and human control in live character experiences.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {547–556},
numpages = {10},
keywords = {interactive drama, artificial intelligence, wizard-of-oz methods, embodied characters},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753653,
author = {Ashbrook, Daniel and Starner, Thad},
title = {MAGIC: A Motion Gesture Design Tool},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753653},
doi = {10.1145/1753326.1753653},
abstract = {Devices capable of gestural interaction through motion sensing are increasingly becoming available to consumers; however, motion gesture control has yet to appear outside of game consoles. Interaction designers are frequently not expert in pattern recognition, which may be one reason for this lack of availability. Another issue is how to effectively test gestures to ensure that they are not unintentionally activated by a user's normal movements during everyday usage. We present MAGIC, a gesture design tool that addresses both of these issues, and detail the results of an evaluation.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2159–2168},
numpages = {10},
keywords = {gesture},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753719,
author = {Wyche, Susan P. and Smyth, Thomas N. and Chetty, Marshini and Aoki, Paul M. and Grinter, Rebecca E.},
title = {Deliberate Interactions: Characterizing Technology Use in Nairobi, Kenya},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753719},
doi = {10.1145/1753326.1753719},
abstract = {We present results from a qualitative study examining how professionals living and working in Nairobi, Kenya regularly use ICT in their everyday lives. There are two contributions of this work for the HCI community. First, we provide empirical evidence demonstrating constraints our participants encountered when using technology in an infrastructure-poor setting. These constraints are limited bandwidth, high costs, differing perceptions of responsiveness, and threats to physical and virtual security. Second, we use our findings to critically evaluate the "access, anytime and anywhere" construct shaping the design of future technologies. We present an alternative vision called deliberate interactions--a planned and purposeful interaction style that involves offline preparation and discuss ways ICT can support this online usage behavior.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2593–2602},
numpages = {10},
keywords = {Kenya, urban computing, everyday technology, hci4d},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753641,
author = {Bailey, Brian P. and Horvitz, Eric},
title = {What's Your Idea? A Case Study of a Grassroots Innovation Pipeline within a Large Software Company},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753641},
doi = {10.1145/1753326.1753641},
abstract = {Establishing a grassroots innovation pipeline has come to the fore as strategy for nurturing innovation within large organizations. A key element of such pipelines is the use of an idea management system that enables and encourages community ideation on defined business problems. The value of these systems can be highly sensitive to design choices, as different designs may influence participation. We report the results of a case study examining the use of one particular idea management system and pipeline. We analyzed the content, interaction, and participation from three creativity challenges organized via the pipeline and conducted interviews with users to uncover motivations for participating and perceptions of the outcomes. Additional interviews were conducted with senior managers to learn about the objectives, successes, and unique nature of the pipeline. From the results, we formulate recommendations for improving the design of idea management systems and execution of the pipelines within organizations.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2065–2074},
numpages = {10},
keywords = {organizations, innovation, idea management, creativity},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753338,
author = {Branham, Stacy and Golovchinsky, Gene and Carter, Scott and Biehl, Jacob T.},
title = {Let's Go from the Whiteboard: Supporting Transitions in Work through Whiteboard Capture and Reuse},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753338},
doi = {10.1145/1753326.1753338},
abstract = {The use of whiteboards is pervasive across a wide range of work domains. But some of the qualities that make them successful--an intuitive interface, physical working space, and easy erasure--inherently make them poor tools for archival and reuse. If whiteboard content could be made available in times and spaces beyond those supported by the whiteboard alone, how might it be appropriated? We explore this question via ReBoard, a system that automatically captures whiteboard images and makes them accessible through a novel set of user-centered access tools. Through the lens of a seven week workplace field study, we found that by enabling new workflows, ReBoard increased the value of whiteboard content for collaboration.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {75–84},
numpages = {10},
keywords = {workflow, whiteboards, information reuse and sharing},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1753326.1753592,
author = {Zhou, Xiaomu and Ackerman, Mark S. and Zheng, Kai},
title = {Doctors and Psychosocial Information: Records and Reuse in Inpatient Care},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753592},
doi = {10.1145/1753326.1753592},
abstract = {We conducted a field-based study at a large teaching hospital to examine doctors' use and documentation of patient care information, with a special focus on a patient's psychosocial information. We were particularly interested in the gaps between the medical work and any representations of the patient. The paper describes how doctors record this information for immediate and long-term use. We found that doctors documented a considerable amount of psychosocial information in their electronic health records (EHR) system. Yet, we also observed that such information was recorded selectively, and a medicalized view-point is a key contributing factor. Our study shows how missing or problematic representations of a patient affect work activities and patient care. We accordingly suggest that EHR systems could be made more usable and useful in the long run, by supporting both representations of medical processes and of patients.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1767–1776},
numpages = {10},
keywords = {organizational memory, cscw, health informatics, physician information needs, ehr, psychosocial information, medical records, electronic patient records},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/1878537.1878594,
author = {Wang, Fuqing and Dong, Wei and Ji, Yindong},
title = {Logic Deduction Agent Based Distributed Parallel Test Platform on Hardware-in-the-Loop Simulation System},
year = {2010},
isbn = {9781450300698},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.1145/1878537.1878594},
doi = {10.1145/1878537.1878594},
abstract = {Nowadays, the speed of high-speed train is very high and the control of high-speed train is mainly based on distributed control network. The most important mission of the control network is to implement the right control logic relationship among all components of the train.This paper proposes an approach to build a logic deduction agent based distributed parallel test platform for the hardware-in-the-loop high-speed train simulation system. The test platform can fully support automatic test which will greatly reduce the cost of testing and improve the quality of test. With the introduction of Logic Deduction Agent (LDA), the intelligence and test efficiency of the test platform are considerably increased. What's more, the parallel and distributed framework based on LDA contributes to the high executing efficiency and flexibility of the test platform. On the other hand, the test coverage can be well evaluated with the help of LDA.},
booktitle = {Proceedings of the 2010 Spring Simulation Multiconference},
articleno = {54},
numpages = {7},
keywords = {logic deduction agent, hardware-in-the-loop simulation, distributed parallel system, test platform},
location = {Orlando, Florida},
series = {SpringSim '10}
}

@inproceedings{10.1145/1878537.1878545,
author = {Alt, Jonathan K. and Lieberman, Stephen and Blais, Curtis},
title = {A Use-Case Approach to the Validation of Social Modeling and Simulation},
year = {2010},
isbn = {9781450300698},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.1145/1878537.1878545},
doi = {10.1145/1878537.1878545},
abstract = {The modeling and simulation (M&amp;S) community is faced with the task of informing public policy decision makers, from both the defense community and from the civilian sector, of the impact of their decisions on the beliefs, values and interests (BVIs) of the populations in their areas of influence. The M&amp;S techniques used for these types of analyses by necessity challenge the traditional boundaries and methods regarding validation efforts. Given that the analysis of populations is largely intractable via reductionist methodologies, we posit that insight must be garnered through experiment and iteration using simulated societies. Further, the designs of these simulation experiments must be based on the information needs of the community. We discuss the concept of developing social simulations by use case, the validation of data sources for model development, validation techniques guided by usefulness, and a case study approach to validation of social simulations.},
booktitle = {Proceedings of the 2010 Spring Simulation Multiconference},
articleno = {7},
numpages = {7},
keywords = {behavioral modeling, ontology, social simulation, VV&amp;A, validation},
location = {Orlando, Florida},
series = {SpringSim '10}
}

@inproceedings{10.1145/1878537.1878583,
author = {Jarmasz, Jerzy and Muller-Gass, Alexandra and Zotov, Vladimir and Lamb, Matt and Scourtoudis, Elena and Wojtarowicz, Dorothy and Thomson, Michael and Bruyn-Martin, Lora},
title = {Blended Solutions for Counter-IED Training},
year = {2010},
isbn = {9781450300698},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
url = {https://doi.org/10.1145/1878537.1878583},
doi = {10.1145/1878537.1878583},
abstract = {Improvised explosive devices (IEDs) are a major threat to coalition troops in recent asymmetric conflicts such as Afghanistan. Soldiers still rely heavily on their own senses to detect and assess IED threats, requiring good IED awareness training methods. Three prototype training tools developed for the Canadian Forces are presented here as a "blended" approach to IED awareness training.},
booktitle = {Proceedings of the 2010 Spring Simulation Multiconference},
articleno = {44},
numpages = {8},
keywords = {counter-IED, training, irregular warfare, virtual demonstrations, serious games},
location = {Orlando, Florida},
series = {SpringSim '10}
}

@inproceedings{10.1145/1791212.1791227,
author = {Khan, Mohammad Maifi Hasan and Le, Hieu K. and LeMay, Michael and Moinzadeh, Parya and Wang, Lili and Yang, Yong and Noh, Dong K. and Abdelzaher, Tarek and Gunter, Carl A. and Han, Jiawei and Jin, Xin},
title = {Diagnostic Powertracing for Sensor Node Failure Analysis},
year = {2010},
isbn = {9781605589886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1791212.1791227},
doi = {10.1145/1791212.1791227},
abstract = {Troubleshooting unresponsive sensor nodes is a significant challenge in remote sensor network deployments. This paper introduces the tele-diagnostic powertracer, an in-situ troubleshooting tool that uses external power measurements to determine the internal health condition of an unresponsive host and the most likely cause of its failure. We developed our own low-cost power meter with low-bandwidth radio to report power measurements and findings, hence allowing remote (i.e., tele-) diagnosis. The tool was deployed and tested in a remote solar-powered sensing network for acoustic and visual environmental monitoring. It was shown to successfully distinguish between several categories of failures that cause unresponsive behavior including energy depletion, antenna damage, radio disconnection, system crashes, and anomalous reboots. It was also able to determine the internal health conditions of an unresponsive node, such as the presence or absence of sensing and data storage activities (for each of multiple sensors). The paper explores the feasibility of building such a remote diagnostic tool from the standpoint of economy, scale and diagnostic accuracy. To the authors' knowledge, this is the first paper that presents a remote diagnostic tool that uses power measurements to diagnose sensor system failures.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {117–128},
numpages = {12},
keywords = {troubleshooting, sensor networks, energy},
location = {Stockholm, Sweden},
series = {IPSN '10}
}

@inproceedings{10.1145/1791212.1791242,
author = {Ghasemzadeh, Hassan and Loseu, Vitali and Jafari, Roozbeh},
title = {Collaborative Signal Processing for Action Recognition in Body Sensor Networks: A Distributed Classification Algorithm Using Motion Transcripts},
year = {2010},
isbn = {9781605589886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1791212.1791242},
doi = {10.1145/1791212.1791242},
abstract = {Body sensor networks are emerging as a promising platform for remote human monitoring. With the aim of extracting bio-kinematic parameters from distributed body-worn sensors, these systems require collaboration of sensor nodes to obtain relevant information from an overwhelmingly large volume of data. Clearly, efficient data reduction techniques and distributed signal processing algorithms are needed. In this paper, we present a data processing technique that constructs motion transcripts from inertial sensors and identifies human movements by taking collaboration between the nodes into consideration. Transcripts of basic motions, called primitives, are built to reduce the complexity of the sensor data. This model leads to a distributed algorithm for segmentation and action recognition. We demonstrate the effectiveness of our framework using data collected from five normal subjects performing ten transitional movements. The results clearly illustrate the effectiveness of our framework. In particular, we obtain a classification accuracy of 84.13% with only one sensor node involved in the classification process.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {244–255},
numpages = {12},
keywords = {distributed classification, collaborative signal processing, body sensor networks, motion transcripts},
location = {Stockholm, Sweden},
series = {IPSN '10}
}

@inproceedings{10.1145/1791314.1791349,
author = {Berral, Josep Ll. and Goiri, \'{I}\~{n}igo and Nou, Ram\'{o}n and Juli\`{a}, Ferran and Guitart, Jordi and Gavald\`{a}, Ricard and Torres, Jordi},
title = {Towards Energy-Aware Scheduling in Data Centers Using Machine Learning},
year = {2010},
isbn = {9781450300421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1791314.1791349},
doi = {10.1145/1791314.1791349},
abstract = {As energy-related costs have become a major economical factor for IT infrastructures and data-centers, companies and the research community are being challenged to find better and more efficient power-aware resource management strategies. There is a growing interest in "Green" IT and there is still a big gap in this area to be covered.In order to obtain an energy-efficient data center, we propose a framework that provides an intelligent consolidation methodology using different techniques such as turning on/off machines, power-aware consolidation algorithms, and machine learning techniques to deal with uncertain information while maximizing performance. For the machine learning approach, we use models learned from previous system behaviors in order to predict power consumption levels, CPU loads, and SLA timings, and improve scheduling decisions. Our framework is vertical, because it considers from watt consumption to workload features, and cross-disciplinary, as it uses a wide variety of techniques.We evaluate these techniques with a framework that covers the whole control cycle of a real scenario, using a simulation with representative heterogeneous workloads, and we measure the quality of the results according to a set of metrics focused toward our goals, besides traditional policies. The results obtained indicate that our approach is close to the optimal placement and behaves better when the level of uncertainty increases.},
booktitle = {Proceedings of the 1st International Conference on Energy-Efficient Computing and Networking},
pages = {215–224},
numpages = {10},
keywords = {simulation, data center, power efficiency, scheduling, machine learning},
location = {Passau, Germany},
series = {e-Energy '10}
}

@inproceedings{10.1145/1755913.1755926,
author = {Bodik, Peter and Goldszmidt, Moises and Fox, Armando and Woodard, Dawn B. and Andersen, Hans},
title = {Fingerprinting the Datacenter: Automated Classification of Performance Crises},
year = {2010},
isbn = {9781605585772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1755913.1755926},
doi = {10.1145/1755913.1755926},
abstract = {Contemporary datacenters comprise hundreds or thousands of machines running applications requiring high availability and responsiveness. Although a performance crisis is easily detected by monitoring key end-to-end performance indicators (KPIs) such as response latency or request throughput, the variety of conditions that can lead to KPI degradation makes it difficult to select appropriate recovery actions. We propose and evaluate a methodology for automatic classification and identification of crises, and in particular for detecting whether a given crisis has been seen before, so that a known solution may be immediately applied. Our approach is based on a new and efficient representation of the datacenter's state called a fingerprint, constructed by statistical selection and summarization of the hundreds of performance metrics typically collected on such systems. Our evaluation uses 4 months of trouble-ticket data from a production datacenter with hundreds of machines running a 24x7 enterprise-class user-facing application. In experiments in a realistic and rigorous operational setting, our approach provides operators the information necessary to initiate recovery actions with 80% correctness in an average of 10 minutes, which is 50 minutes earlier than the deadline provided to us by the operators. To the best of our knowledge this is the first rigorous evaluation of any such approach on a large-scale production installation.},
booktitle = {Proceedings of the 5th European Conference on Computer Systems},
pages = {111–124},
numpages = {14},
keywords = {performance, web applications, datacenters},
location = {Paris, France},
series = {EuroSys '10}
}

@inproceedings{10.1145/1791314.1791347,
author = {Vasudevan, Vijay and Andersen, David and Kaminsky, Michael and Tan, Lawrence and Franklin, Jason and Moraru, Iulian},
title = {Energy-Efficient Cluster Computing with FAWN: Workloads and Implications},
year = {2010},
isbn = {9781450300421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1791314.1791347},
doi = {10.1145/1791314.1791347},
abstract = {This paper presents the architecture and motivation for a cluster-based, many-core computing architecture for energy-efficient, data-intensive computing. FAWN, a Fast Array of Wimpy Nodes, consists of a large number of slower but efficient nodes coupled with low-power storage. We present the computing trends that motivate a FAWN-like approach, for CPU, memory, and storage. We follow with a set of microbenchmarks to explore under what workloads these "wimpy nodes" perform well (or perform poorly). We conclude with an outline of the longer-term implications of FAWN that lead us to select a tightly integrated stacked chip-and-memory architecture for future FAWN development.},
booktitle = {Proceedings of the 1st International Conference on Energy-Efficient Computing and Networking},
pages = {195–204},
numpages = {10},
keywords = {cluster computing, measurement, flash, design, energy efficiency, performance},
location = {Passau, Germany},
series = {e-Energy '10}
}

@inproceedings{10.5555/1811182.1811194,
author = {Palen, Leysia and Anderson, Kenneth M. and Mark, Gloria and Martin, James and Sicker, Douglas and Palmer, Martha and Grunwald, Dirk},
title = {A Vision for Technology-Mediated Support for Public Participation &amp; Assistance in Mass Emergencies &amp; Disasters},
year = {2010},
isbn = {9781450301923},
publisher = {BCS Learning &amp; Development Ltd.},
address = {Swindon, GBR},
abstract = {We present a vision of the future of emergency management that better supports inclusion of activities and information from members of the public during disasters and mass emergency events. Such a vision relies on integration of multiple subfields of computer science, and a commitment to an understanding of the domain of application. It supports the hopes of a grid/cyberinfrastructure-enabled future that makes use of social software. However, in contrast to how emergency management is often understood, it aims to push beyond the idea of monitoring on-line activity, and instead focuses on an understudied but critical aspect of mass emergency response---the needs and roles of members of the public. By viewing the citizenry as a powerful, self-organizing, and collectively intelligent force, information and communication technology can play a transformational role in crisis. Critical topics for research and development include an understanding of the quantity and quality of information (and its continuous change) produced through computer-mediated communication during emergencies; mechanisms for ensuring trustworthiness and security of information; mechanisms for aligning informal and formal sources of information; and new applications of information extraction techniques.},
booktitle = {Proceedings of the 2010 ACM-BCS Visions of Computer Science Conference},
articleno = {8},
numpages = {12},
keywords = {social computing, disasters, technology-mediated social participation, visions of computer science, crisis informatics, natural hazards, social-technical systems, emergency management},
location = {Edinburgh, United Kingdom},
series = {ACM-BCS '10}
}

@inproceedings{10.1145/1900008.1900059,
author = {Zucker, Ron},
title = {The Effects of Annotated Web Documents, Using Context Highlighting, on Quiz Performance and Preparation Time},
year = {2010},
isbn = {9781450300643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1900008.1900059},
doi = {10.1145/1900008.1900059},
abstract = {Highlighting is an annotation method frequently used to indicate importance. This document introduces context highlighting of web documents and demonstrates that passive readers may benefit from summaries produced by context/keyword highlighting. HighBrow, a prototype browser developed to enable active readers to create context/keyword summaries from web documents, is introduced. This document provides the results of an experiment using three groups of passive readers: the first group reading an entire document; the second group reading a context/keyword summary provided by HighBrow; and the third group reading a keyword only summary (provided by a modified version of HighBrow). The experiment was developed to measure quiz performance, preparation time, and efficiency (quiz score divided by time).},
booktitle = {Proceedings of the 48th Annual Southeast Regional Conference},
articleno = {36},
numpages = {5},
keywords = {context highlighting, human computer interaction, annotation, cognition},
location = {Oxford, Mississippi},
series = {ACM SE '10}
}

@inproceedings{10.1145/1772690.1772738,
author = {Irmak, Utku and Kraft, Reiner},
title = {A Scalable Machine-Learning Approach for Semi-Structured Named Entity Recognition},
year = {2010},
isbn = {9781605587998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1772690.1772738},
doi = {10.1145/1772690.1772738},
abstract = {Named entity recognition studies the problem of locating and classifying parts of free text into a set of predefined categories. Although extensive research has focused on the detection of person, location and organization entities, there are many other entities of interest, including phone numbers, dates, times and currencies (to name a few examples). We refer to these types of entities as "semi-structured named entities", since they usually follow certain syntactic formats according to some conventions, although their structure is typically not well-defined. Regular expression solutions require significant amount of manual effort and supervised machine learning approaches rely on large sets of labeled training data. Therefore, these approaches do not scale when we need to support many semi-structured entity types in many languages and regions.In this paper, we study this problem and propose a novel three-level bootstrapping framework for the detection of semi-structured entities. We describe the proposed techniques for phone, date and time entities, and perform extensive evaluations on English, German, Polish, Swedish and Turkish documents. Despite the minimal input from the user, our approach can achieve 95% precision and 84% recall for phone entities, and 94% precision and 81% recall for date and time entities, on average. We also discuss implementation details and report run time performance results, which show significant improvements over regular expression based solutions.},
booktitle = {Proceedings of the 19th International Conference on World Wide Web},
pages = {461–470},
numpages = {10},
keywords = {boostrapping algorithm, weakly-supervised learning, ner},
location = {Raleigh, North Carolina, USA},
series = {WWW '10}
}

@inproceedings{10.1145/1772690.1772736,
author = {Hovelynck, Matthijs and Chidlovskii, Boris},
title = {Multi-Modality in One-Class Classification},
year = {2010},
isbn = {9781605587998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1772690.1772736},
doi = {10.1145/1772690.1772736},
abstract = {We propose a method for improving classification performance in a one-class setting by combining classifiers of different modalities. We apply the method to the problem of distinguishing responsive documents in a corpus of e-mails, like Enron Corpus. We extract the social network of actors which is implicit in a large body of electronic communication and turn it into valuable features for classifying the exchanged documents. Working in a one-class setting we folow a semi-supervised approach based on the Mapping Convergence framework. We propose an alternative interpretation, that allows for broader applicability when positive and negative items are not naturally separable. We propose an extension to the one-class evaluation framework in truly one-case cases when only some positive training examples are available. We extent the one-class setting to the co-training principle that enables us to take advantage of multiple views on the data. We report evaluation results of this extension on three different corpora including Enron Corpus.},
booktitle = {Proceedings of the 19th International Conference on World Wide Web},
pages = {441–450},
numpages = {10},
keywords = {one-class classification, responsiveness, enron, co-training},
location = {Raleigh, North Carolina, USA},
series = {WWW '10}
}

@inproceedings{10.1145/1805986.1806032,
author = {Lunn, Darren and Harper, Simon},
title = {Using Galvanic Skin Response Measures to Identify Areas of Frustration for Older Web 2.0 Users},
year = {2010},
isbn = {9781450300452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1805986.1806032},
doi = {10.1145/1805986.1806032},
abstract = {The World Wide Web (Web) is changing. The much vaunted Web 2.0 sees once static pages evolving into hybrid applications. Content that was once simple is now becoming increasingly complicated due to the many updating components located throughout the page. The information overload and visual complexity of such components is significant. This increased complexity can produce lower performance and higher levels of stress and frustration which negatively effect the user. In previous work we have shown how galvanic skin response (GSR) measurements, collected in tandem with eye-tracking data, can be used as a method for determining how stressed users become when interacting with content. The results of that study demonstrated that when used appropriately, the presence of Web 2.0 content can reduce GSR measurements and be of benefit to users. In this work, the previous study was repeated with twenty-three older Web users to establish if similar patterns of interaction could be established. The results reveal that while older participants made use of dynamic content, unlike previous participants, they were a non-homogenous group with a large variance in the GSR measurements. We assert that a cause of this is hesitancy and therefore developing techniques to reduce hesitancy will benefit older users when interacting with Web 2.0 content.},
booktitle = {Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A)},
articleno = {34},
numpages = {10},
keywords = {web 2.0, eye tracking, galvanic skin response, ageing},
location = {Raleigh, North Carolina},
series = {W4A '10}
}

@article{10.1145/1734200.1734202,
author = {Jurca, Radu and Garcin, Florent and Talwar, Arjun and Faltings, Boi},
title = {Reporting Incentives and Biases in Online Review Forums},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/1734200.1734202},
doi = {10.1145/1734200.1734202},
abstract = {Online reviews have become increasingly popular as a way to judge the quality of various products and services. However, recent work demonstrates that the absence of reporting incentives leads to a biased set of reviews that may not reflect the true quality. In this paper, we investigate underlying factors that influence users when reporting feedback. In particular, we study both reporting incentives and reporting biases observed in a widely used review forum, the Tripadvisor Web site. We consider three sources of information: first, the numerical ratings left by the user for different aspects of quality; second, the textual comment accompanying a review; third, the patterns in the time sequence of reports. We first show that groups of users who discuss a certain feature at length are more likely to agree in their ratings. Second, we show that users are more motivated to give feedback when they perceive a greater risk involved in a transaction. Third, a user's rating partly reflects the difference between true quality and prior expectation of quality, as inferred from previous reviews. We finally observe that because of these biases, when averaging review scores there are strong differences between the mean and the median. We speculate that the median may be a better way to summarize the ratings.},
journal = {ACM Trans. Web},
month = apr,
articleno = {5},
numpages = {27},
keywords = {Online reviews, reputation mechanisms}
}

@article{10.1145/1744161.1744175,
author = {Dubberly, Hugh and Mehta, Rajiv and Evenson, Shelley and Pangaro, Paul},
title = {Reframing Health to Embrace Design of Our Own Well-Being},
year = {2010},
issue_date = {May + June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1072-5520},
url = {https://doi.org/10.1145/1744161.1744175},
doi = {10.1145/1744161.1744175},
abstract = {Editor's Note: Improving healthcare is a wicked problem [1]. Healthcare's many stakeholders can't agree on a solution, because they don't agree on the problem. They come to the discussion from different points of view, with different frames. Wicked problems can be "solved" only by reframing, by providing a new way of understanding the problem that stakeholders can share [1]. This article describes a growing trend: framing health in terms of well-being and broadening healthcare to include self-management. Self-management reframes patients as designers, an example of a shift also occurring in design practice---reframing users as designers. The article concludes with thoughts on what these changes may mean when designing for health.---Hugh Dubberly},
journal = {Interactions},
month = may,
pages = {56–63},
numpages = {8}
}

@article{10.5555/2835434.2835438,
author = {Suomalainen, Piia and Korpinen, Leena and P\"{a}\"{a}kk\"{o}nen, Rauno},
title = {A Comparison of the Usability of a Laptop, Communicator, and Handheld Computer},
year = {2010},
issue_date = {May 2010},
publisher = {Usability Professionals' Association},
address = {Bloomingdale, IL},
volume = {5},
number = {3},
issn = {1931-3357},
abstract = {The goal of this study was to examine the usability of a laptop, communicator, and handheld computer using test subjects and questionnaires. The study aimed to determine how user-friendly and ergonomically correct these devices are. The subjects (25) had 5 minutes to perform typing or calculation tests with each device. While the subjects performed the tasks, an observer monitored the subjects' work posture. After the tasks were completed, the subjects completed questionnaires about the usability of each device Based on the subjects' experiences, the handheld computer and laptop had better ergonomic characteristics than the communicator. Subjects felt the highest amounts of stress in their neck while working on the laptop, subjects felt stress on their backs while working on the communicator, and they felt stress in their eyes while working on the handheld computer. Subjects performed the typing tasks best using the laptop. Our research suggests that companies developing mobile devices should consider ergonomic issues and the ergonomic differences between different types of mobile devices to further improve user satisfaction.},
journal = {J. Usability Studies},
month = may,
pages = {111–123},
numpages = {13},
keywords = {usability, laptop, communicator, mobile devices, handheld computer}
}

@inproceedings{10.1145/1808885.1808900,
author = {Gambi, Alessio and Toffetti, Giovanni and Pezz\`{e}, Mauro},
title = {Protecting SLAs with Surrogate Models},
year = {2010},
isbn = {9781605589633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808885.1808900},
doi = {10.1145/1808885.1808900},
abstract = {In this paper, we propose the use of surrogate models to avoid or limit violations of the service level agreements (protect SLAs) of enterprise applications executed within virtualized data centers (VDCs).Modern enterprise services are delivered along with service level agreements (SLAs) that formalize the expected quality of service, and define penalties in case of violations. By deploying enterprise applications within VDCs, providers can dynamically change the execution configuration of the services to react to unplanned environmental conditions, like sudden changes in the workload mix and intensity, with the goal of avoiding SLA violations while reducing operational costs with respect to traditional over-provisioning solutions.Surrogate models are successfully used in modern engineering to approximate systems' behavior, and thus support a wide scope of activities, especially design optimization. In this paper, we show that by reducing the problem of protecting SLAs in VDCs to an optimization problem, we can adapt surrogate models to this new framework and implement SLA protection controller components. In the paper, we present the main ideas, we illustrate how surrogate models can be used to protect SLAs, and we discuss preliminary results obtained on a case study deployed in an industrial virtualized infrastructure.},
booktitle = {Proceedings of the 2nd International Workshop on Principles of Engineering Service-Oriented Systems},
pages = {71–77},
numpages = {7},
location = {Cape Town, South Africa},
series = {PESOS '10}
}

@inproceedings{10.1145/1810295.1810302,
author = {Wang, Yi and Zhang, Min},
title = {Penalty Policies in Professional Software Development Practice: A Multi-Method Field Study},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810302},
doi = {10.1145/1810295.1810302},
abstract = {Organizational Punishment/Penalty is a pervasive phenomenon in many professional organizations. In some software development organizations, punishment measures have been adopted in an attempt to improve software developers' performance, reduce the software defects, and hence ensure software quality. It is unclear whether these measures are effective. This article presents the results of a multi-method field study that analyzes software engineers' perception towards penalty policies in relation to software quality in a software development process. The results were generated via both qualitative and quantitative methods. Through interviews, we collected the individuals' perception towards the penalty policy. By extracting data in a software configuration management system, we identified several patterns of defects change. We found that while a penalty mechanism does help to reduce software defects in daily coding activity, it fails in achieving programmers' maximum work potential. Meanwhile, experienced software programmers require less time to adapt to penalty policies and benefit from exist of less experienced developers. Some additional findings and implications are also discussed.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {39–47},
numpages = {9},
keywords = {software defects, perception and performance of software developers, penalty policies},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/1809100.1809109,
author = {Geng, Jianning and Liu, Lin and Bryant, Barrett R.},
title = {Towards a Personalized Privacy Management Framework},
year = {2010},
isbn = {9781605589657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809100.1809109},
doi = {10.1145/1809100.1809109},
abstract = {Privacy policies which are stated by the service provider are intended to convey to the users the information how their personal data will be protected. Thus, letting the user understand the privacy policy and trust the service provider is a crucial task. This paper proposes a personalized privacy management framework with which users can participate in the privacy management. Existing privacy policies are analyzed and improved. Within this framework, the privacy policy can not only be regulated, but also can be easily understood by users at a glance.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Software Engineering for Secure Systems},
pages = {58–64},
numpages = {7},
keywords = {privacy policy, standardization, management},
location = {Cape Town, South Africa},
series = {SESS '10}
}

@inproceedings{10.1145/1833310.1833312,
author = {Hoda, Rashina and Noble, James and Marshall, Stuart},
title = {Balancing Acts: Walking the Agile Tightrope},
year = {2010},
isbn = {9781605589664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1833310.1833312},
doi = {10.1145/1833310.1833312},
abstract = {Self-organizing teams are one of the critical success factors on Agile projects - and yet, little is known about the self-organizing nature of Agile teams and the challenges they face in industrial practice. Based on a Grounded Theory study of 40 Agile practitioners across 16 software development organizations in New Zealand and India, we describe how self-organizing Agile teams perform balancing acts between (a) freedom and responsibility (b) cross-functionality and specialization, and (c) continuous learning and iteration pressure, in an effort to maintain their self-organizing nature. We discuss the relationship between these three balancing acts and the fundamental conditions of self-organizing teams - autonomy, cross-fertilization, and self-transcendence.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {5–12},
numpages = {8},
keywords = {Agile software development, software engineering, self-organizing teams},
location = {Cape Town, South Africa},
series = {CHASE '10}
}

@inproceedings{10.1145/1833335.1833343,
author = {Ozkaya, Ipek and Wallin, Peter and Axelsson, Jakob},
title = {Architecture Knowledge Management during System Evolution: Observations from Practitioners},
year = {2010},
isbn = {9781605589671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1833335.1833343},
doi = {10.1145/1833335.1833343},
abstract = {It is widely accepted that awareness of architectural decisions enables better management and planning of system evolution, refactoring, and modernization efforts. In this paper we report data from interviews with software architects about how practitioners utilize architecture during system evolution. Our results show, despite the widely shared view that long-lived systems are better off with strong architectures; basic architecture-centric practices are not followed systematically. The key gap we observe is in correct and timely communication of architectural issues. This overall finding is not surprising. However, our data also contributes to how architecture knowledge management activities can be focused for most benefit throughout a system's lifespan. While the often-referenced problem is lack of time spent on documentation and design practices, our interviews show that lack of quality attribute reasoning early on, and during the lifespan of the system is a key contributor to failing to use architecture knowledge effectively during evolution.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Sharing and Reusing Architectural Knowledge},
pages = {52–59},
numpages = {8},
keywords = {architecture-centric practices, system evolution, architecture knowledge management, software architecture},
location = {Cape Town, South Africa},
series = {SHARK '10}
}

@inproceedings{10.1145/1833310.1833316,
author = {Dubinsky, Yael and Hazzan, Orit},
title = {Ad-Hoc Leadership in Agile Software Development Environments},
year = {2010},
isbn = {9781605589664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1833310.1833316},
doi = {10.1145/1833310.1833316},
abstract = {Leadership is the ability to influence people, leading them to behave in a certain way in order to achieve the group's goals. Leadership is independent of job titles and descriptions. Usually, however, in order to lead, leaders need the power derived from their organizational positions. There are different leadership styles, like task-oriented versus people-oriented, directive versus permissive, autocrat versus democrat. In this paper, we examine the leadership concept in software development environments and focus on leadership in transition processes to agile software development. Specifically, based on our comprehensive research on agile software development, we suggest a leadership style - ad-hoc leadership - that usually emerges in such change processes. We present the characteristics, dynamic and uniqueness of this leadership style and illustrate its usefulness for the analysis of representative scenarios.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {32–38},
numpages = {7},
keywords = {change leader, leadership, agile software development, ad-hoc leadership},
location = {Cape Town, South Africa},
series = {CHASE '10}
}

@inproceedings{10.1145/1833310.1833327,
author = {Shah, Hina and Harrold, Mary Jean},
title = {Studying Human and Social Aspects of Testing in a Service-Based Software Company: Case Study},
year = {2010},
isbn = {9781605589664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1833310.1833327},
doi = {10.1145/1833310.1833327},
abstract = {This paper presents a case study that reports the findings of a preliminary ethnographic study (35 days of fieldwork over a period of two months) conducted at a service-based software company. The focus of the study was on understanding the human-dimension and social aspects involved in software testing. In this paper, we present the design of the study, our observations, and the analysis of the findings. We also discuss the differences between the senior and junior participants' attitudes towards testing, how the seniors' attitudes influence the juniors' attitudes, and reasons that seem to motivate juniors to work on testing projects. Additionally, we report our findings about the relationship between enthusiasm and responsibility with ownership, the relationship between the developer and test engineer, the communication gaps faced by test engineers in various situations, and how hierarchical structuring in an organization may influence enthusiasm of the test engineers.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {102–108},
numpages = {7},
keywords = {software testing, ethnography, field study, human factors, motivation, qualitative study, attitudes},
location = {Cape Town, South Africa},
series = {CHASE '10}
}

@inproceedings{10.1145/1808984.1808987,
author = {Metzger, Andreas and Sammodi, Osama and Pohl, Klaus and Rzepka, Mark},
title = {Towards Pro-Active Adaptation with Confidence: Augmenting Service Monitoring with Online Testing},
year = {2010},
isbn = {9781605589718},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808984.1808987},
doi = {10.1145/1808984.1808987},
abstract = {Service-based applications need to operate in a highly dynamic and distributed world. As those applications are composed of individual services, they have to react to failures of those services to ensure that the applications maintain their expected functionality and quality. Self-adaptation is one solution to this problem, as it allows applications to autonomously react to failures. Currently, monitoring is typically used to identify failures, thus triggering adaptation. However, monitoring only observes failures after they have occurred, which means that adaptation based on monitoring is reactive. This can lead to shortcomings like user dissatisfaction, increased execution times, and late response to critical events. Pro-active adaptation addresses those shortcomings, because in such a setting, the application detects the need for adaptation and thus can adapt before a failure will occur. However, it is important to avoid unnecessary pro-active adaptations, as they can lead to severe shortcomings, such as increased costs or follow-up failures. This means that when taking pro-active adaptation decisions it is key that there is confidence in the predicted future failures, i.e., pro-active adaptation should only be performed if there is certainty that the failure could in fact occur. To avoid unnecessary adaptations, we introduce an approach based on augmenting service monitoring with online testing to produce failure predictions with confidence. We demonstrate the applicability of our approach using a scenario from the eGovernment domain.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems},
pages = {20–28},
numpages = {9},
location = {Cape Town, South Africa},
series = {SEAMS '10}
}

@inproceedings{10.1145/1808266.1808267,
author = {Zhifang, Liu and Bin, Liu and Xiaopeng, Gao},
title = {Test Automation on Mobile Device},
year = {2010},
isbn = {9781605589701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808266.1808267},
doi = {10.1145/1808266.1808267},
abstract = {To ensure the software quality of mobile device, it is essential to build sophisticated testing automation strategy. In this paper, we introduced the idea of constructing a testing framework, elaborated the problems encountered in practice and proposed the solutions accordingly, where several innovative testing methods and tactics were brought forward and discussed in details. Furthermore, the testing framework was used for testing a tailor-made program on mobile phone. Testing result analysis proves that the framework is not only viable and effective on mobile device testing, but also shows that the testing efficiency needs to be improved.},
booktitle = {Proceedings of the 5th Workshop on Automation of Software Test},
pages = {1–7},
numpages = {7},
keywords = {software testing, mobile device testing, test automation},
location = {Cape Town, South Africa},
series = {AST '10}
}

@inproceedings{10.1145/1809085.1809091,
author = {Williams, James},
title = {Social Networking Applications in Health Care: Threats to the Privacy and Security of Health Information},
year = {2010},
isbn = {9781605589732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809085.1809091},
doi = {10.1145/1809085.1809091},
abstract = {The advent of social networking websites for use in health care has prompted concerns about the risks that such systems pose to the security and privacy of personal health information. In this paper, we survey the research literature, in order to provide a current snapshot of privacy and security safeguards for social network websites. We describe some of the unique features of the health care space, and recommend directions for future research in this relatively new area.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Software Engineering in Health Care},
pages = {39–49},
numpages = {11},
keywords = {medicine 2.0, health information privacy, social networks, personal health record},
location = {Cape Town, South Africa},
series = {SEHC '10}
}

@inproceedings{10.1145/1809198.1809205,
author = {Begel, Andrew and Zimmermann, Thomas},
title = {Keeping up with Your Friends: Function Foo, Library Bar.DLL, and Work Item 24},
year = {2010},
isbn = {9781605589756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809198.1809205},
doi = {10.1145/1809198.1809205},
abstract = {Development teams who work with others need to be aware of what everyone is doing in order to manage the risk of taking on dependencies. Using newsfeeds of software development activities mined from software repositories, teams can find relevant information to help them make well-informed decisions that affect the success of their endeavors. In this paper, we describe the architecture of a newsfeed system that we are currently building on top of the Codebook software repository mining platform. We discuss the design, construction and aggregation of newsfeeds, and include other important aspects such as summarization, filtering, context, and privacy.},
booktitle = {Proceedings of the 1st Workshop on Web 2.0 for Software Engineering},
pages = {20–23},
numpages = {4},
keywords = {social networking, mining software repositories, regular language reachability, regular expression, inter-team coordination, knowledge management},
location = {Cape Town, South Africa},
series = {Web2SE '10}
}

@inproceedings{10.1145/1920778.1920815,
author = {Boschman, Lorna R.},
title = {Exergames for Adult Users: A Preliminary Pilot Study},
year = {2010},
isbn = {9781450302357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1920778.1920815},
doi = {10.1145/1920778.1920815},
abstract = {This paper examines fitness videogames (exergames) and factors surrounding their adoption by novice users over 40, first through a literature review and then by discussing a pilot study. Digital games have been promoted as a way to reduce the barriers to embracing an ongoing exercise program. However, understanding the game's instructions also affects a novice player's gameplay experience. In the pilot study, reactions of novice players to two commercially available games with exertive interfaces were systematically analyzed so obstacles to communication between players and software could be identified. The social affordances of game design for adults over 40 are also discussed in order to develop protocols for a larger study analyzing how digital games can be used to sustain exercise adherence over time.},
booktitle = {Proceedings of the International Academic Conference on the Future of Game Design and Technology},
pages = {235–238},
numpages = {4},
keywords = {Wii Fit, exergames, fitness videogames, adult fitness, Dance Dance Revolution, exertive interface and fitness program},
location = {Vancouver, British Columbia, Canada},
series = {Futureplay '10}
}

@inproceedings{10.1145/1806672.1806686,
author = {Ayewah, Nathaniel and Pugh, William},
title = {Null Dereference Analysis in Practice},
year = {2010},
isbn = {9781450300827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806672.1806686},
doi = {10.1145/1806672.1806686},
abstract = {Many analysis techniques have been proposed to determine when a potentially null value may be dereferenced. But we have observed in practice that not every potential null dereference is a "bug" that developers want to fix. In this paper we discuss some of the challenges of using a null dereference analysis in practice, and reasons why developers may not feel it necessary to change code to prevent ever possible null dereference. We revisit previous work on XYLEM, an interprocedural null dereference analysis for Java, and discuss the challenge of comparing the results of different static analysis tools. We also report experimental results for XYLEM, Coverity Prevent, Fortify SCA, Eclipse and FindBugs, and observe that the different tools tradeoff the need to flag all potential null dereferences with the need to minimize the number of cases that are implausible in practice. We conclude by discussing whether it would be useful to extend the Java type system to distinguish between nullable and nonnull types, and prohibit unchecked dereferences of nullable types.},
booktitle = {Proceedings of the 9th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering},
pages = {65–72},
numpages = {8},
keywords = {static analysis, null pointer dereference},
location = {Toronto, Ontario, Canada},
series = {PASTE '10}
}

@inproceedings{10.1145/1806512.1806517,
author = {Yu, Benjamin and Knorr, Edwin M.},
title = {Steps towards a Scientific Approach to a Database Course Transformation: Data Collection and Analysis},
year = {2010},
isbn = {9781450300988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806512.1806517},
doi = {10.1145/1806512.1806517},
abstract = {Instructors modify offerings of their courses in response to changes in emphasis, curriculum, student preparation, resource limitations, and problems with previous offerings. Changes also involve assessment instruments such as assignments and exams, ensuring that students are indeed learning the material, rather than relying on "information" from previous offerings. However, instructors are seldom guided by meaningful and useful data in making these changes. Instead, most rely on their "gut feelings", and many changes are made in an ad hoc manner, with minimal supporting data to assess whether the changes contribute positively or negatively to student learning. This paper reports on our experience with the transformation of an undergraduate database course at the University of British Columbia (UBC), using best practices from education research. We provide examples of the types of data that can be obtained through various instruments, and can be used in an objective analysis to affect course changes. If such data collection methods are put into place before changes to a course are anticipated, instructors will be better prepared to assess how students are affected by those changes.This paper is organized as follows. Sections 1 and 2 provide background information about a process model for course transformation. Section 3 describes the course being studied. Section 4 shows different types of instruments that can be used for data collection. Section 5 provides examples of the data collected, and the analysis that can be performed using that information. Section 6 describes our plans for course transformation based on the data collected.},
booktitle = {Proceedings of the 15th Western Canadian Conference on Computing Education},
articleno = {3},
numpages = {5},
keywords = {data analysis, course transformation, learning, data collection, teaching},
location = {Kelowna, British Columbia, Canada},
series = {WCCCE '10}
}

@inproceedings{10.5555/1838194.1838201,
author = {Nunes, Ingrid and Choren, Ricardo and Nunes, Camila and F\'{a}bri, Bruno and Silva, Fernando and Carvalho, Gustavo and de Lucena, Carlos J. P.},
title = {Supporting Prenatal Care in the Public Healthcare System in a Newly Industrialized Country},
year = {2010},
isbn = {9780982657140},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Most of women's deaths related to pregnancy occur in newly industrialized countries. In association with gynecologists and obstetricians of the Antonio Pedro University Hospital (HUAP) in Brazil, we have identified deficiencies in the prenatal care of the Brazilian public healthcare system that can be computer-supported. They are mainly related to protocols that must be followed in the primary healthcare institutions and the referral process that must take place when a high risk pregnancy is identified, besides other functionalities that can be automated by a software application. In this paper, the Prenatal Care Unified System (SUAP) project will be introduced, which provides a Multi-agent System for supporting and monitoring the prenatal care. This project uses agent technology to manage healthcare records, to act as a clinical decision support system, and to handle the logistics of high risk pregnancy cases. We also describe the challenges encountered during the implementation of the SUAP and discuss the benefits that an agent-based solution provided to the development of our system.},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Industry Track},
pages = {1723–1730},
numpages = {8},
keywords = {software architecture, prenatal care, multi-agent systems, healthcare, applications},
location = {Toronto, Canada},
series = {AAMAS '10}
}

@inproceedings{10.5555/1838206.1838245,
author = {Fan, Xiaocong and Su, Meng},
title = {Using Geometric Diffusions for Recognition-Primed Multi-Agent Decision Making},
year = {2010},
isbn = {9780982657119},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Several areas of multi-agent research, such as large-scale agent organization and experience-based decision making, demand novel perspectives and efficient approaches for multiscale information analysis. A recent breakthrough in harmonic analysis is diffusion geometry and diffusion wavelets, which offers a general framework for multiscale analysis of massive data sets. In this paper, we introduce the "diffusion" concept into the MAS field, and investigate the impacts of using diffusion distance on the performance of solution synthesis in experience-based multi-agent decision making. In particular, we take a two-dimensional perspective to explore the use of diffusion distance and Euclidean distance in identifying 'similar' experiences--a key activity in the process of recognition-primed decision making. An experiment has been conducted on a data set including a large collection of battlefield decision experiences. It is shown that the performance of using diffusion distance can be significantly better than using Euclidean distance in the original experience space. This study allows us to generalize an anytime algorithm for multi-agent decision making, and it also opens the door to the application of diffusion geometry to multiagent research involving massive data analysis.},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
pages = {275–282},
numpages = {8},
keywords = {decision making, recognition, diffusion distance, cognitive agent, experience},
location = {Toronto, Canada},
series = {AAMAS '10}
}

@article{10.1145/1795377.1795380,
author = {Sheng, Hong and Siau, Keng and Nah, Fiona Fui-Hoon},
title = {Understanding the Values of Mobile Technology in Education: A Value-Focused Thinking Approach},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0095-0033},
url = {https://doi.org/10.1145/1795377.1795380},
doi = {10.1145/1795377.1795380},
abstract = {Mobile technology has mobilized the human interaction in all dimensions by supporting mobile collaboration. As collaboration is key to learning in today's educational environment, mobile technology has tremendous potential in supporting and improving education and its delivery. Given that mobile technology for education is a new phenomenon that is gaining popularity, the values of using mobile technology to support education need to be further researched and better understood. In this research, we used the Value-Focused Thinking approach to interview students and instructors to identify the values of education that are enabled by mobile technology. These values are represented in the form of a means-ends objective network that not only captures the values of education facilitated by mobile technology but also depicts the relationships between these values. The means-ends objective network derived from this research can serve as a conceptual foundation for future studies and provide useful guidelines to practitioners for implementation of mobile technology in education.},
journal = {SIGMIS Database},
month = may,
pages = {25–44},
numpages = {20},
keywords = {collaborative learning, mobile technology, means-ends network, value-focused thinking}
}

@inproceedings{10.1145/1787275.1787344,
author = {Jordan, Herbert and Prodan, Radu and Nae, Vlad and Fahringer, Thomas},
title = {Dynamic Load Management for MMOGs in Distributed Environments},
year = {2010},
isbn = {9781450300445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1787275.1787344},
doi = {10.1145/1787275.1787344},
abstract = {To support thousands of concurrent players in virtual worlds simulated by contemporary Massively Multiplayer Online Games, most implementations employ static game world partitioning for distributing the load among multiple game server instances. Further, the resources that manage the resulting subregions are statically allocated, independent of the actual game load. As a result, due to the high variability of the user demand, this approach leads to a low resource utilization causing much higher provisioning costs than necessary. In addition, the number of players supported by a region is limited by the maximum load that can be handled by a single server instance.We propose in this paper a novel game load management technique divided in two (global and a local) layers, capable of dynamically adjusting the amount of allocated resources to the present user demand. The global level assigns the responsibility of serving particular game regions to data centers using a peer-to-peer infrastructure, while the local level within individual facilities maintains the necessary server instances for the assigned obligations. We device two generic heuristics based on the well-known bin-packing problem to achieve the ultimate goal of maximizing the resource utilization on both levels while maintaining user-level Quality of Service (QoS). We evaluate the performance of our proposed solution using simulation-based experiments, which demonstrate a potential cost reduction in maintaining MMOG sessions by up to 60% while maintaining QoS in 99% of the cases.},
booktitle = {Proceedings of the 7th ACM International Conference on Computing Frontiers},
pages = {337–346},
numpages = {10},
keywords = {load balancing, mmogs},
location = {Bertinoro, Italy},
series = {CF '10}
}

@inproceedings{10.5555/1809874.1809898,
author = {Kriplean, Travis and Borning, Alan and Waddell, Paul and Klang, Christoffer and Fogarty, James},
title = {Supporting Agile Modeling through Experimentation in an Integrated Urban Simulation Framework},
year = {2010},
isbn = {9781450300704},
publisher = {Digital Government Society of North America},
abstract = {Decisions regarding major urban transportation projects and land use policies are frequently political and controversial, as well as having significant economic, social, and environmental consequences. UrbanSim is a disaggregate, behaviorally-realistic modeling environment that planning agencies can use to simulate the long-term effects of such decisions. We describe UrbanSim's evolution over the past decade from the perspective of supporting its appropriation by urban modelers, and identify support for experimentation as a key property that enables the adoption of an agile modeling methodology. Finally, we draw out three lessons for supporting agile modeling through experimentation: iterative development of models, providing appropriate domain-specific building blocks, and balancing the development of integrated tools versus interoperating with existing tools and the work practices that surround them.},
booktitle = {Proceedings of the 11th Annual International Digital Government Research Conference on Public Administration Online: Challenges and Opportunities},
pages = {112–121},
numpages = {10},
keywords = {urban modeling, experimentation, UrbanSim, appropriation, agile modeling, simulation},
location = {Puebla, Mexico},
series = {dg.o '10}
}

