@article{10.1145/2018396.2018416,
author = {Seo, DongBack and Boonstra, Albert and Offenbeek, Marjolein},
title = {Managing IS Adoption in Ambivalent Groups},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/2018396.2018416},
doi = {10.1145/2018396.2018416},
abstract = {Insightful implementers refocus user ambivalence and resistance toward trust and acceptance of new systems.},
journal = {Commun. ACM},
month = nov,
pages = {68–73},
numpages = {6}
}

@inproceedings{10.1145/2064959.2064965,
author = {\"{O}zal, Asli and Ranganathan, Anand and Tatbul, Nesime},
title = {Real-Time Route Planning with Stream Processing Systems: A Case Study for the City of Lucerne},
year = {2011},
isbn = {9781450310369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2064959.2064965},
doi = {10.1145/2064959.2064965},
abstract = {Traffic-aware real-time route planning has recently been an application of increasing interest for metropolitan cities with busy traffic. This paper approaches the problem from a stream processing point of view and proposes a general architecture to solve it. This work is inspired by a real use case and is implemented on an industry-strength stream processing engine. Experimental results on this implementation demonstrate the scalability of this approach in terms of increasing data and query rates.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on GeoStreaming},
pages = {21–28},
numpages = {8},
keywords = {traffic sensors, travel time estimation, intelligent transportation systems, real-time route planning, data stream processing},
location = {Chicago, Illinois},
series = {IWGS '11}
}

@inproceedings{10.1145/2070942.2070957,
author = {Flach, Tobias and Mishra, Nilesh and Pedrosa, Luis and Riesz, Christopher and Govindan, Ramesh},
title = {CarMA: Towards Personalized Automotive Tuning},
year = {2011},
isbn = {9781450307185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070942.2070957},
doi = {10.1145/2070942.2070957},
abstract = {Wireless sensing and actuation have been explored in many contexts, but the automotive setting has received relatively little attention. Automobiles have tens of onboard sensors and expose several hundred engine parameters which can be tuned (a form of actuation). The optimal tuning for a vehicle can depend upon terrain, traffic, and road conditions, but the ability to tune a vehicle has only been available to mechanics and enthusiasts. In this paper, we describe the design and implementation of CarMA (Car Mobile Assistant), a system that provides high-level abstractions for sensing automobile parameters and tuning them. Using these abstractions, developers can easily write smart-phone "apps" to achieve fuel efficiency, responsiveness, or safety goals. Users of CarMA can tune their vehicles at the granularity of individual trips, a capability we call personalized tuning. We demonstrate through a variety of applications written on top of CarMA that personalized tuning can result in over 10% gains in fuel efficiency. We achieve this through route-specific or driver-specific customizations. Furthermore, CarMA is capable of improving user satisfaction by increasing responsiveness when necessary, and promoting vehicular safety by appropriately limiting the range of performance available to novice or unsafe drivers.},
booktitle = {Proceedings of the 9th ACM Conference on Embedded Networked Sensor Systems},
pages = {135–148},
numpages = {14},
keywords = {engine control unit, automobile, scanning, tuning},
location = {Seattle, Washington},
series = {SenSys '11}
}

@inproceedings{10.1145/2068816.2068850,
author = {Ding, Yuan and Du, Yuan and Hu, Yingkai and Liu, Zhengye and Wang, Luqin and Ross, Keith and Ghose, Anindya},
title = {Broadcast Yourself: Understanding YouTube Uploaders},
year = {2011},
isbn = {9781450310130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2068816.2068850},
doi = {10.1145/2068816.2068850},
abstract = {YouTube uploaders are the central agents in the YouTube phenomenon. We conduct extensive measurement and analysis of YouTube uploaders. We estimate YouTube scale and examine the uploading behavior of YouTube users. We demonstrate the positive reinforcement between on-line social behavior and uploading behavior. Furthermore, we examine whether YouTube users are truly broadcasting themselves, via characterizing and classifying videos as either user generated or user copied.},
booktitle = {Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference},
pages = {361–370},
numpages = {10},
keywords = {social network, content classification, system scale, YouTube},
location = {Berlin, Germany},
series = {IMC '11}
}

@inproceedings{10.1145/2068816.2068820,
author = {Raftopoulos, Elias and Dimitropoulos, Xenofontas},
title = {Detecting, Validating and Characterizing Computer Infections in the Wild},
year = {2011},
isbn = {9781450310130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2068816.2068820},
doi = {10.1145/2068816.2068820},
abstract = {Although network intrusion detection systems (IDSs) have been studied for several years, their operators are still overwhelmed by a large number of false-positive alerts. In this work we study the following problem: from a large archive of intrusion alerts collected in a production network, we want to detect with a small number of false positives hosts within the network that have been infected by malware. Solving this problem is essential not only for reducing the false-positive rate of IDSs, but also for labeling traces collected in the wild with information about validated security incidents. We use a 9-month long dataset of IDS alerts and we first build a novel heuristic to detect infected hosts from the on average 3 million alerts we observe per day. Our heuristic uses a statistical measure to find hosts that exhibit a repeated multi-stage malicious footprint involving specific classes of alerts. A significant part of our work is devoted to the validation of our heuristic. We conduct a complex experiment to assess the security of suspected infected systems in a production environment using data from several independent sources, including intrusion alerts, blacklists, host scanning logs, vulnerability reports, and search engine queries. We find that the false positive rate of our heuristic is 15% and analyze in-depth the root causes of the false positives. Having validated our heuristic, we apply it to our entire trace, and characterize various important properties of 9 thousand infected hosts in total. For example, we find that among the infected hosts, a small number of heavy hitters originate most outbound attacks and that future infections are more likely to occur close to already infected hosts.},
booktitle = {Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference},
pages = {29–44},
numpages = {16},
keywords = {intrusion detection, alert correlation, snort, network security, malware, j-measure},
location = {Berlin, Germany},
series = {IMC '11}
}

@inproceedings{10.1145/2068816.2068818,
author = {Dainotti, Alberto and Squarcella, Claudio and Aben, Emile and Claffy, Kimberly C. and Chiesa, Marco and Russo, Michele and Pescap\'{e}, Antonio},
title = {Analysis of Country-Wide Internet Outages Caused by Censorship},
year = {2011},
isbn = {9781450310130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2068816.2068818},
doi = {10.1145/2068816.2068818},
abstract = {In the first months of 2011, Internet communications were disrupted in several North African countries in response to civilian protests and threats of civil war. In this paper we analyze episodes of these disruptions in two countries: Egypt and Libya. Our analysis relies on multiple sources of large-scale data already available to academic researchers: BGP interdomain routing control plane data; unsolicited data plane traffic to unassigned address space; active macroscopic traceroute measurements; RIR delegation files; and MaxMind's geolocation database. We used the latter two data sets to determine which IP address ranges were allocated to entities within each country, and then mapped these IP addresses of interest to BGP-announced address ranges (prefixes) and origin ASes using publicly available BGP data repositories in the U.S. and Europe. We then analyzed observable activity related to these sets of prefixes and ASes throughout the censorship episodes. Using both control plane and data plane data sets in combination allowed us to narrow down which forms of Internet access disruption were implemented in a given region over time. Among other insights, we detected what we believe were Libya's attempts to test firewall-based blocking before they executed more aggressive BGP-based disconnection. Our methodology could be used, and automated, to detect outages or similar macroscopically disruptive events in other geographic or topological regions.},
booktitle = {Proceedings of the 2011 ACM SIGCOMM Conference on Internet Measurement Conference},
pages = {1–18},
numpages = {18},
keywords = {network telescope, darknet, connectivity disruption, outages, internet background radiation, censorship},
location = {Berlin, Germany},
series = {IMC '11}
}

@inproceedings{10.1145/2069618.2069633,
author = {Maestri, Leah and Wakkary, Ron},
title = {Understanding Repair as a Creative Process of Everyday Design},
year = {2011},
isbn = {9781450308205},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2069618.2069633},
doi = {10.1145/2069618.2069633},
abstract = {This paper presents the findings from an exploratory study that looks at how creativity plays a role in the repair and reuse of objects in the home. We are interested in a particular form of creativity that manifests in the everyday -- what John Dewey [8] describes as a constant doing and undergoing, as we actively adjust to everyday situations. The goal of this study is to show evidence of repair as not only an act of restoration, but also as an act of creativity that entails the repurposing and resourcing of objects. This study is part of a larger research initiative known as the Everyday Design, where it is believed that everyone is a designer and that design is an ongoing activity that includes the repair, modification, and appropriation of design objects and systems. Furthermore, this study serves as baseline research for future investigations in how to inform the design of technologies whose lifecycle can be extended for various contexts of use through repair.},
booktitle = {Proceedings of the 8th ACM Conference on Creativity and Cognition},
pages = {81–90},
numpages = {10},
keywords = {creativity, repair, appropriation, everyday design, reuse},
location = {Atlanta, Georgia, USA},
series = {C&amp;C '11}
}

@article{10.1145/2037676.2037686,
author = {Lin, Yu-Ru and Candan, K. Sel\c{c}cuk and Sundaram, Hari and Xie, Lexing},
title = {SCENT: Scalable Compressed Monitoring of Evolving Multirelational Social Networks},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7S},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/2037676.2037686},
doi = {10.1145/2037676.2037686},
abstract = {We propose SCENT, an innovative, scalable spectral analysis framework for internet scale monitoring of multirelational social media data, encoded in the form of tensor streams. In particular, a significant challenge is to detect key changes in the social media data, which could reflect important events in the real world, sufficiently quickly. Social media data have three challenging characteristics. First, data sizes are enormous; recent technological advances allow hundreds of millions of users to create and share content within online social networks. Second, social data are often multifaceted (i.e., have many dimensions of potential interest, from the textual content to user metadata). Finally, the data is dynamic; structural changes can occur at multiple time scales and be localized to a subset of users. Consequently, a framework for extracting useful information from social media data needs to scale with data volume, and also with the number and diversity of the facets of the data. In SCENT, we focus on the computational cost of structural change detection in tensor streams. We extend compressed sensing (CS) to tensor data. We show that, through the use of randomized tensor ensembles, SCENT is able to encode the observed tensor streams in the form of compact descriptors. We show that the descriptors allow very fast detection of significant spectral changes in the tensor stream, which also reduce data collection, storage, and processing costs. Experiments over synthetic and real data show that SCENT is faster (17.7x--159x for change detection) and more accurate (above 0.9 F-score) than baseline methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
articleno = {29},
numpages = {22},
keywords = {social network analysis, multirelational learning, tensor analysis, Social media, stream mining}
}

@inproceedings{10.5555/2318776.2318794,
author = {Chang, Keng-hao and Fisher, Drew and Canny, John and Hartmann, Bj\"{o}rn},
title = {How's My Mood and Stress? An Efficient Speech Analysis Library for Unobtrusive Monitoring on Mobile Phones},
year = {2011},
isbn = {9781936968299},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {The human voice encodes a wealth of information about emotion, mood, stress, and mental state. With mobile phones (one of the mostly used modules in body area networks) this information is potentially available to a host of applications and can enable richer, more appropriate, and more satisfying human-computer interaction. In this paper we describe the AMMON (Affective and Mental health MONitor) library, a low footprint C library designed for widely available phones as an enabler of these applications. The library incorporates both core features for emotion recognition (from the Interspeech 2009 Emotion recognition challenge), and the most important features for mental health analysis (glottal timing features). To comfortably run the library on feature phones (the most widely-used class of phones today), we implemented the routines in fixed-point arithmetic, and minimized computational and memory footprint. On identical test data, emotion and stress classification accuracy was indistinguishable from a state-of-the-art reference system running on a PC, achieving 75% accuracy on two-class emotion classification tasks and 84% accuracy on binary classification of stressed and neutral situations. The library uses 30% of real-time on a 1GHz processor during emotion recognition and 70% during stress and mental health analysis.},
booktitle = {Proceedings of the 6th International Conference on Body Area Networks},
pages = {71–77},
numpages = {7},
keywords = {toolkit, mental health, voice analysis, monitor, health care, mobile phones},
location = {Beijing, China},
series = {BodyNets '11}
}

@inproceedings{10.5555/2132325.2132411,
author = {Krichmar, Jeffrey L. and Dutt, Nikil and Nageswaran, Jayram M. and Richert, Micah},
title = {Neuromorphic Modeling Abstractions and Simulation of Large-Scale Cortical Networks},
year = {2011},
isbn = {9781457713989},
publisher = {IEEE Press},
abstract = {Biological neural systems are well known for their robust and power-efficient operation in highly noisy environments. We outline key modeling abstractions for the brain and focus on spiking neural network models. We discuss aspects of neuronal processing and computational issues related to modeling these processes. Although many of these algorithms can be efficiently realized in specialized hardware, we present a case study of simulation of the visual cortex using a GPU based simulation environment that is readily usable by neuroscientists and computer scientists and efficient enough to construct very large networks comparable to brain networks.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
pages = {334–338},
numpages = {5},
keywords = {GPU, synapse, parallel processing, computational neuroscience, spiking neural networks, vision},
location = {San Jose, California},
series = {ICCAD '11}
}

@inproceedings{10.5555/2093889.2093906,
author = {Mihailescu, Madalin and Rodriguez, Andres and Amza, Cristiana and Palcikovs, Dmitrijs and Iszlai, Gabriel and Trossman, Andrew and Ng, Joanna},
title = {Enhancing Application Robustness in Cloud Data Centers},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {We propose OX, a runtime system that uses application-level availability constraints and application topologies discovered on the fly to enhance resilience to infrastructure anomalies for cloud applications. OX allows application owners to specify groups of highly available virtual machines, following component roles and replication semantics. To discover application topologies, OX monitors network traffic among virtual machines, transparently. Based on this information, OX builds on-line topology graphs for applications and incrementally partitions these graphs across the infrastructure to enforce availability constraints and optimize communication between virtual machines. We evaluate OX in a realistic cloud setting using a mix of Hadoop and YCSB/Cassandra workloads. We show how OX increases application robustness, by protecting applications from network interference effects and rack-level failures.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {133–147},
numpages = {15},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.1145/2071423.2071434,
author = {Conroy, David and Wyeth, Peta and Johnson, Daniel},
title = {Modeling Player-like Behavior for Game AI Design},
year = {2011},
isbn = {9781450308274},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2071423.2071434},
doi = {10.1145/2071423.2071434},
abstract = {The design of artificial intelligence in computer games is an important component of a player's game play experience. As games are becoming more life-like and interactive, the need for more realistic game AI will increase. This is particularly the case with respect to AI that simulates how human players act, behave and make decisions. The purpose of this research is to establish a model of player-like behavior that may be effectively used to inform the design of artificial intelligence to more accurately mimic a player's decision making process. The research uses a qualitative analysis of player opinions and reactions while playing a first person shooter video game, with recordings of their in-game actions, speech and facial characteristics. The initial studies provide player data that has been used to design a model of how a player behaves.},
booktitle = {Proceedings of the 8th International Conference on Advances in Computer Entertainment Technology},
articleno = {9},
numpages = {8},
keywords = {player model, user study, video games, AI, design},
location = {Lisbon, Portugal},
series = {ACE '11}
}

@inproceedings{10.1145/2071423.2071431,
author = {Lopes, Ricardo and Bidarra, Rafael},
title = {A Semantic Generation Framework for Enabling Adaptive Game Worlds},
year = {2011},
isbn = {9781450308274},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2071423.2071431},
doi = {10.1145/2071423.2071431},
abstract = {Adaptive games are expected to improve on the pre-scripted and rigid nature of traditional games. Current research uses player and experience modeling techniques to successfully predict some game-play adjustments players desire. These are typically deployed to adapt AI behavior or to evolve content for simple game levels. In this paper we propose a generation framework aimed at creating personalized content for complex and immersive game worlds. This framework, currently under development, captures which content provided the context for a given personal gameplay experience. This model is then used to generate content for the next predicted experience, through retrieval and recombination of semantic gameplay descriptions, i.e. case-based mappings between content and player experience. Through its integration with existing player and experience modeling techniques, this framework aims at generating, in an emergent way, game worlds that better suit players. Dynamic game content, which responds to the player performance, has the ability to personalize player experience, potentially making games even more unpredictable and fun.},
booktitle = {Proceedings of the 8th International Conference on Advances in Computer Entertainment Technology},
articleno = {6},
numpages = {8},
keywords = {semantics, adaptive game worlds, procedural content generation},
location = {Lisbon, Portugal},
series = {ACE '11}
}

@inproceedings{10.1145/2070364.2070410,
author = {West, Matthew T.},
title = {Ubiquitous Computing},
year = {2011},
isbn = {9781450310239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070364.2070410},
doi = {10.1145/2070364.2070410},
abstract = {Ubiquitous computing takes computation from the desktop environment and moves it into every area of our lives. Instantaneous information and computation will be distributed over an array of small wireless networked devices. These can be embedded in daily artifacts such as appliances, light switches, stereos, cellular phones, and watches. This capability will revolutionize computation, allowing it to take place anywhere and at any time. Rather than accessing data only via a monitor and keyboard, one might access data via voice-activated commands and view it on a neighboring wall. Computation will be everywhere. Such technology will allow doctors to access medical histories during surgery uninhibited or help an architect inspect blueprint changes on site. For such a revolution to occur, however, an infrastructure and affordable technology needs to be established. This presentation will consist of a history of ubiquitous computing and an examination of current research developments. Advancing battery technology, wireless protocols (Bluetooth, IEEE 802.11, and LTE), applications, current examples, and social implications will be discussed.},
booktitle = {Proceedings of the 39th Annual ACM SIGUCCS Conference on User Services},
pages = {175–182},
numpages = {8},
keywords = {mark weiser, ubiquitous computing, pervasive computing},
location = {San Diego, California, USA},
series = {SIGUCCS '11}
}

@inproceedings{10.1145/2070821.2070824,
author = {Menzies, Tim and Bird, Christian and Zimmermann, Thomas and Schulte, Wolfram and Kocaganeli, Ekrem},
title = {The Inductive Software Engineering Manifesto: Principles for Industrial Data Mining},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070824},
doi = {10.1145/2070821.2070824},
abstract = {The practices of industrial and academic data mining are very different. These differences have significant implications for (a) how we manage industrial data mining projects; (b) the direction of academic studies in data mining; and (c) training programs for engineers who seek to use data miners in an industrial setting.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {19–26},
numpages = {8},
keywords = {industry, inductive engineering},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@inproceedings{10.1145/2063384.2063408,
author = {Zhang, Xuechen and Davis, Kei and Jiang, Song},
title = {QoS Support for End Users of I/O-Intensive Applications Using Shared Storage Systems},
year = {2011},
isbn = {9781450307710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063384.2063408},
doi = {10.1145/2063384.2063408},
abstract = {While the performance of compute-bound applications can be effectively guaranteed with techniques such as space sharing or QoS-aware process scheduling, it remains a challenge to meet QoS requirements for end users of I/O-intensive applications using shared storage systems because of the difficulty of differentiating I/O services for different applications with individual quality requirements. Furthermore, it is difficult for end users to accurately specify performance goals to the storage system using I/O-related metrics such as request latency or throughput. As access patterns, request rates, and the system workload change in time, a fixed I/O performance goal, such as bounds on throughput or latency, can be expensive to achieve and may not provide performance guarantees such as bounded program execution time.We propose a scheme supporting end-users' QoS goals, specified in terms of program execution time, in shared storage environments. We automatically translate the users' performance goals into instantaneous I/O throughput bounds using a machine learning technique, and use dynamically determined service time windows to efficiently meet the throughput bounds. We have implemented this scheme in the PVFS2 parallel file system and have conducted an extensive evaluation. Our results show that this scheme can satisfy realistic end-user QoS requirements by making highly efficient use of the I/O resources. The scheme seeks to balance programs' attainment of QoS requirements, and saves as much of the remaining I/O capacity as possible for best-effort programs.},
booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {18},
numpages = {12},
keywords = {shared storage system, quality of service, PVFS2, QoS},
location = {Seattle, Washington},
series = {SC '11}
}

@inproceedings{10.1145/2063348.2063382,
author = {Pedicini, Georgia and Green, Jennifer},
title = {SPOTlight on Testing: Stability, Performance and Operational Testing of LANL HPC Clusters},
year = {2011},
isbn = {9781450311397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063348.2063382},
doi = {10.1145/2063348.2063382},
abstract = {Testing is sometimes a forgotten component of system management, but it becomes very important in the realm of High Performance Computing (HPC) clusters. Many large-scale HPC cluster installations are one of a kind, with unknown issues and unexpected behaviors. First, the initial installation may uncover complex configuration interactions that are only apparent at scale; Stability becomes a critical feature of early system testing. Second, Performance may be significantly impacted by small changes to the system. Third, after initial shakeout, users expect a system that is reliable on their terms; ongoing Operational tests verify reliability, and provide early warning of developing problems. A robust test suite should address all of these test categories, and present both tests and results in a manner that meets usability requirements. We will describe Los Alamos National Laboratory's current test suite, and the development project to expand the suite to cover these areas and provide better tools for analysis and reporting.},
booktitle = {State of the Practice Reports},
articleno = {25},
numpages = {8},
keywords = {stability testing, performance testing, test framework, operational testing, serviceability, reliability, test driven development, SPOT, high performance computing, RAS, accessibility},
location = {Seattle, Washington},
series = {SC '11}
}

@inproceedings{10.1145/2096123.2096134,
author = {Hazra, Jagabondhu and Das, Kaushik and Seetharam, Deva P. and Singhee, Amith},
title = {Stream Computing Based Synchrophasor Application for Power Grids},
year = {2011},
isbn = {9781450310611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2096123.2096134},
doi = {10.1145/2096123.2096134},
abstract = {This paper proposes an application of stream computing analytics framework to high speed synchrophasor data for real time monitoring and control of electric grid. High volume streaming synchrophasor data from geographically distributed grid sensors (namely, Phasor Measurement Units) are collected, synchronized, aggregated when required and analyzed using a stream computing platform to estimate the grid stability in real time. This real time stability monitoring scheme will help the grid operators to take preventive or corrective measures ahead of time to mitigate any disturbance before they develop into wide-spread. A protptype of the scheme is demonstrated on a benchmark 3 machines 9 bus system and the IEEE 14 bus test system.},
booktitle = {Proceedings of the First International Workshop on High Performance Computing, Networking and Analytics for the Power Grid},
pages = {43–50},
numpages = {8},
keywords = {power grid, voltage stability, stream computing, synchrophasor},
location = {Seattle, Washington, USA},
series = {HiPCNA-PG '11}
}

@inproceedings{10.1145/2096123.2096139,
author = {Salvini, Stefano and Lopatka, Piotr and Wallom, David},
title = {A Hardware and Software Computational Platform for the HiPerDNO (High Performance Distribution Network Operation) Project},
year = {2011},
isbn = {9781450310611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2096123.2096139},
doi = {10.1145/2096123.2096139},
abstract = {The HiPerDNO project aims to develop new applications to enhance the operational capabilities of Distribution Network Operators (DNO). Their delivery requires an advanced computational strategy. This paper describes a High Performance Computing (HPC) platform developed for these applications whilst also being flexible enough to accommodate new ones emerging from the gradual introduction of smart metering in the Low Voltage (LV) networks (AMI: Advanced Metering Infrastructures). Security and reliability requirements for both data and computations are very stringent. Our proposed architecture would allow the deployment of computations and data access as services, thus achieving independence on the actual hardware and software technologies deployed, and hardening against malicious as well as accidental corruptions. Cost containment and reliance on proven technologies are also of paramount importance to DNOs. We suggest an architecture that fulfills these needs, which includes the following components for the HPC and Data Storage systems: Hadoop Distributed File System, a federation of loosely coupled computational clusters, the PELICAN computational application framework},
booktitle = {Proceedings of the First International Workshop on High Performance Computing, Networking and Analytics for the Power Grid},
pages = {75–82},
numpages = {8},
keywords = {systems design, smart grid, high performance computing applications},
location = {Seattle, Washington, USA},
series = {HiPCNA-PG '11}
}

@inproceedings{10.1145/2089142.2089150,
author = {Peric\`{a}s, Miquel and Martorell, Xavier and Etsion, Yoav},
title = {Implementation of a Hierarchical N-Body Simulator Using the Ompss Programming Model},
year = {2011},
isbn = {9781450311212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2089142.2089150},
doi = {10.1145/2089142.2089150},
abstract = {Many HPC algorithms are highly irregular. They have input-dependent control flow and operate on pointer-based data structures such as trees, graphs, or linked lists. This irregularity makes it challenging to parallelize such algorithms in order to efficiently run them on modern HPC systems. In this paper we study the architectural and programming bottlenecks of the OmpSs task-based programming model when implementing irregular applications. We select a sequential N-body simulation code and describe its parallelization using OmpSs. We then analyze the code, focusing on scalability and load balancing. We conclude that, in general, task-based programming models are well suited to the exploitation of irregular parallelism. Nevertheless, in order to avoid the overheads associated with manually managing the load balancing, the hardware and runtime will need to collectively support much finer-grained tasks.},
booktitle = {Proceedings of the 1st Workshop on Irregular Applications: Architectures and Algorithms},
pages = {23–30},
numpages = {8},
keywords = {ompSs, taskSs, barnes-hut},
location = {Seattle, Washington, USA},
series = {IA<sup>3</sup> '11}
}

@inproceedings{10.1145/2070481.2070528,
author = {Batrinca, Ligia Maria and Mana, Nadia and Lepri, Bruno and Pianesi, Fabio and Sebe, Nicu},
title = {Please, Tell Me about Yourself: Automatic Personality Assessment Using Short Self-Presentations},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070528},
doi = {10.1145/2070481.2070528},
abstract = {Personality plays an important role in the way people manage the images they convey in self-presentations and employment interviews, trying to affect the other"s first impressions and increase effectiveness. This paper addresses the automatically detection of the Big Five personality traits from short (30-120 seconds) self-presentations, by investigating the effectiveness of 29 simple acoustic and visual non-verbal features. Our results show that Conscientiousness and Emotional Stability/Neuroticism are the best recognizable traits. The lower accuracy levels for Extraversion and Agreeableness are explained through the interaction between situational characteristics and the differential activation of the behavioral dispositions underlying those traits.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {255–262},
numpages = {8},
keywords = {personality trait detection, big five, self-presentation},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070550,
author = {Do, Trinh Minh Tri and Blom, Jan and Gatica-Perez, Daniel},
title = {Smartphone Usage in the Wild: A Large-Scale Analysis of Applications and Context},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070550},
doi = {10.1145/2070481.2070550},
abstract = {This paper presents a large-scale analysis of contextualized smartphone usage in real life. We introduce two contextual variables that condition the use of smartphone applications, namely places and social context. Our study shows strong dependencies between phone usage and the two contextual cues, which are automatically extracted based on multiple built-in sensors available on the phone. By analyzing continuous data collected on a set of 77 participants from a European country over 9 months of actual usage, our framework automatically reveals key patterns of phone application usage that would traditionally be obtained through manual logging or questionnaire. Our findings contribute to the large-scale understanding of applications and context, bringing out design implications for interfaces on smartphones.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {353–360},
numpages = {8},
keywords = {location context, large-scale study, smartphone, phone usage, social context},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070486,
author = {McDuff, Daniel and el Kaliouby, Rana and Picard, Rosalind},
title = {Crowdsourced Data Collection of Facial Responses},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070486},
doi = {10.1145/2070481.2070486},
abstract = {In the past, collecting data to train facial expression and affect recognition systems has been time consuming and often led to data that do not include spontaneous expressions. We present the first crowdsourced data collection of dynamic, natural and spontaneous facial responses as viewers watch media online. This system allowed a corpus of 3,268 videos to be collected in under two months.We characterize the data in terms of viewer demographics, position, scale, pose and movement of the viewer within the frame, and illumination of the facial region. We compare statistics from this corpus to those from the CK+ and MMI databases and show that distributions of position, scale, pose, movement and luminance of the facial region are significantly different from those represented in these datasets.We demonstrate that it is possible to efficiently collect massive amounts of ecologically valid responses, to known stimuli, from a diverse population using such a system. In addition facial feature points within the videos can be tracked for over 90% of the frames. These responses were collected without need for scheduling, payment or recruitment. Finally, we describe a subset of data (over 290 videos) that will be available for the research community.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {11–18},
numpages = {8},
keywords = {facial responses, non-verbal communication, crowdsourcing},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070488,
author = {Sarvadevabhatla, Ravi Kiran and Benovoy, Mitchel and Musallam, Sam and Ng-Thow-Hing, Victor},
title = {Adaptive Facial Expression Recognition Using Inter-Modal Top-down Context},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070488},
doi = {10.1145/2070481.2070488},
abstract = {The role of context in recognizing a person's affect is being increasingly studied. In particular, context arising from the presence of multi-modal information such as faces, speech and head pose has been used in recent studies to recognize facial expressions. In most approaches, the modalities are independently considered and the effect of one modality on the other, which we call inter-modal influence (e.g. speech or head pose modifying the facial appearance) is not modeled. In this paper, we describe a system that utilizes context from the presence of such inter-modal influences to recognize facial expressions. To do so, we use 2-D contextual masks which are activated within the facial expression recognition pipeline depending on the prevailing context. We also describe a framework called the Context Engine. The Context Engine offers a scalable mechanism for extending the current system to address additional modes of context that may arise during human-machine interactions. Results on standard data sets demonstrate the utility of modeling inter-modal contextual effects in recognizing facial expressions.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {27–34},
numpages = {8},
keywords = {context, mask, multi-modal, facial expression recognition, human-computer interaction},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070551,
author = {Williamson, Julie R. and Crossan, Andrew and Brewster, Stephen},
title = {Multimodal Mobile Interactions: Usability Studies in Real World Settings},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070551},
doi = {10.1145/2070481.2070551},
abstract = {This paper presents a study that explores the issues of mobile multimodal interactions while on the move in the real world. Because multimodal interfaces allow new kinds of eyes and hands free interactions, usability issues while moving through different public spaces becomes an important issue in user experience and acceptance of multimodal interaction. This study focuses on these issues by deploying an RSS reader that participants used during their daily commute every day for one week. The system allows users on the move to access news feeds eyes free through head- phones playing audio and speech and hands free through wearable sensors attached to the wrists. The results showed participants were able to interact with the system on the move and became more comfortable performing these interactions as the study progressed. Users were also far more comfortable gesturing on the street than on public transport, which was reflected in the number of interactions and the perceived social acceptability of the gestures in different contexts.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {361–368},
numpages = {8},
keywords = {whole body interaction, user studies in the wild, wrist rotation, mobile interaction, inertial sensing},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070485,
author = {Mart\'{\i}nez, H\'{e}ctor P. and Yannakakis, Georgios N.},
title = {Mining Multimodal Sequential Patterns: A Case Study on Affect Detection},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070485},
doi = {10.1145/2070481.2070485},
abstract = {Temporal data from multimodal interaction such as speech and bio-signals cannot be easily analysed without a preprocessing phase through which some key characteristics of the signals are extracted. Typically, standard statistical signal features such as average values are calculated prior to the analysis and, subsequently, are presented either to a multimodal fusion mechanism or a computational model of the interaction. This paper proposes a feature extraction methodology which is based on frequent sequence mining within and across multiple modalities of user input. The proposed method is applied for the fusion of physiological signals and gameplay information in a game survey dataset. The obtained sequences are analysed and used as predictors of user affect resulting in computational models of equal or higher accuracy compared to the models built on standard statistical features.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {3–10},
numpages = {8},
keywords = {preference learning, heart rate variability, skin conductance, sequence classification, sequence pattern mining, game events},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070553,
author = {Baumann, Hannes and Starner, Thad and Iben, Hendrik and Lewandowski, Anna and Zschaler, Patrick},
title = {Evaluation of Graphical User-Interfaces for Order Picking Using Head-Mounted Displays},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070553},
doi = {10.1145/2070481.2070553},
abstract = {Order picking is the process of collecting items from an assortment in inventory. It represents one of the main activities performed in warehouses and accounts for about 60% of the total operational costs of a warehouse. In previous work, we demonstrated the advantages of a head-mounted display (HMD) based picking chart over a traditional text-based pick list, a paper-based graphical pick chart, and a mobile pick-by-voice system. Here we perform two user studies that suggest that adding color cues and context sensing via a laser rangefinder improves picking accuracy with the HMD system. We also examine other variants of the pick chart, such as adding symbols, textual identifiers, images, and descriptions and their effect on accuracy, speed, and subjective usability.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {377–384},
numpages = {8},
keywords = {evaluation, wearable computers, order picking, user interface, user study, head-mounted display},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070490,
author = {Nijholt, Anton and Allison, Brendan Z. and Jacob, Rob J.K.},
title = {Brain-Computer Interaction: Can Multimodality Help?},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070490},
doi = {10.1145/2070481.2070490},
abstract = {This paper is a short introduction to a special ICMI session on brain-computer interaction. During this paper, we first discuss problems, solutions, and a five-year view for brain-computer interaction. We then talk further about unique issues with multimodal and hybrid brain-computer interfaces, which could help address many current challenges. This paper presents some potentially controversial views, which will hopefully inspire discussion about the different views on brain-computer interfacing, how to embed brain-computer interfacing in a multimodal and multi-party context, and, more generally, how to look at brain-computer interfacing from an ambient intelligence point of view.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {35–40},
numpages = {6},
keywords = {hybrid BCIs, multimodal interaction, bandwidth, brain-computer interaction},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070545,
author = {Ros, Raquel and Nalin, Marco and Wood, Rachel and Baxter, Paul and Looije, Rosemarijn and Demiris, Yannis and Belpaeme, Tony and Giusti, Alessio and Pozzi, Clara},
title = {Child-Robot Interaction in the Wild: Advice to the Aspiring Experimenter},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070545},
doi = {10.1145/2070481.2070545},
abstract = {We present insights gleaned from a series of child-robot interaction experiments carried out in a hospital paediatric department. Our aim here is to share good practice in experimental design and lessons learned about the implementation of systems for social HRI with child users towards application in "the wild", rather than in tightly controlled and constrained laboratory environments: a trade-off between the structures imposed by experimental design and the desire for removal of such constraints that inhibit interaction depth, and hence engagement, requires a careful balance.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {335–342},
numpages = {8},
keywords = {interaction design, experimental practice, child-robot interaction, human-robot interaction, social robotics},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070491,
author = {G\"{u}rk\"{o}k, Hayrettin and Hakvoort, Gido and Poel, Mannes},
title = {Modality Switching and Performance in a Thought and Speech Controlled Computer Game},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070491},
doi = {10.1145/2070481.2070491},
abstract = {Providing multiple modalities to users is known to improve the overall performance of an interface. Weakness of one modality can be overcome by the strength of another one. Moreover, with respect to their abilities, users can choose between the modalities to use the one that is the best for them. In this paper we explored whether this holds for direct control of a computer game which can be played using a brain-computer interface (BCI) and an automatic speech recogniser (ASR). Participants played the games in unimodal mode (i.e. ASR-only and BCI-only) and multimodal mode where they could switch between the two modalities. The majority of the participants switched modality during the multimodal game but for the most of the time they stayed in ASR control. Therefore multimodality did not provide a significant performance improvement over unimodal control in our particular setup. We also investigated the factors which influence modality switching. We found that performance and peformance-related factors were prominently effective in modality switching.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {41–48},
numpages = {8},
keywords = {games, hybrid BCI, multimodal interaction, automatic speech recogniser, SSVEP, brain-computer interface},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.1145/2070481.2070533,
author = {Lai, Chi-Hsia and Niinim\"{a}ki, Matti and Tahiroglu, Koray and Kildal, Johan and Ahmaniemi, Teemu},
title = {Perceived Physicality in Audio-Enhanced Force Input},
year = {2011},
isbn = {9781450306416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070481.2070533},
doi = {10.1145/2070481.2070533},
abstract = {This paper investigates how the perceived physicality of the action of applying force with a finger on a rigid surface (such as on a force-sensing touch screen) can be enhanced using real-time synthesized audio feedback. A selection of rich and evocative audio designs was used. Additionally, audio-tactile cross-modal integration was encouraged, by observing that the main rules of multisensory integration were supported. The study conducted showed that richness of perceived physicality increased considerably, mostly in its auditory expression (what pressing sounded like). In addition, in many instances it was observed that the haptic expression of physicality also increased (what pressing felt like), including some perception of compliance. This last result was particularly interesting as it showed that audio-tactile cross-modal integration might be present.},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
pages = {287–294},
numpages = {8},
keywords = {audio feedback, force input device, multimodal., physicality},
location = {Alicante, Spain},
series = {ICMI '11}
}

@inproceedings{10.5555/2078101.2078103,
author = {Conroy, David and Wyeth, Peta},
title = {Building Better Bad Guys: A New Framework for Game AI Design},
year = {2011},
isbn = {9781450305662},
publisher = {Interactive Entertainment},
address = {Wellington, NZL},
abstract = {Realistic artificial intelligence in video games is important to developers in the games industry. It helps to better immerse the player and keep them in a state of flow. In order to achieve this it is important to design computer opponents to behave and react similarly to human players. In this study we designed a model of human behaviour for a specific interactive component in gaming (aiming). It was built using player game play data and user opinion regarding the subject. The result was a system of behaviour akin to that of human players.},
booktitle = {Proceedings of the 7th Australasian Conference on Interactive Entertainment},
articleno = {2},
numpages = {3},
keywords = {game design, interaction design, non-player characters},
location = {Wellington, New Zealand},
series = {IE '10}
}

@inproceedings{10.1145/2072298.2072305,
author = {Subramanian, Ramanathan and Yanulevskaya, Victoria and Sebe, Nicu},
title = {Can Computers Learn from Humans to See Better? Inferring Scene Semantics from Viewers' Eye Movements},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072305},
doi = {10.1145/2072298.2072305},
abstract = {This paper describes an attempt to bridge the semantic gap between computer vision and scene understanding employing eye movements. Even as computer vision algorithms can efficiently detect scene objects, discovering semantic relationships between these objects is as essential for scene understanding. Humans understand complex scenes by rapidly moving their eyes (saccades) to selectively focus on salient entities (fixations). For 110 social scenes, we compared verbal descriptions provided by observers against eye movements recorded during a free-viewing task. Data analysis confirms (i) a strong correlation between task-explicit linguistic descriptions and task-implicit eye movements, both of which are influenced by underlying scene semantics and (ii) the ability of eye movements in the form of fixations and saccades to indicate salient entities and entity relationships mentioned in scene descriptions.We demonstrate how eye movements are useful for inferring the meaning of social (everyday scenes depicting human activities) and affective (emotion-evoking content like expressive faces, nudes) scenes. While saliency has always been studied through the prism of fixations, we show that saccades are particularly useful for (i) distinguishing mild and high-intensity facial expressions and (ii) discovering interactive actions between scene entities.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {33–42},
numpages = {10},
keywords = {fixations and saccades, salient entities and interactions, scene semantics, eye movements},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072351,
author = {Song, Wei and Tjondronegoro, Dian and Docherty, Michael},
title = {Saving Bitrate vs. Pleasing Users: Where is the Break-Even Point in Mobile Video Quality?},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072351},
doi = {10.1145/2072298.2072351},
abstract = {This paper presents a comprehensive study to find the most efficient bitrate requirement to deliver mobile video that optimizes bandwidth, while at the same time maintains good user viewing experience. In the study, forty participants were asked to choose the lowest quality video that would still provide for a comfortable and long-term viewing experience, knowing that higher video quality is more expensive and bandwidth intensive. This paper proposes the lowest pleasing bitrates and corresponding encoding parameters for five different content types: cartoon, movie, music, news and sports. It also explores how the lowest pleasing quality is influenced by content type, image resolution, bitrate, and user gender, prior viewing experience, and preference. In addition, it analyzes the trajectory of users' progression while selecting the lowest pleasing quality. The findings reveal that the lowest bitrate requirement for a pleasing viewing experience is much higher than that of the lowest acceptable quality. Users' criteria for the lowest pleasing video quality are related to the video's content features, as well as its usage purpose and the user's personal preferences. These findings can provide video providers guidance on what quality they should offer to please mobile users.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {403–412},
numpages = {10},
keywords = {user profile, mobile video, encoding parameters, bitrate, content type, acceptability},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2071536.2071583,
author = {Vutborg, Ren\'{e} and Kjeldskov, Jesper and Paay, Jeni and Pedell, Sonja and Vetere, Frank},
title = {Supporting Young Children's Communication with Adult Relatives across Time Zones},
year = {2011},
isbn = {9781450310901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2071536.2071583},
doi = {10.1145/2071536.2071583},
abstract = {Regular contact between children and their adult relatives can be a problem if they live in different time zones. In this situation, finding an agreed time to contact each other can be both confusing and complicated. This paper presents a study of the effect of time zone differences on communication between grandparents and grandchildren living in different time zones. We deployed a system between time zone distributed families to study this effect and analysed its use based on four parameters of time and events based theory: rigid sequential structures (that some events cannot occur before others), fixed durations (that most events always last the same time), standard temporal locations (that events have a standard time when they occur during the day) and uniform rates of recurrence (that some events always reoccur at a uniform rate). Our findings highlight the importance of: the need to consider the parents' role in facilitating contact and making the technology easy to use by children independently; the advantage of concurrent synchronous and asynchronous interaction forms; and the need to respect people's private time. These findings can inform the design of technology for supporting young children's communications with adult relatives across time zones.},
booktitle = {Proceedings of the 23rd Australian Computer-Human Interaction Conference},
pages = {291–300},
numpages = {10},
keywords = {play across distance, storytelling, intergenerational play, communication across time zones},
location = {Canberra, Australia},
series = {OzCHI '11}
}

@inproceedings{10.1145/2072561.2072570,
author = {Niu, Jianwei and Huo, Da and Zeng, Xiao and Mugan, Jonathan},
title = {Interactive and Real-Time Generation of Home Video Summaries on Mobile Devices},
year = {2011},
isbn = {9781450309950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072561.2072570},
doi = {10.1145/2072561.2072570},
abstract = {With the proliferation of mobile devices and multimedia, videos have become an indispensable part of life-logs for personal experiences. In this paper, we present a real-time and interactive application for home video summarization on mobile devices. The main challenge of this method is lack of information about the video content in the following frames, which we term "partial-context" in this paper. First of all, real-time segmentation algorithm based on partial-context is applied to decompose the captured video into segments in line with the change in dominant camera motion. Secondly, the main challenge to conventional video summarization is the semantic understanding of the video content. Thus, we leverage the fact that it is easy to get user input on a mobile device and attack this problem through the user interaction. The user preference is learned and modeled by a Gaussian Mixture Model (GMM), which is updated each time when users manually select key frames. Evaluation results demonstrate that our system significantly improves user experience and provides an efficient automatic/semi-automatic video summarization solution for mobile users.},
booktitle = {Proceedings of the 2011 International ACM Workshop on Interactive Multimedia on Mobile and Portable Devices},
pages = {27–32},
numpages = {6},
location = {Scottsdale, Arizona, USA},
series = {IMMPD '11}
}

@inproceedings{10.1145/2381416.2381432,
author = {Islinger, Tobias and K\"{o}hler, Thorsten and Wolff, Christian},
title = {Human Modeling in a Driver Analyzing Context: Challenge and Benefit},
year = {2011},
isbn = {9781450312318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2381416.2381432},
doi = {10.1145/2381416.2381432},
abstract = {In the past years, driver analyzing has become a field of increasing interest. Within this topic, camera based as well as camera free systems are in the scope of researchers all over the world with the overall goal to detect, for example, critical driver states like drowsiness or distraction. Unfortunately, there are yet no comprehensive models for understanding the driver and his states in the automotive context. Therefore, we present a user model tailored to automotive needs. This model allows us to understand the driver in the automotive environment and to set up a general architecture from which we can decide on necessary input information for detecting a certain driver state.},
booktitle = {Proceedings of the 3rd International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {99–104},
numpages = {6},
keywords = {context recognition, vehicle context information, driver monitoring, driver analyzer, driver model},
location = {Salzburg, Austria},
series = {AutomotiveUI '11}
}

@inproceedings{10.1145/2381416.2381434,
author = {Meschtscherjakov, Alexander and Wilfinger, David and Gridling, Nicole and Neureiter, Katja and Tscheligi, Manfred},
title = {Capture the Car! Qualitative in-Situ Methods to Grasp the Automotive Context},
year = {2011},
isbn = {9781450312318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2381416.2381434},
doi = {10.1145/2381416.2381434},
abstract = {In terms of human computer interaction (HCI), the car interior is a space, which can be divided into three areas: the driver's area, the front seat area, and the back seat area. So far HCI researchers have primary focused on the driver, and how in-car electronic devices can be designed to assist the driver in order to increase safety and comfort. We propose that for investigating interactive technology in the car in a holistic way, all three areas have to be taken into account. For that purpose we argue for an increased usage of qualitative in-situ studies, which have been hardly applied in automotive user interface research. So far the HCI community has mainly focused on laboratory studies utilizing driving simulators. Despite the broad range of available field study methods, such as ethnographic and self-reporting studies, the adaption of these methods for the automotive context is challenging due to the specific characteristics of this environment. For instance, cars provide only very limited space, the environment is constantly changing while driving and the driver must not be distracted from driving safely. As a consequence, a lack of experience exists, on how in-situ methods should be applied to cars. In this paper we describe three qualitative in-situ studies, we conducted to research the driver, the front seat passenger, and the rear seat passenger spaces. All three studies used a different method tailored to fit these three areas best. To share our experiences and insights we discuss the strengths and pitfalls of each method.},
booktitle = {Proceedings of the 3rd International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {105–112},
numpages = {8},
keywords = {ethnography, culturing probing, contextual inquiry, in-situ study},
location = {Salzburg, Austria},
series = {AutomotiveUI '11}
}

@inproceedings{10.1145/2072609.2072614,
author = {Aisopos, Fotis and Papadakis, George and Varvarigou, Theodora},
title = {Sentiment Analysis of Social Media Content Using N-Gram Graphs},
year = {2011},
isbn = {9781450309899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072609.2072614},
doi = {10.1145/2072609.2072614},
abstract = {Sentiment Analysis over Social Media facilitates the extraction of useful conclusions about the average public opinion on a variety of topics, but poses serious technical challenges. This is because of the sparse, noisy, multilingual content that is posted on-line by Social Media users. In this paper, we introduce a novel method for capturing textual patterns that inherently supports this challenging type of content. In essence, it creates a graph whose nodes correspond to the character n-grams of a document, while its weighted edges denote the average distance between them. Multiple documents of the same polarity can be aggregated into a polarity class graph, which can be compared with individual documents in order to identify the category of their sentiment. To evaluate our approach, we conducted large scale experiments on a real-world data set stemming from a snapshot of Twitter activity. The outcomes of our evaluation indicate significant improvements over other the methods typically used in this context, not only with respect to effectiveness, but also to efficiency.},
booktitle = {Proceedings of the 3rd ACM SIGMM International Workshop on Social Media},
pages = {9–14},
numpages = {6},
keywords = {social media, polarity classification, n-gram graphs},
location = {Scottsdale, Arizona, USA},
series = {WSM '11}
}

@inproceedings{10.5555/2483628.2483632,
author = {Van, Alina and Gay, Valerie C. and Kennedy, Paul J. and Barin, Edward and Leijdekkers, Peter},
title = {Understanding Risk Factors in Cardiac Rehabilitation Patients with Random Forests and Decision Trees},
year = {2011},
isbn = {9781921770029},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Cardiac rehabilitation is a well-recognised non-pharmacological intervention recommended for the prevention of cardiovascular disease. Numerous studies have produced large amounts of data to examine the above aspects in patient groups. In this paper, datasets collected for over a 10 year period by one Australian hospital are analysed using decision trees to derive prediction rules for the outcome of phase II cardiac rehabilitation. Analysis includes prediction of the outcome of the cardiac rehabilitation program in terms of three groups of cardiovascular risk factors: physiological, psychosocial and performance risk factors. Random forests are used for feature selection to make the models compact and interpretable. Balanced sampling is used to deal with heavily imbalanced class distribution. Experimental results show that the outcome of phase II cardiac rehabilitation in terms of physiological, psychosocial and performance risk factor can be predicted based on initial readings of cholesterol level and hypertension, level achieved in six minute walk test, and Hospital Anxiety and Depression Score (HADS) anxiety score and HADS depression score respectively. This will allow for identifying high risk patient groups and developing personalised cardiac rehabilitation programs for those patients to increase their chances of success and minimize their risk of failure.},
booktitle = {Proceedings of the Ninth Australasian Data Mining Conference - Volume 121},
pages = {11–22},
numpages = {12},
keywords = {cardiac rehabilitation, random forests, balanced sampling, decision trees, feature selection},
location = {Ballarat, Australia},
series = {AusDM '11}
}

@article{10.1145/2095272.2095275,
author = {Hamilton, M. and Salim, F. and Cheng, E. and Choy, S. L.},
title = {Transafe: A Crowdsourced Mobile Platform for Crime and Safety Perception Management},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0095-2737},
url = {https://doi.org/10.1145/2095272.2095275},
doi = {10.1145/2095272.2095275},
abstract = {An earlier version of this paper was presented at the 2011 IEEE International Symposium on Technology and Society (ISTAS) at Saint Xavier University in Chicago, Illinois (and printed in the 2011 ISTAS proceedings).This paper describes a proposed mobile platform, Transafe, that captures and analyses public perceptions of safety to deliver 'crowdsourced' collective intelligence about places in the City of Melbourne, Australia, and their affective states at various times of the day. Public perceptions of crime on public transport in Melbourne are often mismatched with actual crime statistics and such perceptions thus can act as social barriers to visitors and locals traversing within and through the city. Using interactive mobile applications and social media, the visualization of this crowdsourced safety perception information will increase the commuter's awareness of various situations in the City of Melbourne. In addition, through social behavioral analysis and ethnographic research, the collective public intelligence will also help inform the stakeholders of the city for future policy-making and policing strategies for safety perception management. At the centre of the proposed platform is the design and development of a mobile phone application that can contribute to people feeling safer by supporting users to report crimes and misdemeanors that they witness, and provide information about transportation and emergency services around where the users are located. The proposed application can also act as a crime deterrent with one feature that enables user tracking by up to three nominated friends if the user opts to activate tracking when feeling unsafe while roaming the city.},
journal = {SIGCAS Comput. Soc.},
month = dec,
pages = {32–37},
numpages = {6},
keywords = {safety perception, crime perception, crowdsourcing, mobile application}
}

@article{10.1145/2043236.2043244,
author = {Yu, Harlan and Schultze, Stephen},
title = {Using Software to Liberate U.S. Case Law},
year = {2011},
issue_date = {Winter 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/2043236.2043244},
doi = {10.1145/2043236.2043244},
abstract = {Although public information is open, it is not always easily accessible.},
journal = {XRDS},
month = dec,
pages = {12–15},
numpages = {4}
}

@article{10.1145/2095272.2095274,
author = {Cikic, Sabine and K\"{u}cklich, Julian R.},
title = {What FarmVille Can Teach Us about Cooperative Workflows and Architectures},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0095-2737},
url = {https://doi.org/10.1145/2095272.2095274},
doi = {10.1145/2095272.2095274},
abstract = {An earlier version of this paper was presented at the 2011 IEEE International Symposium on Technology and Society (ISTAS) at Saint Xavier University in Chicago, Illinois (and printed in the 2011 ISTAS proceedings).Perhaps the most remarkable characteristic of digital social games such as Zynga's FarmVille is that they are designed in such a way that any user, regardless of their skills and experience, can familiarise themselves in a matter of moments with the object of the game, the interface, and the tools and options involved in playing them. In contrast, developers of collaborative platforms for teaching and research have been struggling for the last 15 to 20 years to gain a wide acceptance among users, and with very limited success. Although simple and intuitive usability has long been recognised as a key design goal, no-one has, apparently, yet succeeded in integrating all the tools and functions required for academic work into a platform with an intuitive interface that target groups make enthusiastic use of. This paper investigates which principles and features may be adopted to make the use of these platforms easier and more intuitive. This electronic document is a "live" template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.},
journal = {SIGCAS Comput. Soc.},
month = dec,
pages = {18–31},
numpages = {14},
keywords = {user experience, cooperative workflows, social games}
}

@inproceedings{10.1145/2107596.2107602,
author = {Cui, Yanqing and Honkala, Mikko},
title = {The Consumption of Integrated Social Networking Services on Mobile Devices},
year = {2011},
isbn = {9781450310963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2107596.2107602},
doi = {10.1145/2107596.2107602},
abstract = {Some mobile devices automatically fetch content from social networking services and integrate it into the device user interface. In this paper, we explore how people consume integrated social networking services in a field study with an innovative aggregator named Linked Internet UI Concept, or LinkedUI. Twenty users completed the field study. We logged their activities and performed user interviews. The study reveals two main use cases: habitual checking where users frequently glanced at the integrated services at short intervals; and serendipitous content discovery where they come across some content when performing other tasks. The study also reveals that the users only attended to a small proportion of the full content set, such as content recently published and content from selected contacts. This indicates the user need of quick access to relevant content.},
booktitle = {Proceedings of the 10th International Conference on Mobile and Ubiquitous Multimedia},
pages = {53–62},
numpages = {10},
keywords = {notification, mobile, integration, social networking services, aggregation, social network aggregator},
location = {Beijing, China},
series = {MUM '11}
}

@inproceedings{10.5555/2431518.2431980,
author = {Lee, Wee-Leong},
title = {Spreadsheet Based Experiential Learning Environment for Project Management},
year = {2011},
publisher = {Winter Simulation Conference},
abstract = {Research has demonstrated that people learn best when they are actively involved in the learning process. Games and simulations are especially effective as discovery learning approaches since they pull learners into the learning experience in interesting, fun and challenging ways. This article seeks to demonstrate the effective use of simulation and gaming technique in providing an engaging and high-energy approach to teaching the concepts and best practices of project management that will have practical and lasting value. The project management game described here provides a means of immersing people in situations that mimic the complexities of the real world, challenging them to take risks and make mistakes without real consequences.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3882–3892},
numpages = {11},
location = {Phoenix, Arizona},
series = {WSC '11}
}

@inproceedings{10.5555/2431518.2431653,
author = {Fackler, James and Spaeder, Michael},
title = {Why Doesn't Healthcare Embrace Simulation and Modeling? What Would It Take?},
year = {2011},
publisher = {Winter Simulation Conference},
abstract = {Physicians do modeling -- every day, all day. It's just that it's done with hideous imprecision making cross-patient conclusions hazardous and extensibility impossible. Most of these mental models are devoid of formal logic. Rather, these mental models are patterns matched in a specific patient with a specific problem(s) based on a clinician's experience and "book-knowledge". We will explore some of the steps that could contribute to the broader acceptance of mathematical models in health care. We will distinguish models that impact the care of the individual patient from that of a larger population.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1137–1142},
numpages = {6},
location = {Phoenix, Arizona},
series = {WSC '11}
}

@inproceedings{10.5555/2431518.2431649,
author = {Bigus, Joseph P. and Chen-Ritzo, Ching-Hua and Sorrentino, Robert},
title = {A Framework for Evidence-Based Health Care Incentives Simulation},
year = {2011},
publisher = {Winter Simulation Conference},
abstract = {We present a general simulation framework designed for modeling incentives in a health care delivery system. This first version of the framework focuses on representing provider incentives. Key framework components are described in detail, and we provide an overview of how data-driven analytic methods can be integrated with this framework to enable evidence-based simulation. The software implementation of a simple simulation model based on this framework is also presented.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1103–1116},
numpages = {14},
location = {Phoenix, Arizona},
series = {WSC '11}
}

@inproceedings{10.5555/2431518.2431663,
author = {Miller, Martin J. and Shahi, Niloo and Dias, Ashley N.},
title = {Improving Simulation Results with Static Models},
year = {2011},
publisher = {Winter Simulation Conference},
abstract = {Effective simulation models require robust development methodologies. Planning, design, data, and testing are integral to ensure valuable answers to the model's customers. This paper discusses how supporting static models provide guidelines and directional correctness to simulation models. Static models can also provide supplemental answers which allow the reduction in simulation model complexity.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1223–1230},
numpages = {8},
location = {Phoenix, Arizona},
series = {WSC '11}
}

@inproceedings{10.1145/2077434.2077437,
author = {Marcus, Aaron},
title = {Cross-Cultural User-Experience Design},
year = {2011},
isbn = {9781450311359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2077434.2077437},
doi = {10.1145/2077434.2077437},
abstract = {User interfaces for desktop, Web, mobile, and vehicle platforms extend across culturally diverse user communities, sometimes within a single country or language group, and certainly across the globe. If user interfaces are to be usable, useful, and appealing to such a wide range of users, user-interface/user-experience developers must account for cultural aspects in globalizing/localizing products and services. In this course, participants will learn practical principles and techniques that are immediately useful for both analysis and design tasks. Where time permits, they will have an opportunity to put their understanding into practice through a series of group exercises. Some handout materials are available in Mandarin.},
booktitle = {SIGGRAPH Asia 2011 Courses},
articleno = {3},
numpages = {201},
location = {Hong Kong, China},
series = {SA '11}
}

@inproceedings{10.1145/2024156.2024162,
author = {Fiss, Juliet and Agarwala, Aseem and Curless, Brian},
title = {Candid Portrait Selection from Video},
year = {2011},
isbn = {9781450308076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024156.2024162},
doi = {10.1145/2024156.2024162},
abstract = {In this paper, we train a computer to select still frames from video that work well as candid portraits. Because of the subjective nature of this task, we conduct a human subjects study to collect ratings of video frames across multiple videos. Then, we compute a number of features and train a model to predict the average rating of a video frame. We evaluate our model with cross-validation, and show that it is better able to select quality still frames than previous techniques, such as simply omitting frames that contain blinking or motion blur, or selecting only smiles. We also evaluate our technique qualitatively on videos that were not part of our validation set, and were taken outdoors and under different lighting conditions.},
booktitle = {Proceedings of the 2011 SIGGRAPH Asia Conference},
articleno = {128},
numpages = {8},
location = {Hong Kong, China},
series = {SA '11}
}

@article{10.1145/2070781.2024162,
author = {Fiss, Juliet and Agarwala, Aseem and Curless, Brian},
title = {Candid Portrait Selection from Video},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2070781.2024162},
doi = {10.1145/2070781.2024162},
abstract = {In this paper, we train a computer to select still frames from video that work well as candid portraits. Because of the subjective nature of this task, we conduct a human subjects study to collect ratings of video frames across multiple videos. Then, we compute a number of features and train a model to predict the average rating of a video frame. We evaluate our model with cross-validation, and show that it is better able to select quality still frames than previous techniques, such as simply omitting frames that contain blinking or motion blur, or selecting only smiles. We also evaluate our technique qualitatively on videos that were not part of our validation set, and were taken outdoors and under different lighting conditions.},
journal = {ACM Trans. Graph.},
month = dec,
pages = {1–8},
numpages = {8}
}

@inproceedings{10.1145/2185216.2185318,
author = {Sarkar, Bidyut Biman and Sanyal, Sugata and Chaki, Nabendu},
title = {Distributed Framework for Tele Health Monitoring System},
year = {2011},
isbn = {9781450310116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2185216.2185318},
doi = {10.1145/2185216.2185318},
abstract = {In recent years, it has been a challenge to design and implement remote health care and monitoring services, with an objective to extend instant cost effective treatment and supply of life saving drugs. There are specialized disciplines in medical sciences for consultation, services, and surgery. Tele medicine is one such area of service through which the aged, rural residents and dwellers of remote hilly and highland areas can be benefited. Often, people in these areas suffer from disease like Obstructive Sleep Apnea (OSA), hypertension, cardiovascular problems, obesity, diabetes etc., or respiratory diseases like asthma, pulmonology related to lungs and so on with a high risk of mortality. In this paper, we propose a Petri Net based framework for analysis and integration of tele-health monitoring service systems over wireless and hybrid networks.},
booktitle = {Proceedings of the 1st International Conference on Wireless Technologies for Humanitarian Relief},
pages = {385–390},
numpages = {6},
keywords = {SOA, sensors, MANET, automation, tele medicine, OSA, Petri net},
location = {Amritapuri, Kollam, Kerala, India},
series = {ACWR '11}
}

@inproceedings{10.1145/2185216.2185282,
author = {Yoneki, Eiko and Crowcroft, Jon},
title = {EpiMap: Towards Quantifying Contact Networks and Modelling the Spread of Infections in Developing Countries},
year = {2011},
isbn = {9781450310116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2185216.2185282},
doi = {10.1145/2185216.2185282},
abstract = {We describe the EpiMap project, in which mobile phones and sensors record the proximity of other devices, to gather information on human interactions within the rural communities of developing countries. Collected information will be used to develop improved mathematical models of the spread of infectious diseases, such as measles, tuberculosis and pneumococcal diseases. Modelling will be complemented by the use of surveys to aid in the understanding of living conditions in these villages. EpiMap is an extension of the FluPhone project, which we carried out in 2010. FluPhone collected data on human interaction, by using mobile phones to record information such as locality and user symptoms. Delay tolerant opportunistic networks such as the Haggle framework [5] were used as a basis for communication. We introduce the EpiMap vision for a system of opportunistic networks combined with satellite communication, designed to face the challenges posed by weak power and communications infrastructure in the rural regions of developing countries in Asia, Africa and South America. We aim to use a delay-tolerant small satellite for data transfer between developing countries and Europe and North America. Data collected through EpiMap can be used to help design more efficient vaccination strategies and equitable control programmes.},
booktitle = {Proceedings of the 1st International Conference on Wireless Technologies for Humanitarian Relief},
pages = {233–240},
numpages = {8},
keywords = {contact networks, epidemiology, small satellite},
location = {Amritapuri, Kollam, Kerala, India},
series = {ACWR '11}
}

@article{10.1145/2043652.2043660,
author = {Goncalves, Romulo and Kersten, Martin},
title = {The Data Cyclotron Query Processing Scheme},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/2043652.2043660},
doi = {10.1145/2043652.2043660},
abstract = {A grand challenge of distributed query processing is to devise a self-organizing architecture which exploits all hardware resources optimally to manage the database hot set, minimize query response time, and maximize throughput without single point global coordination. The Data Cyclotron architecture [Goncalves and Kersten 2010] addresses this challenge using turbulent data movement through a storage ring built from distributed main memory and capitalizing on the functionality offered by modern remote-DMA network facilities. Queries assigned to individual nodes interact with the storage ring by picking up data fragments, which are continuously flowing around, that is, the hot set.The storage ring is steered by the Level Of Interest (LOI) attached to each data fragment, which represents the cumulative query interest as it passes around the ring multiple times. A fragment with LOI below a given threshold, inversely proportional to the ring load, is pulled out to free up resources. This threshold is dynamically adjusted in a fully distributed manner based on ring characteristics and locally observed query behavior. It optimizes resource utilization by keeping the average data access latency low. The approach is illustrated using an extensive and validated simulation study. The results underpin the fragment hot set management robustness in turbulent workload scenarios.A fully functional prototype of the proposed architecture has been implemented using modest extensions to MonetDB and runs within a multirack cluster equipped with Infiniband. Extensive experimentation using both microbenchmarks and high-volume workloads based on TPC-H demonstrates its feasibility.The Data Cyclotron architecture and experiments open a new vista for modern distributed database architectures with a plethora of new research challenges.},
journal = {ACM Trans. Database Syst.},
month = dec,
articleno = {27},
numpages = {35},
keywords = {column-stores, Infiniband, Throughput}
}

@article{10.1145/2063231.2063237,
author = {Toups, Zachary O. and Kerne, Andruid and Hamilton, William A.},
title = {The Team Coordination Game: Zero-Fidelity Simulation Abstracted from Fire Emergency Response Practice},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/2063231.2063237},
doi = {10.1145/2063231.2063237},
abstract = {Crisis response engenders a high-stress environment in which teams gather, transform, and mutually share information. Prior educational approaches have not successfully addressed these critical skills. The assumption has been that the highest fidelity simulations result in the best learning. Deploying high-fidelity simulations is expensive and dangerous; they do not address team coordination. Low-fidelity approaches are ineffective because they are not stressful.Zero-fidelity simulation develops and invokes the principle of abstraction, focusing on human-information and human-human transfers of meaning, to derive design from work practice. Our principal hypothesis is that crisis responders will experience zero-fidelity simulation as effective simulation of team coordination. We synthesize the sustained iterative design and evaluation of the Team Coordination Game. We develop and apply new experimental methods to show that participants learn to cooperate and communicate, applying what they learn in practice. Design implications address how to employ the abstraction principle to develop zero-fidelity simulations.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {23},
numpages = {37},
keywords = {Zero-fidelity simulation, game interface design, information distribution, education games}
}

@article{10.1145/2071356.2071363,
author = {Ioannides, Charalambos and Eder, Kerstin I.},
title = {Coverage-Directed Test Generation Automated by Machine Learning -- A Review},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/2071356.2071363},
doi = {10.1145/2071356.2071363},
abstract = {The increasing complexity and size of digital designs, in conjunction with the lack of a potent verification methodology that can effectively cope with this trend, continue to inspire engineers and academics in seeking ways to further automate design verification. In an effort to increase performance and to decrease engineering effort, research has turned to artificial intelligence (AI) techniques for effective solutions. The generation of tests for simulation-based verification can be guided by machine-learning techniques. In fact, recent advances demonstrate that embedding machine-learning (ML) techniques into a coverage-directed test generation (CDG) framework can effectively automate the test generation process, making it more effective and less error-prone. This article reviews some of the most promising approaches in this field, aiming to evaluate the approaches and to further stimulate more directed research in this area.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jan,
articleno = {7},
numpages = {21},
keywords = {Markov models, genetic algorithms, genetic programming, Coverage-directed test generation (CDG), Bayesian networks, inductive logic programming (ILP)}
}

@article{10.1145/2073242.2073243,
author = {Ortega, Felipe},
title = {WikiSym 2011 in Mountain View, California},
year = {2012},
issue_date = {Winter 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
number = {Winter},
issn = {1931-1745},
url = {https://doi.org/10.1145/2073242.2073243},
doi = {10.1145/2073242.2073243},
abstract = {This report presents a summary of WikiSym 2011, the 7th International Symposium on Wikis and Open Collaboration that was held in Mount View (CA, USA) on October 3-5, 2011. The symposium was hosted by Microsoft Research at their Silicon Valley Campus. Andrea Forte, from Drexel University, was the Program Chair and I took the role of General Chair. As in other previous editions, WikiSym 2011 was the meeting point for researchers, practitiones, entrepreneurs and enthusiasts from the whole world interested in open collaboration and its related technologies. The 2011 program included a packed agenda of keynotes, presentations, panels and workshops over the 3 main conference days. The Open Space, and unconference track in WikiSym, promoted further personal interaction among participants with a free agenda of face-to-face meetings set up and conducted by attendees. This report includes a conference review, mainly from my personal notes and first-hand impressions over these days.},
journal = {SIGWEB Newsl.},
month = jan,
articleno = {1},
numpages = {6}
}

@article{10.14778/2140436.2140444,
author = {Schnaitter, Karl and Polyzotis, Neoklis},
title = {Semi-Automatic Index Tuning: Keeping DBAs in the Loop},
year = {2012},
issue_date = {January 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/2140436.2140444},
doi = {10.14778/2140436.2140444},
abstract = {To obtain a high level of system performance, a database administrator (DBA) must choose a set of indices that is appropriate for the workload. The system can aid in this challenging task by providing recommendations for the index configuration. We propose a new index recommendation technique, termed semi-automatic tuning, that keeps the DBA "in the loop" by generating recommendations that use feedback about the DBA's preferences. The technique also works online, which avoids the limitations of commercial tools that require the workload to be known in advance. The foundation of our approach is the Work Function Algorithm, which can solve a wide variety of online optimization problems with strong competitive guarantees. We present an experimental analysis that validates the benefits of semi-automatic tuning in a wide variety of conditions.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {478–489},
numpages = {12}
}

@inproceedings{10.1145/2090236.2090255,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090255},
doi = {10.1145/2090236.2090255},
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@article{10.1145/2094114.2094124,
author = {Kersten, Martin and Manegold, Stefan and Mullender, Sjoerd},
title = {The Database Architectures Research Group at CWI},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2094114.2094124},
doi = {10.1145/2094114.2094124},
journal = {SIGMOD Rec.},
month = jan,
pages = {39–44},
numpages = {6}
}

@article{10.1145/2070719.2070726,
author = {Yu, Chen and Schermerhorn, Paul and Scheutz, Matthias},
title = {Adaptive Eye Gaze Patterns in Interactions with Human and Artificial Agents},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2070719.2070726},
doi = {10.1145/2070719.2070726},
abstract = {Efficient collaborations between interacting agents, be they humans, virtual or embodied agents, require mutual recognition of the goal, appropriate sequencing and coordination of each agent's behavior with others, and making predictions from and about the likely behavior of others. Moment-by-moment eye gaze plays an important role in such interaction and collaboration. In light of this, we used a novel experimental paradigm to systematically investigate gaze patterns in both human-human and human-agent interactions. Participants in the study were asked to interact with either another human or an embodied agent in a joint attention task. Fine-grained multimodal behavioral data were recorded including eye movement data, speech, first-person view video, which were then analyzed to discover various behavioral patterns. Those patterns show that human participants are highly sensitive to momentary multimodal behaviors generated by the social partner (either another human or an artificial agent) and they rapidly adapt their gaze behaviors accordingly. Our results from this data-driven approach provide new findings for understanding micro-behaviors in human-human communication which will be critical for the design of artificial agents that can generate human-like gaze behaviors and engage in multimodal interactions with humans.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jan,
articleno = {13},
numpages = {25},
keywords = {human-robot interaction, Multimodal interface, gaze-based interaction}
}

@article{10.1145/2096140.2096146,
author = {Hung, Shin-Yuan and Chang, Chia-Ming and Chen, Kuanchiun and Tang, King-Zoo and Chou, Chin-Hua},
title = {Buyer Acceptance of G2b E-Government Services: An Empirical Study of Inter-Entity Supply Contracts},
year = {2012},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/2096140.2096146},
doi = {10.1145/2096140.2096146},
abstract = {The traditional focus of e-government services research has been on individuals rather than institutional users; however, buyer-accepted G2B e-government services are critical for an effective e-government services market. The purpose of this study is to identify the factors that determine buyer acceptance of G2B e-government services in the context of inter-entity supply contracts. Based primarily on an adapted version of the Theory of Planned Behavior (TPB), a sample of 185 organizational buyers of real G2B e-government services in Taiwan was examined. The findings show that perceived usefulness, perceived risk, external influence, interpersonal influence, self-efficacy, and facilitating conditions are critical factors in determining buyer acceptance. The resulting implications and recommendations for G2B e-government services research and practice are also discussed.},
journal = {SIGMIS Database},
month = jan,
pages = {81–97},
numpages = {17},
keywords = {information technology acceptance, g2b e-government services, theory of planned behavior}
}

@article{10.1145/2070719.2070725,
author = {Mutlu, Bilge and Kanda, Takayuki and Forlizzi, Jodi and Hodgins, Jessica and Ishiguro, Hiroshi},
title = {Conversational Gaze Mechanisms for Humanlike Robots},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2070719.2070725},
doi = {10.1145/2070719.2070725},
abstract = {During conversations, speakers employ a number of verbal and nonverbal mechanisms to establish who participates in the conversation, when, and in what capacity. Gaze cues and mechanisms are particularly instrumental in establishing the participant roles of interlocutors, managing speaker turns, and signaling discourse structure. If humanlike robots are to have fluent conversations with people, they will need to use these gaze mechanisms effectively. The current work investigates people's use of key conversational gaze mechanisms, how they might be designed for and implemented in humanlike robots, and whether these signals effectively shape human-robot conversations. We focus particularly on whether humanlike gaze mechanisms might help robots signal different participant roles, manage turn-exchanges, and shape how interlocutors perceive the robot and the conversation. The evaluation of these mechanisms involved 36 trials of three-party human-robot conversations. In these trials, the robot used gaze mechanisms to signal to its conversational partners their roles either of two addressees, an addressee and a bystander, or an addressee and a nonparticipant. Results showed that participants conformed to these intended roles 97% of the time. Their conversational roles affected their rapport with the robot, feelings of groupness with their conversational partners, and attention to the task.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jan,
articleno = {12},
numpages = {33},
keywords = {gaze, conversational roles, humanlike robots, turn-taking, Human-robot interaction, discourse structure, conversation}
}

@inproceedings{10.1145/2110363.2110372,
author = {Ben Abacha, Asma and Zweigenbaum, Pierre},
title = {Medical Question Answering: Translating Medical Questions into Sparql Queries},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110372},
doi = {10.1145/2110363.2110372},
abstract = {Designing question answering systems requires efficient and deep analysis of natural language questions. A key process for this task is to translate the semantic relations expressed in the question into a machine-readable representation.In this paper we tackle question analysis in the medical field. More precisely, we study how to translate a natural language question into a machine-readable representation. The underlying transformation process requires determining three key points: (i) What are the main characteristics of medical questions? (ii) Which methods are the most fitted for the extraction of these characteristics? and (iii) how to translate the extracted information into a machine-understandable representation?We present a complete question analysis approach including medical entity recognition, semantic relation extraction and automatic translation to SPARQL queries. Our study supports the fact that SPARQL can represent a wide range of natural language questions in a question-answering perspective. Experiments on a corpus of real questions show that we obtain encouraging results in medical entity recognition and relation extraction. The obtained results also show that the output SPARQL queries correctly represent more than 60% of the original questions.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {41–50},
numpages = {10},
keywords = {sparql, medical question analysis, machine learning, rdf, information extraction, question answering},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.1145/2110363.2110375,
author = {Borazio, Marko and Van Laerhoven, Kristof},
title = {Combining Wearable and Environmental Sensing into an Unobtrusive Tool for Long-Term Sleep Studies},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110375},
doi = {10.1145/2110363.2110375},
abstract = {Long-term sleep monitoring of patients has been identified as a useful tool to observe sleep trends manifest themselves over weeks or months for use in behavioral studies. In practice, this has been limited to coarse-grained methods such as actigraphy, for which the levels of activity are logged, and which provide some insight but have simultaneously been found to lack accuracy to be used for studying sleeping disorders. This paper presents a method to automatically detect the user's sleep at home on a long-term basis. Inertial, ambient light, and time data tracked from a wrist-worn sensor, and additional night vision footage is used for later expert inspection. An evaluation on over 4400 hours of data from a focus group of test subjects demonstrates a high re-call night segment detection, obtaining an average of 94%. Further, a clustering to visualize reoccurring sleep patterns is presented, and a myoclonic twitch detection is introduced, which exhibits a precision of 74%. The results indicate that long-term sleep pattern detections are feasible.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {71–80},
numpages = {10},
keywords = {sleep detection, long-term monitoring, activity recognition},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.1145/2110363.2110423,
author = {Stevens, Nicholas and Giannareas, Ana Rosa and Kern, Vanessa and Viesca, Adrian and Fortino-Mullen, Margaret and King, Andrew and Lee, Insup},
title = {Smart Alarms: Multivariate Medical Alarm Integration for Post CABG Surgery Patients},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110423},
doi = {10.1145/2110363.2110423},
abstract = {In order to monitor patients in the Intensive Care Unit, healthcare practitioners set threshold alarms on each of many individual vital sign monitors. The current alarm algorithms elicit numerous false positive alarms producing an inefficient healthcare system, where nurses habitually ignore low level alarms due to their overabundance.In this paper, we describe an algorithm that considers multiple vital signs when monitoring a post coronary artery bypass graft (post-CABG) surgery patient. The algorithm employs a Fuzzy Expert System to mimic the decision processes of nurses. In addition, it includes a Clinical Decision Support tool that uses Bayesian theory to display the possible CABG-related complications the patient might be undergoing at any point in time, as well as the most relevant risk factors. As a result, this multivariate approach decreases clinical alarms by an average of 59% with a standard deviation of 17% (Sample of 32 patients, 1,451 hours of vital sign data). Interviews comparing our proposed system with the approach currently used in hospitals have also confirmed the potential efficiency gains from this approach.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {533–542},
numpages = {10},
keywords = {vital sign monitor, fuzzy logic, bayesian theory, clinical data integration, clinical decision support},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.1145/2110363.2110420,
author = {Seeger, Christian and Buchmann, Alejandro and Van Laerhoven, Kristof},
title = {An Event-Based BSN Middleware That Supports Seamless Switching between Sensor Configurations},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110420},
doi = {10.1145/2110363.2110420},
abstract = {Recent advances in wearable sensors have surged in novel fitness and preventive health care systems that measure step counts, activity levels, and performed exercises with inertial sensors, enabling users to monitor condition and day-to-day lifestyle. This paper presents a middleware designed for a smartphone unit to support health monitoring applications. Its event-driven architecture enables modular system design and seamless switching between sets of embedded sensors. The strengths of the middleware are highlighted in a deployed feasibility study where daily and gym activities are recognized through an interchangeable set of wireless sensors. The study demonstrates that the setup is suitable for daily usage with minimal impact on the phone's resources.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {503–512},
numpages = {10},
keywords = {fitness application, middleware, smartphone, body sensor networks, event-based},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.1145/2110363.2110411,
author = {Mason-Blakley, Fieran and Weber, Jens},
title = {CIS System Hazards Derived from Literature Using Systems and Human Factors Perspectives},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110411},
doi = {10.1145/2110363.2110411},
abstract = {The FDA and other national regulatory agencies have expressed their intentions to begin enforcement of medical device regulations on Health Informatics Technology (HIT) vendors. A mechanism which might be employed to achieve this enforcement in the US is the Quality Systems Regulations (QSR), while similar legislation might be employed elsewhere. In order for vendors to achieve conformance with QSR regulations, they must first identify hazards which their products may pose. In order to identify these hazards we have undertaken a literature review from which we have extracted taxonomies of Clinical Informatics Systems (CIS) devices, systems and hazards. We present these three taxonomies, and a discussion of contemporary risk classfication strategies which are being applied to HIT. The taxonomies which have been developed provide actionable hazards for HIT vendors, and provide a potential basis for best practices in the engineering of HIT systems.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {419–428},
numpages = {10},
keywords = {hazard, emr, system safety},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.1145/2110363.2110393,
author = {Hansen, Derek L. and Johnson, Christianne},
title = {Veiled Viral Marketing: Disseminating Information on Stigmatized Illnesses via Social Networking Sites},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110393},
doi = {10.1145/2110363.2110393},
abstract = {Social networking sites such as Facebook and MySpace offer new ways of encouraging healthy behavior by leveraging existing social relationships, particularly among young adults. However, spreading information through social networks can be sensitive and problematic when the content relates to stigmatized illnesses. This paper proposes a novel strategy, Veiled Viral Marketing, which uses social networking sites to anonymously disseminate information about stigmatized illnesses to social networking friends. We describe the successes and failures of this approach based on an empirical study of a prototype Facebook App called FactCheck: HPV which was downloaded by over 1,022 users. We find interest in sending "veiled" invitations, though not as much interest as sending invitations from a known friend (1:2 ratio). However, we find that users are 5 times more likely to download the app if invited from a "veiled" friend compared to a known friend. We conclude with design suggestions related to Veiled Viral Marketing and dissemination of information on stigmatized illnesses more generally.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {247–254},
numpages = {8},
keywords = {sexually transmitted disease, human papillomavirus, hpv, facebook, social networking sites, health promotion, stigma, dissemination},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.5555/2523712.2523719,
author = {Perimal-Lewis, Lua and Qin, Shaowen and Thompson, Campbell and Hakendorf, Paul},
title = {Gaining Insight from Patient Journey Data Using a Process-Oriented Analysis Approach},
year = {2012},
isbn = {9781921770104},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Hospitals are continually struggling to cater for the increasing demand for inpatient services. This is due to increased population, aging, and the rising incidence of chronic diseases associated with modern life. The high demand for hospital services leads to unpredictable bed availability, longer waiting period for acute admission, difficulties in keeping planned admission, stressed hospital staff, undesirable patient and family experience, as well as unclear impact on the quality of care patients receive. This study aims to gain insight into patient journey data to identify problems that could cause access block. Process mining techniques combined with statistical data analysis are adapted to discover inpatient flow process patterns and their correlation with patient types, ward types, waiting time and Length of Stay (LOS). Open source process mining software, ProM, is used in this study. The study is done in collaboration with Flinders Medical Centre (FMC) using data from their Patient Journey Database.},
booktitle = {Proceedings of the Fifth Australasian Workshop on Health Informatics and Knowledge Management - Volume 129},
pages = {59–66},
numpages = {8},
keywords = {process mining, inpatient journey analysis, length of stay, patient flow},
location = {Melbourne, Australia},
series = {HIKM '12}
}

@inproceedings{10.1145/2133601.2133613,
author = {Bauer, Lujo and Liang, Yuan and Reiter, Michael K. and Spensky, Chad},
title = {Discovering Access-Control Misconfigurations: New Approaches and Evaluation Methodologies},
year = {2012},
isbn = {9781450310918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2133601.2133613},
doi = {10.1145/2133601.2133613},
abstract = {Accesses that are not permitted by implemented policy but that share similarities with accesses that have been allowed, may be indicative of access-control policy misconfigurations. Identifying such misconfigurations allows administrators to resolve them before they interfere with the use of the system. We improve upon prior work in identifying such misconfigurations in two main ways. First, we develop a new methodology for evaluating misconfiguration prediction algorithms and applying them to real systems. We show that previous evaluations can substantially overestimate the benefits of using such algorithms in practice, owing to their tendency to reward predictions that can be deduced to be redundant. We also show, however, that these and other deductions can be harnessed to substantially recover the benefits of prediction. Second, we propose an approach that significantly simplifies the use of misconfiguration prediction algorithms. We remove the need to hand-tune (and empirically determine the effects of) various parameters, and instead replace them with a single, intuitive tuning parameter. We show empirically that this approach is generally competitive in terms of benefit and accuracy with algorithms that require hand-tuned parameters.},
booktitle = {Proceedings of the Second ACM Conference on Data and Application Security and Privacy},
pages = {95–104},
numpages = {10},
keywords = {access control, misconfiguration, machine learning},
location = {San Antonio, Texas, USA},
series = {CODASPY '12}
}

@inproceedings{10.1145/2132176.2132212,
author = {Daniels, Morgan and Faniel, Ixchel and Fear, Kathleen and Yakel, Elizabeth},
title = {Managing Fixity and Fluidity in Data Repositories},
year = {2012},
isbn = {9781450307826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2132176.2132212},
doi = {10.1145/2132176.2132212},
abstract = {Data repositories walk a fine line between the fixity and fluidity of the data they curate. Change is constant, but too much change affects the integrity of data. This paper examines data transformations in three repositories, serving the zoological, archaeological, and quantitative social science research communities. Based on in-depth analysis of 27 interviews, we identify a typology of changes: adding value; correcting errors; creating consistency; changing representations of data to reflect new knowledge; responding to designated communities; and evolving practices around collecting. Then we discuss the nature of these changes in terms of the data and collections. Our findings indicate that organizational differences and the diverse needs of the repositories' designated communities play a large role in how they manage change.},
booktitle = {Proceedings of the 2012 IConference},
pages = {279–286},
numpages = {8},
keywords = {data reuse, data repositories, data curation, digital preservation},
location = {Toronto, Ontario, Canada},
series = {iConference '12}
}

@inproceedings{10.1145/2133601.2133638,
author = {Gondi, Kalpana and Bisht, Prithvi and Venkatachari, Praveen and Sistla, A. Prasad and Venkatakrishnan, V. N.},
title = {SWIPE: Eager Erasure of Sensitive Data in Large Scale Systems Software},
year = {2012},
isbn = {9781450310918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2133601.2133638},
doi = {10.1145/2133601.2133638},
abstract = {We describe SWIPE, an approach to reduce the life time of sensitive, memory resident data in large scale applications written in C. In contrast to prior approaches that used a delayed or lazy approach to the problem of erasing sensitive data, SWIPE uses a novel eager erasure approach that minimizes the risk of accidental sensitive data leakage. SWIPE achieves this by transforming a legacy C program to include additional instructions that erase sensitive data immediately after its intended use. SWIPE is guided by a highly-scalable static analysis technique that precisely identifies the locations to introduce erase instructions in the original program. The programs transformed using SWIPE enjoy several additional benefits: minimization of leaks that arise due to data dependencies; erasure of sensitive data with minimal developer guidance; and negligible performance overheads.},
booktitle = {Proceedings of the Second ACM Conference on Data and Application Security and Privacy},
pages = {295–306},
numpages = {12},
keywords = {static analysis, security, data lifetime minimization, confidentiality},
location = {San Antonio, Texas, USA},
series = {CODASPY '12}
}

@inproceedings{10.1145/2132176.2132211,
author = {Rosson, Mary Beth and Li, Na and Ryan, Timothy and Tapia, Andrea H.},
title = {Addressing Ownership, Access and Participation Needs in Scientific Collaboration},
year = {2012},
isbn = {9781450307826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2132176.2132211},
doi = {10.1145/2132176.2132211},
abstract = {In this paper we discuss the initial phases of design work that we have carried out to support the collaboration of physical anthropologists who use High-Resolution Computed Tomography (HRCT) data as a key element in their research. Drawing from sociotechnical analyses reported in earlier papers, we highlight design issues that are of particular relevance to the tasks of scanning and sharing HRCT data -- balancing the costs and benefits experienced by the stakeholders who create and share the digital files, and addressing pervasive issues of data ownership and control. We describe the evolutionary approach to technology design that we are pursuing, summarize how we have addressed these key design issues, report feedback from system users, and discuss lessons learned -- for both the continuing development of our system and scientific collaboration systems more generally.},
booktitle = {Proceedings of the 2012 IConference},
pages = {271–278},
numpages = {8},
keywords = {web information systems, scientific collaboration, sociotechnical systems, e-science},
location = {Toronto, Ontario, Canada},
series = {iConference '12}
}

@inproceedings{10.1145/2124295.2124313,
author = {Rendle, Steffen},
title = {Learning Recommender Systems with Adaptive Regularization},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124313},
doi = {10.1145/2124295.2124313},
abstract = {Many factorization models like matrix or tensor factorization have been proposed for the important application of recommender systems. The success of such factorization models depends largely on the choice of good values for the regularization parameters. Without a careful selection they result in poor prediction quality as they either underfit or overfit the data. Regularization values are typically determined by an expensive search that requires learning the model parameters several times: once for each tuple of candidate values for the regularization parameters. In this paper, we present a new method that adapts the regularization automatically while training the model parameters. To achieve this, we optimize simultaneously for two criteria: (1) as usual the model parameters for the regularized objective and (2) the regularization of future parameter updates for the best predictive quality on a validation set. We develop this for the generic model class of Factorization Machines which subsumes a wide variety of factorization models. We show empirically, that the advantages of our adaptive regularization method compared to expensive hyperparameter search do not come to the price of worse predictive quality. In total with our method, learning regularization parameters is as easy as learning model parameters and thus there is no need for any time-consuming search of regularization values because they are found on-the-fly. This makes our method highly attractive for practical use.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {133–142},
numpages = {10},
keywords = {matrix factorization, regularization, tensor factorization},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124308,
author = {Dasgupta, Anirban and Gurevich, Maxim and Zhang, Liang and Tseng, Belle and Thomas, Achint O.},
title = {Overcoming Browser Cookie Churn with Clustering},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124308},
doi = {10.1145/2124295.2124308},
abstract = {Many large Internet websites are accessed by users anonymously, without requiring registration or logging-in. However, to provide personalized service these sites build anonymous, yet persistent, user models based on repeated user visits. Cookies, issued when a web browser first visits a site, are typically employed to anonymously associate a website visit with a distinct user (web browser). However, users may reset cookies, making such association short-lived and noisy. In this paper we propose a solution to the cookie churn problem: a novel algorithm for grouping similar cookies into clusters that are more persistent than individual cookies. Such clustering could potentially allow more robust estimation of the number of unique visitors of the site over a certain long time period, and also better user modeling which is key to plenty of web applications such as advertising and recommender systems.We present a novel method to cluster browser cookies into groups that are likely to belong to the same browser based on a statistical model of browser visitation patterns. We address each step of the clustering as a binary classification problem estimating the probability that two different subsets of cookies belong to the same browser. We observe that our clustering problem is a generalized interval graph coloring problem, and propose a greedy heuristic algorithm for solving it. The scalability of this method allows us to cluster hundreds of millions of browser cookies and provides significant improvements over baselines such as constrained K-means.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {83–92},
numpages = {10},
keywords = {browser cookie churn, similarity measure, distributed computing, bayes factor, clustering algorithms},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2124295.2124312,
author = {Ahmed, Amr and Aly, Moahmed and Gonzalez, Joseph and Narayanamurthy, Shravan and Smola, Alexander J.},
title = {Scalable Inference in Latent Variable Models},
year = {2012},
isbn = {9781450307475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2124295.2124312},
doi = {10.1145/2124295.2124312},
abstract = {Latent variable techniques are pivotal in tasks ranging from predicting user click patterns and targeting ads to organizing the news and managing user generated content. Latent variable techniques like topic modeling, clustering, and subspace estimation provide substantial insight into the latent structure of complex data with little or no external guidance making them ideal for reasoning about large-scale, rapidly evolving datasets. Unfortunately, due to the data dependencies and global state introduced by latent variables and the iterative nature of latent variable inference, latent-variable techniques are often prohibitively expensive to apply to large-scale, streaming datasets.In this paper we present a scalable parallel framework for efficient inference in latent variable models over streaming web-scale data. Our framework addresses three key challenges: 1) synchronizing the global state which includes global latent variables (e.g., cluster centers and dictionaries); 2) efficiently storing and retrieving the large local state which includes the data-points and their corresponding latent variables (e.g., cluster membership); and 3) sequentially incorporating streaming data (e.g., the news). We address these challenges by introducing: 1) a novel delta-based aggregation system with a bandwidth-efficient communication protocol; 2) schedule-aware out-of-core storage; and 3) approximate forward sampling to rapidly incorporate new data. We demonstrate state-of-the-art performance of our framework by easily tackling datasets two orders of magnitude larger than those addressed by the current state-of-the-art. Furthermore, we provide an optimized and easily customizable open-source implementation of the framework1.},
booktitle = {Proceedings of the Fifth ACM International Conference on Web Search and Data Mining},
pages = {123–132},
numpages = {10},
keywords = {large-scale systems, latent models, graphical models, inference},
location = {Seattle, Washington, USA},
series = {WSDM '12}
}

@inproceedings{10.1145/2145204.2145319,
author = {Poole, Erika Shehan},
title = {Interacting with Infrastructure: A Case for Breaching Experiments in Home Computing Research},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145319},
doi = {10.1145/2145204.2145319},
abstract = {Why do user experience problems with home computing persist, despite several decades worth of academic study and countless technological innovations to overcome these issues? This paper presents the results of a multi-week trial investigating technical support practices in North American homes using a combination of breaching experiments and custom software. What this study uncovered was not a one-size-fits-all solution to technical problems in residential settings, but instead a rich description of the articulation work required to acquire devices, maintain and configure them over time, and seek help when problems occur. Based on this study, I argue that many of the user experience problems experienced with home computing and electronics are due to issues related to individual agency rather than technical or user interface characteristics of any given technology combination. Additionally, I make a case for the use of breaching experiments to study phenomenon related to technologies infrastructures that are difficult to capture via other traditionally used methods.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {759–768},
numpages = {10},
keywords = {breaching experiment, home, computing, research methods},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145346,
author = {Quercia, Daniele and Lambiotte, Renaud and Stillwell, David and Kosinski, Michal and Crowcroft, Jon},
title = {The Personality of Popular Facebook Users},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145346},
doi = {10.1145/2145204.2145346},
abstract = {We study the relationship between Facebook popularity (number of contacts) and personality traits on a large number of subjects. We test to which extent two prevalent viewpoints hold. That is, popular users (those with many social contacts) are the ones whose personality traits either predict many offline (real world) friends or predict propensity to maintain superficial relationships. We find that the predictor for number of friends in the real world (Extraversion) is also a predictor for number of Facebook contacts. We then test whether people who have many social contacts on Facebook are the ones who are able to adapt themselves to new forms of communication, present themselves in likable ways, and have propensity to maintain superficial relationships. We show that there is no statistical evidence to support such a conjecture.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {955–964},
numpages = {10},
keywords = {personality, web 2.0, social networks},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145309,
author = {Follmer, Sean and Ballagas, Rafael (Tico) and Raffle, Hayes and Spasojevic, Mirjana and Ishii, Hiroshi},
title = {People in Books: Using a FlashCam to Become Part of an Interactive Book for Connected Reading},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145309},
doi = {10.1145/2145204.2145309},
abstract = {We introduce People in Books with FlashCam technology, a system that supports children and long-distance family members to act as characters in children's storybooks while they read stories together over a distance. By segmenting the video chat streams of the child and remote family member from their background surroundings, we create the illusion that the child and adult reader are immersed among the storybook illustrations. The illusion of inhabiting a shared story environment helps remote family members feel a sense of togetherness and encourages active reading behaviors for children ages three to five. People In Books is designed to fit into families' traditional reading practices, such as reading ebooks on couches or in bed via netbook or tablet computers. To accommodate this goal we implemented FlashCam, a computationally cost effective and physically small background subtraction system for mobile devices that allows users to move locations and change lighting conditions while they engage in background-subtracted video communications. A lab evaluation compared People in Books with a conventional remote reading application. Results show that People in Books motivates parents and children to be more performative readers and encourages open-ended play beyond the story, while creating a strong sense of togetherness.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {685–694},
numpages = {10},
keywords = {family communication, background subtraction, reading, video conferencing, children's literacy},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145215,
author = {Mark, Gloria and Bagdouri, Mossaab and Palen, Leysia and Martin, James and Al-Ani, Ban and Anderson, Kenneth},
title = {Blogs as a Collective War Diary},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145215},
doi = {10.1145/2145204.2145215},
abstract = {Disaster-related research in human-centered computing has typically focused on the shorter-term, emergency period of a disaster event, whereas effects of some crises are long-term, lasting years. Social media archived on the Internet provides researchers the opportunity to examine societal reactions to a disaster over time. In this paper we examine how blogs written during a protracted conflict might reflect a collective view of the event. The sheer amount of data originating from the Internet about a significant event poses a challenge to researchers; we employ topic modeling and pronoun analysis as methods to analyze such large-scale data. First, we discovered that blog war topics temporally tracked the actual, measurable violence in the society suggesting that blog content can be an indicator of the health or state of the affected population. We also found that people exhibited a collective identity when they blogged about war, as evidenced by a higher use of first-person plural pronouns compared to blogging on other topics. Blogging about daily life decreased as violence in the society increased; when violence waned, there was a resurgence of daily life topics, potentially illustrating how a society returns to normalcy.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {37–46},
numpages = {10},
keywords = {topic modeling, longitudinal study, crisis informatics, collective identity, crisis, blogs, war},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145336,
author = {Pine, Katie},
title = {Fragmentation and Choreography: Caring for a Patient and a Chart during Childbirth},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145336},
doi = {10.1145/2145204.2145336},
abstract = {CSCW has long been concerned with how work is coordinated. A rich body of literature examines the mechanisms underlying cooperative work and the articulation of discrete tasks into meaningful sequences of action. However, there is less treatment of how workers balance multiple streams of work at once. In hospitals, the introduction of Health Information Technologies coupled with increased requirements for documentation means that workers must simultaneously care for and integrate two work trajectories: that related to the patient and that related to the medical record. Using data from an ethnographic study of labor &amp; delivery nurses in a mid-size hospital, I describe the situated, embodied, and effortful work of coordinating multiple streams of action into a single coherent performance of work, a process I refer to as choreography, and present a number of choreography practices. I then describe implications of this perspective for CSCW.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {887–896},
numpages = {10},
keywords = {documentation, hit, choreography., coordination},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145376,
author = {Oleksik, Gerard and Milic-Frayling, Natasa and Jones, Rachel},
title = {Beyond Data Sharing: Artifact Ecology of a Collaborative Nanophotonics Research Centre},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145376},
doi = {10.1145/2145204.2145376},
abstract = {Scientific communities have long been concerned with the design and implementation of effective infrastructures for data access and collaborative scientific work. Recent studies have shown an increase in collaborative data generation and reuse. However, further improvements require a deeper understanding of the social and technological circumstances under which they emerge. To that effect we conduct in-situ observation study of a Nano-photonics Research Centre. We consider the artifact ecology that evolved from the Centre's common experimentation and data platform, the scientific practices, and the intricate interactions with digital artifacts that arise from the researchers' activities. We uncover the use of progress summaries for collaborative data interpretation and knowledge sharing. By studying this reputable collaborative scientific environment we (1) identified the factors that led to its functional and effective artifact ecology and (2) propose expansions of tools and services to improve it further. The latter include effective support for contextual search, browsing, and flexible viewing of information artifacts based on relevant parameters and properties.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1165–1174},
numpages = {10},
keywords = {collaborative work, data sharing, information management, escience, artifact ecology},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145222,
author = {Wyche, Susan P. and Grinter, Rebecca E.},
title = {"This is How We Do It in My Country": A Study of Computer-Mediated Family Communication among Kenyan Migrants in the United States},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145222},
doi = {10.1145/2145204.2145222},
abstract = {Although computer-mediated family communication remains a longstanding focus of study in Computer Supported Cooperative Work (CSCW), families who face challenges in communication due to differences in technology infrastructures remain understudied. To address this gap in the literature, we interviewed 39 Kenyan migrants living in Atlanta, Georgia, U.S. who regularly communicate with friends and family members living in their homeland. The contributions of this work are primarily empirical. Findings from our study reveal how high costs, identity management, and infrastructural differences between rural and urban areas in Kenya, impact decisions families and their extended members make when using information and communication technology (ICT). We present design implications and reflect on how understanding Kenyan migrants' ICT practices can positively influence design for the broader population.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {87–96},
numpages = {10},
keywords = {transnational communication, photosharing, family, connectedness, kenya},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145238,
author = {Rotman, Dana and Preece, Jenny and Hammock, Jen and Procita, Kezee and Hansen, Derek and Parr, Cynthia and Lewis, Darcy and Jacobs, David},
title = {Dynamic Changes in Motivation in Collaborative Citizen-Science Projects},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145238},
doi = {10.1145/2145204.2145238},
abstract = {Online citizen science projects engage volunteers in collecting, analyzing, and curating scientific data. Existing projects have demonstrated the value of using volunteers to collect data, but few projects have reached the full collaborative potential of scientists and volunteers. Understanding the shared and unique motivations of these two groups can help designers establish the technical and social infrastructures needed to promote effective partnerships. We present findings from a study of the motivational factors affecting participation in ecological citizen science projects. We show that volunteers are motivated by a complex framework of factors that dynamically change throughout their cycle of work on scientific projects; this motivational framework is strongly affected by personal interests as well as external factors such as attribution and acknowledgment. Identifying the pivotal points of motivational shift and addressing them in the design of citizen-science systems will facilitate improved collaboration between scientists and volunteers.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {217–226},
numpages = {10},
keywords = {volunteers, citizen science, scientists, crowdsourcing, motivation, collaboration, ecology},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145334,
author = {Lee, Soyoung and Tang, Charlotte and Park, Sun Young and Chen, Yunan},
title = {Loosely Formed Patient Care Teams: Communication Challenges and Technology Design},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145334},
doi = {10.1145/2145204.2145334},
abstract = {We conducted an observational study to investigate nurses' communication behaviors in an Emergency Department (ED). Our observations reveal unique collaboration practices exercised by ED staff, which we term as "loosely formed team collaboration." Specifically, ED patient care teams are dynamically and quickly assembled upon patient arrival, wherein team members engage in interdependent and complex care activities. The responsible care team then disassembles when a patient leaves the ED. The coordination mechanism required for these work practices challenges nurses' communication processes, often increasing breakdown susceptibility. Our analysis of the ED nurses' communication behaviors and use of communication channels highlights the importance of maintaining team awareness and supporting role-based communication. This points to the need for explicit efforts to coordinate tasks and informative interruptions. These findings call for the design of future communication technologies to meet the needs of loosely formed collaborative environments to provide team-based communication, lightweight feedback, and information transparency.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {867–876},
numpages = {10},
keywords = {emergency department, collaborative work., communication behaviors, loosely formed patient care team, nursing work},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2145204.2145330,
author = {Xu, Yan and Poole, Erika Shehan and Miller, Andrew D. and Eiriksdottir, Elsa and Kestranek, Dan and Catrambone, Richard and Mynatt, Elizabeth D.},
title = {This is Not a One-Horse Race: Understanding Player Types in Multiplayer Pervasive Health Games for Youth},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145330},
doi = {10.1145/2145204.2145330},
abstract = {Technology-based interventions for promoting health behavior-change frequently leverage multiplayer game mechanics such as group-based competitions. However, health interventions successful for groups writ large may not always translate to successful behavior change at the individual level. In this paper, we explore the tension between group and individual success, based on an empirical study on a long-term real-world deployment of a pervasive health game for youth. We report five distinctive player types along the dimensions of motivation, behavior, and influence on others. Based on the findings, we provide design suggestions to help game designers integrate group-based mechanisms that maximize intervention effectiveness.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {843–852},
numpages = {10},
keywords = {physical activity, pervasive games, behavior change, adolescents},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2166966.2166979,
author = {Kim, Jihie and Chang, Yu-Han and Cai, Sen and Jain, Siddharth},
title = {PedConnect: An Intelligent Assistant for Teacher Social Networking},
year = {2012},
isbn = {9781450310482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2166966.2166979},
doi = {10.1145/2166966.2166979},
abstract = {Social networking has gained immense traction in many areas, including teaching and learning. Networking sites for teachers aim to facilitate teacher communication and information sharing, but fall short of their potential. In order to support more effective use of online resources and better communication among teachers, we develop a suite of new user modeling and recommendation capabilities within a middle school teacher networking site. We foster collaboration among novice and experienced teachers when they share similar interests, enabling new mentoring relationships, and promote the use of relevant educational resources. We illustrate our approach with an implemented system called PedConnect that analyzes user activities and presents intelligent suggestions for collaboration and resource use.},
booktitle = {Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces},
pages = {71–74},
numpages = {4},
keywords = {user profiling, teacher social networking, topic modeling},
location = {Lisbon, Portugal},
series = {IUI '12}
}

@article{10.1145/2146382.2146386,
author = {Rusu, Florin and Dobra, Alin},
title = {GLADE: A Scalable Framework for Efficient Analytics},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/2146382.2146386},
doi = {10.1145/2146382.2146386},
abstract = {In this paper we introduce GLADE, a scalable distributed framework for large scale data analytics. GLADE consists of a simple user-interface to define Generalized Linear Aggregates (GLA), the fundamental abstraction at the core of GLADE, and a distributed runtime environment that executes GLAs by using parallelism extensively.GLAs are derived from User-Defined Aggregates (UDA), a relational database extension that allows the user to add specialized aggregates to be executed inside the query processor. GLAs extend the UDA interface with methods to Serialize/Deserialize the state of the aggregate required for distributed computation. As a significant departure from UDAs which can be invoked only through SQL, GLAs give the user direct access to the state of the aggregate, thus allowing for the computation of significantly more complex aggregate functions.GLADE runtime is an execution engine optimized for the GLA computation. The runtime takes the user-defined GLA code, compiles it inside the engine, and executes it right near the data by taking advantage of parallelism both inside a single machine as well as across a cluster of computers. This results in maximum possible execution time performance (all our experimental tasks are I/O-bound) and linear scaleup.},
journal = {SIGOPS Oper. Syst. Rev.},
month = feb,
pages = {12–18},
numpages = {7}
}

@inproceedings{10.1145/2148131.2148148,
author = {Harrison, Chris and Ramamurthy, Shilpa and Hudson, Scott E.},
title = {On-Body Interaction: Armed and Dangerous},
year = {2012},
isbn = {9781450311748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2148131.2148148},
doi = {10.1145/2148131.2148148},
abstract = {Recent technological advances in input sensing, as well as ultra-small projectors, have opened up new opportunities for interaction -- the use of the body itself as both an input and output platform. Such on-body interfaces offer new interactive possibilities, and the promise of access to computation, communication and information literally in the palm of our hands. The unique context of on-body interaction allows us to take advantage of extra dimensions of input our bodies naturally afford us. In this paper, we consider how the arms and hands can be used to enhance on-body interactions, which is typically finger input centric. To explore this opportunity, we developed Armura, a novel interactive on-body system, supporting both input and graphical output. Using this platform as a vehicle for exploration, we proto-typed many applications and interactions. This helped to confirm chief use modalities, identify fruitful interaction approaches, and in general, better understand how interfaces operate on the body. We highlight the most compelling techniques we uncovered. Further, this paper is the first to consider and prototype how conventional interaction issues, such as cursor control and clutching, apply to the on-body domain. Finally, we bring to light several new and unique interaction techniques.},
booktitle = {Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction},
pages = {69–76},
numpages = {8},
keywords = {arumura, sensing, depth camera, projectors, vision-based input, computer vision, free-space gestures},
location = {Kingston, Ontario, Canada},
series = {TEI '12}
}

@inproceedings{10.1145/2184751.2184863,
author = {Viswanathan, Murlikrishna and Zhang, Zhen-Xing and Tian, Xue-Wei and Lim, Joon S.},
title = {Emotional-Speech Recognition Using the Neuro-Fuzzy Network},
year = {2012},
isbn = {9781450311724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2184751.2184863},
doi = {10.1145/2184751.2184863},
abstract = {Emotion recognition based on a speech signal is one of the intensively studied research topics in the domains of human-computer interaction and affective computing. The presented paper is concerned with emotional-speech recognition based on the neuro-fuzzy network with a weighted fuzzy membership function (NEWFM). NEWFM has a feature selection method and makes fuzzy classifiers. In this paper, NEWFM was utilized for classifying four kinds of emotional-speech signals. This NEWFM classification method achieves as high as 86% overall classification accuracy. Significantly, the NEWFM classifier efficiently detects sadness, with a 97.5% recognition rate.},
booktitle = {Proceedings of the 6th International Conference on Ubiquitous Information Management and Communication},
articleno = {96},
numpages = {5},
keywords = {feature selection, fuzzy classifier, emotional-speech recognition, neuro-fuzzy network},
location = {Kuala Lumpur, Malaysia},
series = {ICUIMC '12}
}

@inproceedings{10.1145/2145694.2145699,
author = {Bailie, Scott and Leeser, Miriam},
title = {Incremental Clustering Applied to Radar Deinterleaving: A Parameterized FPGA Implementation},
year = {2012},
isbn = {9781450311557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145694.2145699},
doi = {10.1145/2145694.2145699},
abstract = {ICED (Incremental Clustering of Evolving Data) is a novel incremental clustering algorithm designed for data whose characteristics change over time. ICED is an unsupervised clustering technique that assumes no prior knowledge of the incoming data, and supports removing clusters that contain stale data. The user controls the FPGA implementation through a combination of compile time parameters (number of clusters) and run time parameters (distance threshold, fade cycle length). ICED has been applied to a radar application: pulse deinterleaving. ICED is the first implementation of incremental clustering on an FPGA of which we are aware. The implementation runs 39 times faster than an equivalent C implementation on a 3GHz Intel Xeon processor, and is capable of processing radar data in real time.},
booktitle = {Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {25–28},
numpages = {4},
keywords = {pulse deinterleaving, electronic warfare, clustering, FPGA},
location = {Monterey, California, USA},
series = {FPGA '12}
}

@inproceedings{10.5555/2442691.2442720,
author = {Bai, Yin and Xu, Bin and Ma, Yuanchao and Sun, Guodong and Zhao, Yu},
title = {Will You Have a Good Sleep Tonight? Sleep Quality Prediction with Mobile Phone},
year = {2012},
isbn = {9781936968602},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Understanding the relationship between sleep and daily life can provide insights into a healthy life style since the sleep quality is one of the most important indicators of people's health status. This paper studies the extent to which a person's sleep quality can be predicted by his/her daily context information. A combination of the machine learning technology and medical knowledge is used to study the relation between context and sleep quality, so that sleep quality can be predicted in real time according to the relation.We propose a novel sleep quality predicting framework from user context data, without requiring users to wear special devices. We develop a data collecting and analyzing prototype system called SleepMiner, which uses on-phone data such as mobile sensor data and communication data to extract human contexts. Then the relationship between context data and sleep quality is analyzed and a learning model based on factor graph model is proposed to predict sleep quality. From experimental results we demonstrate that it is possible to accurately infer sleep quality (around 78%) from user context information. A set of solutions are proposed to address the practical problems of Android phone in data collection, making SleepMiner work with minimal impact on the phone's resources. We finally carry out experiments to evaluate our design in effectiveness and efficiency.},
booktitle = {Proceedings of the 7th International Conference on Body Area Networks},
pages = {124–130},
numpages = {7},
keywords = {sleep quality prediction, factor graph model, mobile phone},
location = {Oslo, Norway},
series = {BodyNets '12}
}

@inproceedings{10.1145/2162081.2162096,
author = {Zohar, Eyal and Cidon, Israel and Mokryn, Osnat (Ossi)},
title = {Celleration: Loss-Resilient Traffic Redundancy Elimination for Cellular Data},
year = {2012},
isbn = {9781450312073},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2162081.2162096},
doi = {10.1145/2162081.2162096},
abstract = {In this paper we present Celleration, a novel gateway-to-mobile Traffic Redundancy Elimination (TRE) system, designed for the new generation of data-intensive cellular networks.Cellular TRE needs to account for the mobile device's limited battery power and the characteristics of the cellular network such as users' mobility, high packet-loss and long round-trip delays.Celleration is based on a novel TRE technique, in which the cellular gateway observes the forwarded chunks to identify the beginning of a previously observed chunk chain, which in turn is used as a reliable predictor to multiple future chunks. These predictions establish an ad-hoc gateway-to-mobile TRE learning mechanism that leverages the gateway's history records and the user mobile device's cached content for an efficient TRE operation for both the backhaul and the wireless last-mile.We present a data analysis of captured cellular traffic from 130 cellular sites and a long-term study of a social network. Finally, we analyze Celleration redundancy elimination and performance under high packet loss.},
booktitle = {Proceedings of the Twelfth Workshop on Mobile Computing Systems &amp; Applications},
articleno = {10},
numpages = {6},
location = {San Diego, California},
series = {HotMobile '12}
}

@inproceedings{10.1145/2157136.2157267,
author = {Petit, Jordi and Gim\'{e}nez, Omer and Roura, Salvador},
title = {Jutge.Org: An Educational Programming Judge},
year = {2012},
isbn = {9781450310987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2157136.2157267},
doi = {10.1145/2157136.2157267},
abstract = {Jutge.org is an open access educational online programming judge where students can try to solve more than 800 problems using 22 programming languages. The verdict of their solutions is computed using exhaustive test sets run under time, memory and security restrictions. By contrast to many popular online judges, Jutge.org is designed for students and instructors: On one hand, the problem repository is mainly aimed to beginners, with a clear organization and gradding. On the other hand, the system is designed as a virtual learning environment where instructors can administer their own courses, manage their roster of students and tutors, add problems, attach documents, create lists of problems, assignments, contests and exams. This paper presents Jutge.org and offers some case studies of courses using it.},
booktitle = {Proceedings of the 43rd ACM Technical Symposium on Computer Science Education},
pages = {445–450},
numpages = {6},
keywords = {tools, instructional technologies, active learning, CS1/2, experiences},
location = {Raleigh, North Carolina, USA},
series = {SIGCSE '12}
}

@article{10.1145/2133360.2133363,
author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
title = {Isolation-Based Anomaly Detection},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2133360.2133363},
doi = {10.1145/2133360.2133363},
abstract = {Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. This article proposes a method called Isolation Forest (iForest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure---fundamentally different from all existing methods.As a result, iForest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that iForest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. iForest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {3},
numpages = {39},
keywords = {Anomaly detection, random tree ensemble, outlier detection, binary tree, isolation forest, ensemble methods, isolation}
}

@inproceedings{10.1145/2185475.2185477,
author = {Del Vento, Davide},
title = {Performance Optimization on a Supercomputer with CTuning and the PGI Compiler},
year = {2012},
isbn = {9781450311472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2185475.2185477},
doi = {10.1145/2185475.2185477},
abstract = {In this paper we show a machine learning based implementation of autotuning, built with the cTuning CC framework. We implemented the PGI compiler in the cTuning CC framework, plugged in a few additional benchmarks and tested it on a Cray XT5m supercomputer. The main contribution of the present paper consists in combining existing autotuning techniques and using them with the PGI production compiler. Although not ready for production workflows yet, our results are encouraging.},
booktitle = {Proceedings of the 2nd International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era},
pages = {12–20},
numpages = {9},
keywords = {automatic performance tuning, benchmarks, machine learning compiler, self-optimization, self-tuning compiler},
location = {London, United Kingdom},
series = {EXADAPT '12}
}

@inproceedings{10.1145/2150976.2150984,
author = {Ahmad, Faraz and Chakradhar, Srimat T. and Raghunathan, Anand and Vijaykumar, T. N.},
title = {Tarazu: Optimizing MapReduce on Heterogeneous Clusters},
year = {2012},
isbn = {9781450307598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2150976.2150984},
doi = {10.1145/2150976.2150984},
abstract = {Data center-scale clusters are evolving towards heterogeneous hardware for power, cost, differentiated price-performance, and other reasons. MapReduce is a well-known programming model to process large amount of data on data center-scale clusters. Most MapReduce implementations have been designed and optimized for homogeneous clusters. Unfortunately, these implementations perform poorly on heterogeneous clusters (e.g., on a 90-node cluster that contains 10 Xeon-based servers and 80 Atom-based servers, Hadoop performs worse than on 10-node Xeon-only or 80-node Atom-only homogeneous sub-clusters for many of our benchmarks). This poor performance remains despite previously proposed optimizations related to management of straggler tasks. In this paper, we address MapReduce's poor performance on heterogeneous clusters. Our first contribution is that the poor performance is due to two key factors: (1) the non-intuitive effect that MapReduce's built-in load balancing results in excessive and bursty network communication during the Map phase, and (2) the intuitive effect that the heterogeneity amplifies load imbalance in the Reduce computation. Our second contribution is Tarazu, a suite of optimizations to improve MapReduce performance on heterogeneous clusters. Tarazu consists of (1) Communication-Aware Load Balancing of Map computation (CALB) across the nodes, (2) Communication-Aware Scheduling of Map computation (CAS) to avoid bursty network traffic and (3) Predictive Load Balancing of Reduce computation (PLB) across the nodes. Using the above 90-node cluster, we show that Tarazu significantly improves performance over a baseline of Hadoop with straightforward tuning for hardware heterogeneity.},
booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {61–74},
numpages = {14},
keywords = {shuffle, cluster scheduling, heterogeneous clusters, load imbalance, MapReduce},
location = {London, England, UK},
series = {ASPLOS XVII}
}

@article{10.1145/2189750.2150984,
author = {Ahmad, Faraz and Chakradhar, Srimat T. and Raghunathan, Anand and Vijaykumar, T. N.},
title = {Tarazu: Optimizing MapReduce on Heterogeneous Clusters},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2189750.2150984},
doi = {10.1145/2189750.2150984},
abstract = {Data center-scale clusters are evolving towards heterogeneous hardware for power, cost, differentiated price-performance, and other reasons. MapReduce is a well-known programming model to process large amount of data on data center-scale clusters. Most MapReduce implementations have been designed and optimized for homogeneous clusters. Unfortunately, these implementations perform poorly on heterogeneous clusters (e.g., on a 90-node cluster that contains 10 Xeon-based servers and 80 Atom-based servers, Hadoop performs worse than on 10-node Xeon-only or 80-node Atom-only homogeneous sub-clusters for many of our benchmarks). This poor performance remains despite previously proposed optimizations related to management of straggler tasks. In this paper, we address MapReduce's poor performance on heterogeneous clusters. Our first contribution is that the poor performance is due to two key factors: (1) the non-intuitive effect that MapReduce's built-in load balancing results in excessive and bursty network communication during the Map phase, and (2) the intuitive effect that the heterogeneity amplifies load imbalance in the Reduce computation. Our second contribution is Tarazu, a suite of optimizations to improve MapReduce performance on heterogeneous clusters. Tarazu consists of (1) Communication-Aware Load Balancing of Map computation (CALB) across the nodes, (2) Communication-Aware Scheduling of Map computation (CAS) to avoid bursty network traffic and (3) Predictive Load Balancing of Reduce computation (PLB) across the nodes. Using the above 90-node cluster, we show that Tarazu significantly improves performance over a baseline of Hadoop with straightforward tuning for hardware heterogeneity.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {61–74},
numpages = {14},
keywords = {MapReduce, load imbalance, heterogeneous clusters, shuffle, cluster scheduling}
}

@article{10.1145/2248487.2150984,
author = {Ahmad, Faraz and Chakradhar, Srimat T. and Raghunathan, Anand and Vijaykumar, T. N.},
title = {Tarazu: Optimizing MapReduce on Heterogeneous Clusters},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2248487.2150984},
doi = {10.1145/2248487.2150984},
abstract = {Data center-scale clusters are evolving towards heterogeneous hardware for power, cost, differentiated price-performance, and other reasons. MapReduce is a well-known programming model to process large amount of data on data center-scale clusters. Most MapReduce implementations have been designed and optimized for homogeneous clusters. Unfortunately, these implementations perform poorly on heterogeneous clusters (e.g., on a 90-node cluster that contains 10 Xeon-based servers and 80 Atom-based servers, Hadoop performs worse than on 10-node Xeon-only or 80-node Atom-only homogeneous sub-clusters for many of our benchmarks). This poor performance remains despite previously proposed optimizations related to management of straggler tasks. In this paper, we address MapReduce's poor performance on heterogeneous clusters. Our first contribution is that the poor performance is due to two key factors: (1) the non-intuitive effect that MapReduce's built-in load balancing results in excessive and bursty network communication during the Map phase, and (2) the intuitive effect that the heterogeneity amplifies load imbalance in the Reduce computation. Our second contribution is Tarazu, a suite of optimizations to improve MapReduce performance on heterogeneous clusters. Tarazu consists of (1) Communication-Aware Load Balancing of Map computation (CALB) across the nodes, (2) Communication-Aware Scheduling of Map computation (CAS) to avoid bursty network traffic and (3) Predictive Load Balancing of Reduce computation (PLB) across the nodes. Using the above 90-node cluster, we show that Tarazu significantly improves performance over a baseline of Hadoop with straightforward tuning for hardware heterogeneity.},
journal = {SIGPLAN Not.},
month = mar,
pages = {61–74},
numpages = {14},
keywords = {MapReduce, cluster scheduling, heterogeneous clusters, load imbalance, shuffle}
}

@inproceedings{10.1145/2150976.2151021,
author = {Vasi\'{c}, Nedeljko and Novakovi\'{c}, Dejan and Miu\v{c}in, Svetozar and Kosti\'{c}, Dejan and Bianchini, Ricardo},
title = {DejaVu: Accelerating Resource Allocation in Virtualized Environments},
year = {2012},
isbn = {9781450307598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2150976.2151021},
doi = {10.1145/2150976.2151021},
abstract = {Effective resource management of virtualized environments is a challenging task. State-of-the-art management systems either rely on analytical models or evaluate resource allocations by running actual experiments. However, both approaches incur a significant overhead once the workload changes. The former needs to re-calibrate and re-validate models, whereas the latter has to run a new set of experiments to select a new resource allocation. During the adaptation period, the system may run with an inefficient configuration. In this paper, we propose DejaVu - a framework that (1) minimizes the resource management overhead by identifying a small set of workload classes for which it needs to evaluate resource allocation decisions, (2) quickly adapts to workload changes by classifying workloads using signatures and caching their preferred resource allocations at runtime, and (3) deals with interference by estimating an "interference index". We evaluate DejaVu by running representative network services on Amazon EC2. DejaVu achieves more than 10x speedup in adaptation time for each workload change relative to the state-of-the-art. By enabling quick adaptation, DejaVu saves up to 60% of the service provisioning cost. Finally, DejaVu is easily deployable as it does not require any extensive instrumentation or human intervention.},
booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {423–436},
numpages = {14},
keywords = {resource management, data center, virtualization},
location = {London, England, UK},
series = {ASPLOS XVII}
}

@article{10.1145/2189750.2151021,
author = {Vasi\'{c}, Nedeljko and Novakovi\'{c}, Dejan and Miu\v{c}in, Svetozar and Kosti\'{c}, Dejan and Bianchini, Ricardo},
title = {DejaVu: Accelerating Resource Allocation in Virtualized Environments},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2189750.2151021},
doi = {10.1145/2189750.2151021},
abstract = {Effective resource management of virtualized environments is a challenging task. State-of-the-art management systems either rely on analytical models or evaluate resource allocations by running actual experiments. However, both approaches incur a significant overhead once the workload changes. The former needs to re-calibrate and re-validate models, whereas the latter has to run a new set of experiments to select a new resource allocation. During the adaptation period, the system may run with an inefficient configuration. In this paper, we propose DejaVu - a framework that (1) minimizes the resource management overhead by identifying a small set of workload classes for which it needs to evaluate resource allocation decisions, (2) quickly adapts to workload changes by classifying workloads using signatures and caching their preferred resource allocations at runtime, and (3) deals with interference by estimating an "interference index". We evaluate DejaVu by running representative network services on Amazon EC2. DejaVu achieves more than 10x speedup in adaptation time for each workload change relative to the state-of-the-art. By enabling quick adaptation, DejaVu saves up to 60% of the service provisioning cost. Finally, DejaVu is easily deployable as it does not require any extensive instrumentation or human intervention.},
journal = {SIGARCH Comput. Archit. News},
month = mar,
pages = {423–436},
numpages = {14},
keywords = {resource management, virtualization, data center}
}

@article{10.1145/2248487.2151021,
author = {Vasi\'{c}, Nedeljko and Novakovi\'{c}, Dejan and Miu\v{c}in, Svetozar and Kosti\'{c}, Dejan and Bianchini, Ricardo},
title = {DejaVu: Accelerating Resource Allocation in Virtualized Environments},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2248487.2151021},
doi = {10.1145/2248487.2151021},
abstract = {Effective resource management of virtualized environments is a challenging task. State-of-the-art management systems either rely on analytical models or evaluate resource allocations by running actual experiments. However, both approaches incur a significant overhead once the workload changes. The former needs to re-calibrate and re-validate models, whereas the latter has to run a new set of experiments to select a new resource allocation. During the adaptation period, the system may run with an inefficient configuration. In this paper, we propose DejaVu - a framework that (1) minimizes the resource management overhead by identifying a small set of workload classes for which it needs to evaluate resource allocation decisions, (2) quickly adapts to workload changes by classifying workloads using signatures and caching their preferred resource allocations at runtime, and (3) deals with interference by estimating an "interference index". We evaluate DejaVu by running representative network services on Amazon EC2. DejaVu achieves more than 10x speedup in adaptation time for each workload change relative to the state-of-the-art. By enabling quick adaptation, DejaVu saves up to 60% of the service provisioning cost. Finally, DejaVu is easily deployable as it does not require any extensive instrumentation or human intervention.},
journal = {SIGPLAN Not.},
month = mar,
pages = {423–436},
numpages = {14},
keywords = {data center, virtualization, resource management}
}

@article{10.1145/2365864.2151038,
author = {Gerofi, Balazs and Ishikawa, Yutaka},
title = {Enhancing TCP Throughput of Highly Available Virtual Machines via Speculative Communication},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/2365864.2151038},
doi = {10.1145/2365864.2151038},
abstract = {Checkpoint-recovery based virtual machine (VM) replication is an attractive technique for accommodating VM installations with high-availability. It provides seamless failover for the entire software stack executed in the VM regardless the application or the underlying operating system (OS), it runs on commodity hardware, and it is inherently capable of dealing with shared memory non-determinism of symmetric multiprocessing (SMP) configurations. There have been several studies aiming at alleviating the overhead of replication, however, due to consistency requirements, network performance of the basic replication mechanism remains extremely poor.,In this paper we revisit the replication protocol and extend it with speculative communication. Speculative communication silently acknowledges TCP packets of the VM, enabling the guest's TCP stack to progress with transmission without exposing the messages to the clients before the corresponding execution state is checkpointed to the backup host. Furthermore, we propose replication aware congestion control, an extension to the guest's TCP stack that aggressively fills up the VMM's replication buffer so that speculative packets can be backed up and released earlier to the clients. We observe up to an order of magnitude improvement in bulk data transfer with speculative communication, and close to native VM network performance when replication awareness is enabled in the guest OS. We provide results of micro-, as well as application-level benchmarks.},
journal = {SIGPLAN Not.},
month = mar,
pages = {87–96},
numpages = {10},
keywords = {recovery, checkpoint, hypervisor, virtualization, fault tolerance}
}

@inproceedings{10.1145/2151024.2151038,
author = {Gerofi, Balazs and Ishikawa, Yutaka},
title = {Enhancing TCP Throughput of Highly Available Virtual Machines via Speculative Communication},
year = {2012},
isbn = {9781450311762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2151024.2151038},
doi = {10.1145/2151024.2151038},
abstract = {Checkpoint-recovery based virtual machine (VM) replication is an attractive technique for accommodating VM installations with high-availability. It provides seamless failover for the entire software stack executed in the VM regardless the application or the underlying operating system (OS), it runs on commodity hardware, and it is inherently capable of dealing with shared memory non-determinism of symmetric multiprocessing (SMP) configurations. There have been several studies aiming at alleviating the overhead of replication, however, due to consistency requirements, network performance of the basic replication mechanism remains extremely poor.,In this paper we revisit the replication protocol and extend it with speculative communication. Speculative communication silently acknowledges TCP packets of the VM, enabling the guest's TCP stack to progress with transmission without exposing the messages to the clients before the corresponding execution state is checkpointed to the backup host. Furthermore, we propose replication aware congestion control, an extension to the guest's TCP stack that aggressively fills up the VMM's replication buffer so that speculative packets can be backed up and released earlier to the clients. We observe up to an order of magnitude improvement in bulk data transfer with speculative communication, and close to native VM network performance when replication awareness is enabled in the guest OS. We provide results of micro-, as well as application-level benchmarks.},
booktitle = {Proceedings of the 8th ACM SIGPLAN/SIGOPS Conference on Virtual Execution Environments},
pages = {87–96},
numpages = {10},
keywords = {virtualization, checkpoint, fault tolerance, recovery, hypervisor},
location = {London, England, UK},
series = {VEE '12}
}

