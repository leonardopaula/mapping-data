@inproceedings{10.5555/2534504.2534544,
author = {Kocielnik, Rafal and Maggi, Fabrizio Maria and Sidorova, Natalia},
title = {Enabling Self-Reflection with LifelogExplorer: Generating Simple Views from Complex Data},
year = {2013},
isbn = {9781936968800},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Nowadays, people are overwhelmed with multiple tasks and responsibilities, resulting in increasing stress level. At the same time, it becomes harder to find time for self-reflection and diagnostics of problems that can be source of stress. In this paper, we propose a tool that supports a person in self-reflection by providing views on life events in their relation to person's well-being in a concise and intuitive form. The tool, called LifelogExplorer, takes sensor data (like skin conductance and accelerometer measurements) and data obtained from digital sources (like personal calendars) as input and generates views on this data which are comprehensible and meaningful for the user due to filtering and aggregation options which help to cope with the data explosion. We evaluate our approach on the data collected from two case studies focused on addressing stress at work: 1) with academic staff of a university, and 2) with teachers from a vocational school.},
booktitle = {Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {184–191},
numpages = {8},
location = {Venice, Italy},
series = {PervasiveHealth '13}
}

@inproceedings{10.4108/icst.pervasivehealth.2013.252133,
author = {Guo, Rui and Li, Shuangjiang and He, Li and Gao, Wei and Qi, Hairong and Owens, Gina},
title = {Pervasive and Unobtrusive Emotion Sensing for Human Mental Health},
year = {2013},
isbn = {9781936968800},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.pervasivehealth.2013.252133},
doi = {10.4108/icst.pervasivehealth.2013.252133},
abstract = {In this paper, we present a pervasive and unobtrusive system for sensing human emotions, which are inferred based on the recording, processing, and analysis of the Galvanic Skin Response (GSR) signal from human bodies. Being different from traditional multimodal emotion sensing systems, our proposed system recognizes human emotions with the single modularity of GSR signal, which is captured by wearable sensing devices. A comprehensive set of features is extracted from GSR signal and fed into supervised classifiers for emotion identification. Our system has been evaluated by specific experiments to investigate the characteristics of human emotions in practice. The high accuracy of emotion classification highlights the great potential of this system in improving humans' mental health in the future.},
booktitle = {Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {436–439},
numpages = {4},
location = {Venice, Italy},
series = {PervasiveHealth '13}
}

@inproceedings{10.4108/icst.pervasivehealth.2013.252061,
author = {Giggins, Oonagh and Kelly, Daniel and Caulfield, Brian},
title = {Evaluating Rehabilitation Exercise Performance Using a Single Inertial Measurement Unit},
year = {2013},
isbn = {9781936968800},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.pervasivehealth.2013.252061},
doi = {10.4108/icst.pervasivehealth.2013.252061},
abstract = {Inertial measurement units (IMUs) may be used during exercise to provide feedback on exercise technique. However the number of IMUs that are required to deliver effective feedback during lower limb exercises has not yet been established. This preliminary investigation sought to investigate whether a single IMU on the shin is capable of identifying conditions of poor technique during seven lower limb exercises. Nine healthy volunteers (five male, four female, age: 26.3 ± 6.7 years, height: 1.77 ± 0.08m, weight: 73.4 ± 8.7 kg) performed the exercises firstly with correct technique and then with different conditions of poor technique. Acceleration and angular velocity were recorded from the IMU positioned on the shin during all exercise performance conditions. Maximum and minimum acceleration and angular velocity (in X, Y, Z) and the range of each were calculated for each condition. A paired t test was used to analyse whether there was a difference in the IMU parameters between the different exercise conditions. Joint range of motion at the hip, knee and ankle were calculated using data derived from a marker based motion analysis system in order to confirm that expected deviations had occurred. The data presented has revealed that a single IMU can be used to identify the conditions of poor technique during five of the seven exercises studied. This investigation provides preliminary evidence to suggest that one IMU placed on the shin can be used to identify poor technique during seven common rehabilitation exercises, however pattern recognition techniques must be developed in order to facilitate objective real time performance measurement and feedback.},
booktitle = {Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {49–56},
numpages = {8},
keywords = {biofeedback, exercise performance, rehabilitation},
location = {Venice, Italy},
series = {PervasiveHealth '13}
}

@inproceedings{10.4108/icst.pervasivehealth.2013.252157,
author = {Poole, Erika Shehan and Eir\'{\i}ksd\'{o}ttir, Elsa and Miller, Andrew D. and Xu, Yan and Catrambone, Richard and Mynatt, Elizabeth D.},
title = {Designing for Spectators and Coaches: Social Support in Pervasive Health Games for Youth},
year = {2013},
isbn = {9781936968800},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.pervasivehealth.2013.252157},
doi = {10.4108/icst.pervasivehealth.2013.252157},
abstract = {Active video games and other technology-based interventions can promote physical activity participation in children and adolescents, particularly those who are uninterested in traditional sports or lack access to gyms, sports clubs, or safe neighborhood recreational environments. Yet simply placing a game console in a home or school might not be sufficient for changing physical activity behaviors. Rather, social support and opportunities for structured group activity may be important aspects of pervasive health games. We know little, however, about how to design active video games and other technology-based interventions in ways that explicitly allow for the provision of social support by other players as well as "spectators" of the game. Based on the results of a longitudinal study of an active video game used in American schools, this paper contributes design recommendations for features in pervasive health games that explicitly encourage social support.},
booktitle = {Proceedings of the 7th International Conference on Pervasive Computing Technologies for Healthcare},
pages = {161–168},
numpages = {8},
keywords = {active video games, human-computer interaction, social support, youth},
location = {Venice, Italy},
series = {PervasiveHealth '13}
}

@inproceedings{10.5555/2484920.2485018,
author = {Niewiadomski, Rados\l{}aw and Hofmann, Jennifer and Urbain, J\'{e}r\^{o}me and Platt, Tracey and Wagner, Johannes and Piot, Bilal and Cakmak, Huseyin and Pammi, Sathish and Baur, Tobias and Dupont, Stephane and Geist, Matthieu and Lingenfelser, Florian and McKeown, Gary and Pietquin, Olivier and Ruch, Willibald},
title = {Laugh-Aware Virtual Agent and Its Impact on User Amusement},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper we present a complete interactive system enabled to detect human laughs and respond appropriately, by integrating the information of the human behavior and the context. Furthermore, the impact of our autonomous laughter-aware agent on the humor experience of the user and interaction between user and agent is evaluated by subjective and objective means. Preliminary results show that the laughter-aware agent increases the humor experience (i.e., felt amusement of the user and the funniness rating of the film clip), and creates the notion of a shared social experience, indicating that the agent is useful to elicit positive humor-related affect and emotional contagion.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {619–626},
numpages = {8},
keywords = {virtual agents, laughter synthesis, laughter detection},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/2484313.2484373,
author = {Hasan, Ragib and Saxena, Nitesh and Haleviz, Tzipora and Zawoad, Shams and Rinehart, Dustin},
title = {Sensing-Enabled Channels for Hard-to-Detect Command and Control of Mobile Devices},
year = {2013},
isbn = {9781450317672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484313.2484373},
doi = {10.1145/2484313.2484373},
abstract = {The proliferation of mobile computing devices has enabled immense opportunities for everyday users. At the same time, however, this has opened up new, and perhaps more severe, possibilities for attacks. In this paper, we explore a novel generation of mobile malware that exploits the rich variety of sensors available on current mobile devices.Two properties distinguish the proposed malware from the existing state-of-the-art. First, in addition to the misuse of the various traditional services available on modern mobile devices, this malware can be used for the purpose of targeted context-aware attacks. Second, this malware can be commanded and controlled over context-aware, out-of-band channels as opposed to a centralized infrastructure. These communication channels can be used to quickly reach out to a large number of infected devices, while offering a high degree of undetectability. In particular, unlike traditional network-based communication, the proposed sensing-enabled channels cannot be detected by monitoring the cellular or wireless communication networks. To demonstrate the feasibility of our proposed attack, we present different flavors of command and control channels based on acoustic, visual, magnetic and vibrational signaling. We further build and test a proof-of-concept Android application implementing many such channels.},
booktitle = {Proceedings of the 8th ACM SIGSAC Symposium on Information, Computer and Communications Security},
pages = {469–480},
numpages = {12},
keywords = {mobile malware, mobile device sensors, command &amp; control, mobile security, covert channel},
location = {Hangzhou, China},
series = {ASIA CCS '13}
}

@inproceedings{10.1145/2484313.2484362,
author = {Chu, Cheng-Kang and Liu, Joseph K. and Wong, Jun Wen and Zhao, Yunlei and Zhou, Jianying},
title = {Privacy-Preserving Smart Metering with Regional Statistics and Personal Enquiry Services},
year = {2013},
isbn = {9781450317672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484313.2484362},
doi = {10.1145/2484313.2484362},
abstract = {In smart grid, households may send the readings of their energy usage to the utility and a third-party service provider which provides analyzed statistics data to users. User privacy becomes an important issue in this application. In this paper, we propose a new cryptographic-based solution for the privacy issue in smart grid systems. The advantages of our system are twofold: Households can send authenticated energy consumption readings to a third-party service provider anonymously. The service provider learns only the region where the readings come from but not their respective identities. On the other hand, users with personal secret information can enquiry their usage history records or regional statistics.Formal security analysis is provided to show that our scheme is secure. We further analyze the performance of our system by giving simulation results.},
booktitle = {Proceedings of the 8th ACM SIGSAC Symposium on Information, Computer and Communications Security},
pages = {369–380},
numpages = {12},
keywords = {authentication, privacy, smart metering},
location = {Hangzhou, China},
series = {ASIA CCS '13}
}

@inproceedings{10.1145/2488388.2488494,
author = {Song, Yang and Shi, Xiaolin and Fu, Xin},
title = {Evaluating and Predicting User Engagement Change with Degraded Search Relevance},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488494},
doi = {10.1145/2488388.2488494},
abstract = {User engagement in search refers to the frequency for users (re-)using the search engine to accomplish their tasks. Among factors that affected users' visit frequency, relevance of search results is believed to play a pivotal role. While multiple work in the past has demonstrated the correlation between search success and user engagement based on longitudinal analysis, we examine this problem from a different perspective in this work. Specifically, we carefully designed a large-scale controlled experiment on users of a large commercial Web search engine, in which users were separated into control and treatment groups, where users in treatment group were presented with search results which are deliberate degraded in relevance. We studied users' responses to the relevance degradation through tracking several behavioral metrics (such as query per user, click per session) over an extended period of time both during and following the experiment. By quantifying the relationship between user engagement and search relevance, we observe significant differences between user's short-term search behavior and long-term engagement change. By leveraging some of the key findings from the experiment, we developed a machine learning model to predict the long term impact of relevance degradation on user engagement. Overall, our model achieves over 67% of accuracy in predicting user engagement drop. Besides, our model is also capable of predicting engagement change for low-frequency users with very few user signals. We believe that insights from this study can be leveraged by search engine companies to detect and intervene search relevance degradation and to prevent long term user engagement drop.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1213–1224},
numpages = {12},
keywords = {search quality, search relevance, longitudinal analysis, user engagement},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2488388.2488433,
author = {Gupta, Pankaj and Goel, Ashish and Lin, Jimmy and Sharma, Aneesh and Wang, Dong and Zadeh, Reza},
title = {WTF: The Who to Follow Service at Twitter},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488433},
doi = {10.1145/2488388.2488433},
abstract = {WTF ("Who to Follow") is Twitter's user recommendation service, which is responsible for creating millions of connections daily between users based on shared interests, common connections, and other related factors. This paper provides an architectural overview and shares lessons we learned in building and running the service over the past few years. Particularly noteworthy was our design decision to process the entire Twitter graph in memory on a single server, which significantly reduced architectural complexity and allowed us to develop and deploy the service in only a few months. At the core of our architecture is Cassovary, an open-source in-memory graph processing engine we built from scratch for WTF. Besides powering Twitter's user recommendations, Cassovary is also used for search, discovery, promoted products, and other services as well. We describe and evaluate a few graph recommendation algorithms implemented in Cassovary, including a novel approach based on a combination of random walks and SALSA. Looking into the future, we revisit the design of our architecture and comment on its limitations, which are presently being addressed in a second-generation system under development.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {505–514},
numpages = {10},
keywords = {graph processing, hadoop, link prediction},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2488388.2488514,
author = {Yan, Xiaohui and Guo, Jiafeng and Lan, Yanyan and Cheng, Xueqi},
title = {A Biterm Topic Model for Short Texts},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488514},
doi = {10.1145/2488388.2488514},
abstract = {Uncovering the topics within short texts, such as tweets and instant messages, has become an important task for many content analysis applications. However, directly applying conventional topic models (e.g. LDA and PLSA) on such short texts may not work well. The fundamental reason lies in that conventional topic models implicitly capture the document-level word co-occurrence patterns to reveal topics, and thus suffer from the severe data sparsity in short documents. In this paper, we propose a novel way for modeling topics in short texts, referred as biterm topic model (BTM). Specifically, in BTM we learn the topics by directly modeling the generation of word co-occurrence patterns (i.e. biterms) in the whole corpus. The major advantages of BTM are that 1) BTM explicitly models the word co-occurrence patterns to enhance the topic learning; and 2) BTM uses the aggregated patterns in the whole corpus for learning topics to solve the problem of sparse word co-occurrence patterns at document-level. We carry out extensive experiments on real-world short text collections. The results demonstrate that our approach can discover more prominent and coherent topics, and significantly outperform baseline methods on several evaluation metrics. Furthermore, we find that BTM can outperform LDA even on normal texts, showing the potential generality and wider usage of the new topic model.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1445–1456},
numpages = {12},
keywords = {short text, content analysis, topic model, biterm},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2487788.2488075,
author = {Byrne Evans, Maire and O'Hara, Kieron and Tiropanis, Thanassis and Webber, Craig},
title = {Crime Applications and Social Machines: Crowdsourcing Sensitive Data},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488075},
doi = {10.1145/2487788.2488075},
abstract = {The authors explore some issues with the United Kingdom (U.K.) crime reporting and recording systems which currently produce Open Crime Data. The availability of Open Crime Data seems to create a potential data ecosystem which would encourage crowdsourcing, or the creation of social machines, in order to counter some of these issues. While such solutions are enticing, we suggest that in fact the theoretical solution brings to light fairly compelling problems, which highlight some limitations of crowdsourcing as a means of addressing Berners-Lee's "social constraint." The authors present a thought experiment -- a Gendankenexperiment - in order to explore the implications, both good and bad, of a social machine in such a sensitive space and suggest a Web Science perspective to pick apart the ramifications of this thought experiment as a theoretical approach to the characterisation of social machines.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {891–896},
numpages = {6},
keywords = {transparency, crime data, trust, network science, social machines, open data},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1109/CCGrid.2013.89,
author = {Ghribi, Chaima and Hadji, Makhlouf and Zeghlache, Djamal},
title = {Energy Efficient VM Scheduling for Cloud Data Centers: Exact Allocation and Migration Algorithms},
year = {2013},
isbn = {9780768549965},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2013.89},
doi = {10.1109/CCGrid.2013.89},
abstract = {This paper presents two exact algorithms for energy efficient scheduling of virtual machines (VMs) in cloud data centers. Modeling of energy aware allocation and consolidation to minimize overall energy consumption leads us to the combination of an optimal allocation algorithm with a consolidation algorithm relying on migration of VMs at service departures. The optimal allocation algorithm is solved as a bin packing problem with a minimum power consumption objective. It is compared with an energy aware best fit algorithm. The exact migration algorithm results from a linear and integer formulation of VM migration to adapt placement when resources are released. The proposed migration is general and goes beyond the current state of the art by minimizing both the number of migrations needed for consolidation and energy consumption in a single algorithm with a set of valid inequalities and conditions. Experimental results show the benefits of combining the allocation and migration algorithms and demonstrate their ability to achieve significant energy savings while maintaining feasible convergence times when compared with the best fit heuristic.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {671–678},
numpages = {8},
keywords = {cloud data center, energy efficiency, VM migration, VM placement, linear integer programming},
location = {Delft, Netherlands},
series = {CCGRID '13}
}

@inproceedings{10.1145/2488388.2488491,
author = {Sodomka, Eric and Lahaie, S\'{e}bastien and Hillard, Dustin},
title = {A Predictive Model for Advertiser Value-per-Click in Sponsored Search},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488491},
doi = {10.1145/2488388.2488491},
abstract = {Sponsored search is a form of online advertising where advertisers bid for placement next to search engine results for specific keywords. As search engines compete for the growing share of online ad spend, it becomes important for them to understand what keywords advertisers value most, and what characteristics of keywords drive value. In this paper we propose an approach to keyword value prediction that draws on advertiser bidding behavior across the terms and campaigns in an account. We provide original insights into the structure of sponsored search accounts that motivate the use of a hierarchical modeling strategy. We propose an economically meaningful loss function which allows us to implicitly fit a linear model for values given observables such as bids and click-through rates. The model draws on demographic and textual features of keywords and takes advantage of the hierarchical structure of sponsored search accounts. Its predictive quality is evaluated on several high-revenue and high-exposure advertising accounts on a major search engine. Besides the general evaluation of advertiser welfare, our approach has potential applications to keyword and bid suggestion.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1179–1190},
numpages = {12},
keywords = {sponsored search, regret loss function, hierarchical model},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1109/CCGrid.2013.87,
author = {Cunha, Carlos Augusto and Silva, Luis Moura e},
title = {SHStream: Self-Healing Framework for HTTP Video-Streaming},
year = {2013},
isbn = {9780768549965},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2013.87},
doi = {10.1109/CCGrid.2013.87},
abstract = {HTTP video-streaming is leading delivery of video content over the Internet. This phenomenon is explained by the ubiquity of web browsers, the permeability of HTTP traffic and the recent video technologies around HTML5. However, the inclusion of multimedia requests imposes new requirements on web servers due to responses with lifespans that can reach dozens of minutes and timing requirements for data fragments transmitted during the response period. Consequently, webservers require real-time performance control to avoid playback outages caused by overloading and performance anomalies. We present SHStream, a self-healing framework for web servers delivering video-streaming content that provides (1) load admittance to avoid server overloading; (2) prediction of performance anomalies using online data stream learning algorithms; (3) continuous evaluation and selection of the best algorithm for prediction; and (4) proactive recovery by migrating the server to other hosts using container-based virtualization techniques. Evaluation of our framework using several variants of Hoeffding trees and ensemble algorithms showed that with a small number of learning instances, it is possible to achieve approximately 98% of recall and 99% of precision for failure predictions. Additionally, proactive failover can be performed in less than 1 second.},
booktitle = {Proceedings of the 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {514–520},
numpages = {7},
location = {Delft, Netherlands},
series = {CCGRID '13}
}

@inproceedings{10.1145/2488388.2488406,
author = {Carrascal, Juan Pablo and Riederer, Christopher and Erramilli, Vijay and Cherubini, Mauro and de Oliveira, Rodrigo},
title = {Your Browsing Behavior for a Big Mac: Economics of Personal Information Online},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488406},
doi = {10.1145/2488388.2488406},
abstract = {Most online service providers offer free services to users and in part, these services collect and monetize personally identifiable information (PII), primarily via targeted advertisements. Against this backdrop of economic exploitation of PII, it is vital to understand the value that users put to their own PII. Although studies have tried to discover how users value their privacy, little is known about how users value their PII while browsing, or the exploitation of their PII. Extracting valuations of PII from users is non-trivial - surveys cannot be relied on as they do not gather information of the context where PII is being released, thus reducing validity of answers. In this work, we rely on refined Experience Sampling - a data collection method that probes users to valuate their PII at the time and place where it was generated in order to minimize retrospective recall and hence increase measurement validity. For obtaining an honest valuation of PII, we use a reverse second price auction. We developed a web browser plugin and had 168 users - living in Spain - install and use this plugin for 2 weeks in order to extract valuations of PII in different contexts.We found that users value items of their online browsing history for about ∈7 (~10USD), and they give higher valuations to their offline PII, such as age and address (about 25∈ or ~36USD). When it comes to PII shared in specific online services, users value information pertaining to financial transactions and social network interactions more than activities like search and shopping. No significant distinction was found between valuations of different quantities of PII (e.g. one vs. 10 search keywords), but deviation was found between types of PII (e.g. photos vs. keywords). Finally, the users' preferred goods for exchanging their PII included money and improvements in service, followed by getting more free services and targeted advertisements.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {189–200},
numpages = {12},
keywords = {privacy, pii, economics of personal information, experience sampling},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2482767.2482787,
author = {Gupta, Vishakha and Knauerhase, Rob and Brett, Paul and Schwan, Karsten},
title = {Kinship: Efficient Resource Management for Performance and Functionally Asymmetric Platforms},
year = {2013},
isbn = {9781450320535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2482767.2482787},
doi = {10.1145/2482767.2482787},
abstract = {On-chip heterogeneity has become key to balancing performance and power constraints, resulting in disparate (functionally overlapping but not equivalent) cores on a single die. Requiring developers to deal with such heterogeneity can impede adoption through increased programming effort and result in cross-platform incompatibility. We propose that systems software must evolve to dynamically accommodate heterogeneity and to automatically choose task-to-resource mappings to best use these features.We describe the kinship approach for mapping workloads to heterogeneous cores. A hypervisor-level realization of the approach on a variety of experimental heterogeneous platforms demonstrates the general applicability and utility of kinship-based scheduling, matching dynamic workloads to available resources as well as scaling with the number of processes and with different types/configurations of compute resources. Performance advantages of kinship based scheduling are evident for runs across multiple generations of heterogeneous platforms.},
booktitle = {Proceedings of the ACM International Conference on Computing Frontiers},
articleno = {16},
numpages = {10},
keywords = {kinship, functional asymmetry, performance asymmetry, dynamic scheduling},
location = {Ischia, Italy},
series = {CF '13}
}

@inproceedings{10.5555/2486788.2486884,
author = {Kasi, Bakhtiar Khan and Sarma, Anita},
title = {Cassandra: Proactive Conflict Minimization through Optimized Task Scheduling},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Software conflicts arising because of conflicting changes are a regular occurrence and delay projects. The main precept of workspace awareness tools has been to identify potential conflicts early, while changes are still small and easier to resolve. However, in this approach conflicts still occur and require developer time and effort to resolve. We present a novel conflict minimization technique that proactively identifies potential conflicts, encodes them as constraints, and solves the constraint space to recommend a set of conflict-minimal development paths for the team. Here we present a study of four open source projects to characterize the distribution of conflicts and their resolution efforts. We then explain our conflict minimization technique and the design and implementation of this technique in our prototype, Cassandra. We show that Cassandra would have successfully avoided a majority of conflicts in the four open source test subjects. We demonstrate the efficiency of our approach by applying the technique to a simulated set of scenarios with higher than normal incidence of conflicts. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {732–741},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/2662593.2662596,
author = {Shafiei, Nastaran and van Breugel, Franck},
title = {Towards Model Checking of Computer Games with Java PathFinder},
year = {2013},
isbn = {9781467362634},
publisher = {IEEE Press},
abstract = {We show that Java source code of computer games can be checked for bugs such as uncaught exceptions by the model checker Java PathFinder (JPF). To model check Java games, we need to tackle the state space explosion problem and handle native calls. To address those two challenges we use our extensions of JPF, jpf-probabilistic and jpf-nhandler. The former deals with the randomization in the source code of the game, which is a cause of the state space explosion problem. The latter handles native calls automatically. We show how JPF enhanced with our extensions can check games such as the text based game Hamurabi and a graphics based version of rock-paper-scissors.},
booktitle = {Proceedings of the 3rd International Workshop on Games and Software Engineering: Engineering Computer Games to Enable Positive, Progressive Change},
pages = {15–21},
numpages = {7},
location = {San Francisco, California},
series = {GAS '13}
}

@inproceedings{10.5555/2486788.2486927,
author = {Malik, Haroon and Hemmati, Hadi and Hassan, Ahmed E.},
title = {Automatic Detection of Performance Deviations in the Load Testing of Large Scale Systems},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Load testing is one of the means for evaluating the performance of Large Scale Systems (LSS). At the end of a load test, performance analysts must analyze thousands of performance counters from hundreds of machines under test. These performance counters are measures of run-time system properties such as CPU utilization, Disk I/O, memory consumption, and network traffic. Analysts observe counters to find out if the system is meeting its Service Level Agreements (SLAs). In this paper, we present and evaluate one supervised and three unsupervised approaches to help performance analysts to 1) more effectively compare load tests in order to detect performance deviations which may lead to SLA violations, and 2) to provide them with a smaller and manageable set of important performance counters to assist in root-cause analysis of the detected deviations. Our case study is based on load test data obtained from both a large scale industrial system and an open source benchmark application. The case study shows, that our wrapper-based supervised approach, which uses a search-based technique to find the best subset of performance counters and a logistic regression model for deviation prediction, can provide up to 89% reduction in the set of performance counters while detecting performance deviations with few false positives (i.e., 95% average precision). The study also shows that the supervised approach is more stable and effective than the unsupervised approaches but it has more overhead due to its semi-automated training phase. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1012–1021},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/2486788.2486838,
author = {Lewis, Chris and Lin, Zhongpeng and Sadowski, Caitlin and Zhu, Xiaoyan and Ou, Rong and Whitehead Jr., E. James},
title = {Does Bug Prediction Support Human Developers? Findings from a Google Case Study},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {372–381},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/2486788.2486945,
author = {Paasivaara, Maria and Lassenius, Casper and Damian, Daniela and R\"{a}ty, Petteri and Schr\"{o}ter, Adrian},
title = {Teaching Students Global Software Engineering Skills Using Distributed Scrum},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { In this paper we describe distributed Scrum augmented with best practices in global software engineering (GSE) as an important paradigm for teaching critical competencies in GSE. We report on a globally distributed project course between the University of Victoria, Canada and Aalto University, Finland. The project-driven course involved 16 students in Canada and 9 students in Finland, divided into three cross-site Scrum teams working on a single large project. To assess learning of GSE competencies we employed a mixed-method approach including 13 post-course interviews, pre-, post-course and iteration questionnaires, observations, recordings of Daily Scrums as well as collection of project asynchronous communication data. Our analysis indicates that the Scrum method, along with supporting collaboration practices and tools, supports the learning of important GSE competencies, such as distributed communication and teamwork, building and maintaining trust, using appropriate collaboration tools, and inter-cultural collaboration. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1128–1137},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/2486788.2486844,
author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
title = {Boa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {422–431},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/2662528.2662534,
author = {Jain, Shilpi and Babar, Muhammad Ali and Fernandez, Jude},
title = {Conducting Empirical Studies in Industry: Balancing Rigor and Relevance},
year = {2013},
isbn = {9781467362863},
publisher = {IEEE Press},
abstract = {Empirical Software Engineering research has achieved considerable results in building our knowledge about selecting and applying appropriate empirical methods for technology evaluation. Empirical studies in general and empirical studies in industrial settings in particular have played an important role in successful transition of many Software Engineering technologies to industry, for example, defect detection techniques and automated test cases. However, conducting empirical research in industrial settings remains a challenging undertaking for a variety of reasons. There is no substantial literature reporting on the challenges and complexities involved in conducting empirical studies in an industry in general and in settings whose business models are built around global sourcing. This paper reports some of our experiences and lessons learned from conducting empirical research in industry. Some of the observed challenges include short time horizon for research, high expectations, limited research skills, and the 'acceptable' research rigor. The paper discusses some of these issues with relevant examples and provides some strategies for overcoming these issues. We also stress that researchers and practitioners should share their experiences of conducting empirical research in order to help build a body of knowledge to guide the future efforts.},
booktitle = {Proceedings of the 1st International Workshop on Conducting Empirical Studies in Industry},
pages = {9–14},
numpages = {6},
keywords = {evidence based software engineering, empirical studies, field experiments},
location = {San Francisco, California},
series = {CESI '14}
}

@inproceedings{10.1145/2486092.2486130,
author = {Huang, Shell Ying and Hsu, Wen Jing and Fang, Hui and Song, Tiancheng},
title = {A Marine Traffic Simulation System for Hub Ports},
year = {2013},
isbn = {9781450319201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486092.2486130},
doi = {10.1145/2486092.2486130},
abstract = {Ensuring congestion-free marine traffic is crucial for hub ports in the world. For these hub ports, there has been increasing demand in marine transport. Therefore a capacity-assessment tool that models and simulates the navigational network, the traffic flows and complex navigational behaviors of vessels is needed. The simulation model presented in this paper is unique in the scale and complexity of the waterway networks covered, the flexibility in defining the traffic flow patterns, and the degree of accuracy demanded of navigational behaviors. As such, none of the existing models and simulation tools is adequate for assessing the waterway capacity. The model was calibrated based on detailed analysis of historical records and consultations with domain experts. The model was used in the study of several future scenarios of a hob port. The simulation system built has laid a useful foundation for planning future marine traffic for hub ports.},
booktitle = {Proceedings of the 1st ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {295–304},
numpages = {10},
keywords = {marine traffic simulation, capacity assessment, hub ports},
location = {Montr©al, Qu\'{e}bec, Canada},
series = {SIGSIM PADS '13}
}

@inproceedings{10.5555/2662693.2662695,
author = {Dustdar, Schahram and Li, Fei and Truong, Hong-Linh and Sehic, Sanjin and Nastic, Stefan and Qanbari, Soheil and V\"{o}gler, Michael and Clae\ss{}ens, Markus},
title = {Green Software Services: From Requirements to Business Models},
year = {2013},
isbn = {9781467362672},
publisher = {IEEE Press},
abstract = {In recent years, green software research is gaining momentum because of the acute need for sustainable development. Most past research has been focused on the definitions, metrics and technical solutions for green software, but few has addressed green software from the business perspective. In this paper, we present the analysis on three key elements of Green Software Services (GSS)---stakeholders, their requirements, and business models. The stakeholders of GSS are detailed with the services each stakeholder can provide and consume, thus clarifying their interests to GSS. Based on this analysis, we present the domain-independent, high-level requirements to GSS that cover diverse needs of different stakeholders. Six business models are then proposed to promote collaborations of stakeholders on the delivery of GSS. In the end, the relationship between GSS and cloud is discussed and a GSS marketplace is envisioned.},
booktitle = {Proceedings of the 2nd International Workshop on Green and Sustainable Software},
pages = {1–7},
numpages = {7},
location = {San Francisco, California},
series = {GREENS '13}
}

@inproceedings{10.5555/2662693.2662696,
author = {Menarini, Massimiliano and Seracini, Filippo and Zhang, Xiang and Rosing, Tajana and Kr\"{u}ger, Ingolf},
title = {Green Web Services: Improving Energy Efficiency in Data Centers via Workload Predictions},
year = {2013},
isbn = {9781467362672},
publisher = {IEEE Press},
abstract = {Improving energy efficiency of data centers is an important research challenge. Web services are an important part of data centers' workload, and a large contributor to their energy footprint. This paper contributes an approach that, leveraging statistical data over web services usage patterns, dynamically predicts the resources required by the web service application. Our framework, SOPRA, uses these predictions to constantly adapt the allocation of resources to minimize the energy utilization of the data center. We demonstrate the viability of our approach by executing SOPRA over a synthetic workload. We compare the energy savings achieved by SOPRA with the traditional over allocation strategy and with the saving achievable by using a static predictor. Furthermore, we show how different service level agreements (SLA) influence the ability to save energy. The results of our experiments show that, with our workload, we can save up to 52.49% of energy over the over-allocation approach while a static prediction can only achieve a 44.78% saving. Moreover, our results show that the SLA has a high impact on energy savings. Using a more demanding SLA, the energy saving SOPRA was able to achieve was only 28.29%.},
booktitle = {Proceedings of the 2nd International Workshop on Green and Sustainable Software},
pages = {8–15},
numpages = {8},
keywords = {service level agreements, web services, energy efficiency, proactive resource adaptation, data centers},
location = {San Francisco, California},
series = {GREENS '13}
}

@inproceedings{10.5555/2663546.2663555,
author = {Gambi, Alessio and Moldovan, Daniel and Copil, Georgiana and Truong, Hong-Linh and Dustdar, Schahram},
title = {On Estimating Actuation Delays in Elastic Computing Systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {Elastic controllers autonomically adjust the allocation of resources in cloud computing systems. Usually such controllers assume that control actions will take immediate effect. In clouds, however, actuation times may be long, and the controllers can hardly guarantee acceptable levels of service if they neglect these actuation delays. Therefore, the ability to correctly estimate the time that control actions take effect on the systems is crucial. However, detecting actuation delays in elastic computing systems is challenging because cloud systems provide only inaccurate and incomplete data about reconfigurations timing.In this paper, we tackle the problem of estimating the delay of control actions in elastic systems. We identify recurring types of changes in the monitored metrics and requirements to properly carry out the estimation. Based on that, we develop a novel framework for the actuation delays estimation that utilizes change point detection techniques. We conduct several experiments with real-world systems to illustrate the feasibility and applicability of our framework.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {33–42},
numpages = {10},
keywords = {elasticity, system identification, change point detection, cloud},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.5555/2662693.2662701,
author = {Corral, Luis and Georgiev, Anton B. and Sillitti, Alberto and Succi, Giancarlo},
title = {A Method for Characterizing Energy Consumption in Android Smartphones},
year = {2013},
isbn = {9781467362672},
publisher = {IEEE Press},
abstract = {Cellular phones and tablets are ubiquitous, with a market penetration that is counted in millions of active users and units sold. The increasing computing capabilities and strict autonomy requirements on mobile devices drive a particular concern on energy utilization and optimization of this kind of equipment. In this paper, we investigate an approach to relate the energy consumption of smartphones with the operational status of the device, surveying parameters exposed by the operating system using an Android application. Our goal is to explore the means to expand the information that may help to produce more reliable measurements that can be used in further research for designing energy optimization profiles for mobile devices and identify optimization needs.},
booktitle = {Proceedings of the 2nd International Workshop on Green and Sustainable Software},
pages = {38–45},
numpages = {8},
keywords = {mobile, applications, measurement, smartphone, power, Android, energy},
location = {San Francisco, California},
series = {GREENS '13}
}

@inproceedings{10.1145/2487166.2487182,
author = {Tu, Jinlong and Lu, Lian and Chen, Minghua and Sitaraman, Ramesh K.},
title = {Dynamic Provisioning in Next-Generation Data Centers with on-Site Power Production},
year = {2013},
isbn = {9781450320528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487166.2487182},
doi = {10.1145/2487166.2487182},
abstract = {The critical need for clean and economical sources of energy is transforming data centers that are primarily energy consumers to also energy producers. We focus on minimizing the operating costs of next-generation data centers that can jointly optimize the energy supply from on-site generators and the power grid, and the energy demand from servers as well as power conditioning and cooling systems. We formulate the cost minimization problem and present an offline optimal algorithm. For "on-grid" data centers that use only the grid, we devise a deterministic online algorithm that achieves the best possible competitive ratio of 2αs, where αs is a normalized look-ahead window size. The competitive ratio of an online algorithm is defined as the maximum ratio (over all possible inputs) between the algorithm's cost (with no or limited look-ahead) and the offline optimal assuming complete future information. We remark that the results hold as long as the overall energy demand (including server, cooling, and power conditioning) is a convex and increasing function in the total number of active servers and also in the total server load. For "hybrid" data centers that have on-site power generation in addition to the grid, we develop an online algorithm that achieves a competitive ratio of at most Pmax(2--αs)/co+cm/L [1+2 Pmax--co/Pmax(1+αg], where αs and αg are normalized look-ahead window sizes, Pmax is the maximum grid power price, and L, co, and cm are parameters of an on-site generator.Using extensive workload traces from Akamai with the corresponding grid power prices, we simulate our offline and online algorithms in a realistic setting. Our offline (resp., online) algorithm achieves a cost reduction of 25.8% (resp., 20.7%) for a hybrid data center and 12.3% (resp., 7.3%) for an on-grid data center. The cost reductions are quite significant and make a strong case for a joint optimization of energy supply and energy demand in a data center. A hybrid data center provides about 13% additional cost reduction over an on-grid data center representing the additional cost benefits that on-site power generation provides over using the grid alone.},
booktitle = {Proceedings of the Fourth International Conference on Future Energy Systems},
pages = {137–148},
numpages = {12},
keywords = {dynamic provisioning, data centers, online algorithm, on-site power production},
location = {Berkeley, California, USA},
series = {e-Energy '13}
}

@inproceedings{10.1145/2487166.2487176,
author = {Blunck, Henrik and Bouvin, Niels Olof and Mose Entwistle, Johanne and Gr\o{}nb\ae{}k, Kaj and Kj\ae{}rgaard, Mikkel B. and Nielsen, Matthias and Graves Petersen, Marianne and Rasmussen, Majken K. and W\"{u}stenberg, Markus},
title = {Computational Environmental Ethnography: Combining Collective Sensing and Ethnographic Inquiries to Advance Means for Reducing Environmental Footprints},
year = {2013},
isbn = {9781450320528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487166.2487176},
doi = {10.1145/2487166.2487176},
abstract = {We lack an understanding of human values, motivations and behavior in regards to new means for changing people's behavior towards more sustainable choices in their everyday life. Previous anthropological and sociological studies have identified these objects of study to be quite complex and to require new methods to be unfolded further. Especially behavior within the privacy of people's homes has proven challenging to uncover through the use of traditional qualitative and quantitative social scientific methods (e.g. interviews, participatory observations and questionnaires). Furthermore, many research experiments are attempting to motivate environmental improvements through feedback via, e.g., room displays, web pages or smart phones, based on (smart) metering of energy usage, or for saving energy by automatic control of, e.g., heating, lighting or appliances. However, existing evaluation methods are primarily unilateral by opting for either a quantitative or a qualitative method or for a simple combination and therefore do not provide detailed insight into the potentials and impacts of such solutions. This paper therefore proposes a combined quantitative and qualitative collective sensing and anthropologic investigation methodology we term Computational Environmental Ethnography, which provides quantitative sensing data that document behavior while facilitating qualitative investigations to link the data to explanations and ideas for further sensing. We propose this methodology to include the establishment of base lines, comparative experimental feedback, traceable sensor data with respect for different privacy levels, visualization of sensor data, qualitative explanations of recurrent and exceptional patterns in sensor data, taking place as part of an innovative process and in an iterative interplay among complementing disciplines, potentially including also partners from industry. Experiences from using the methodology in a zero-emission home setting, as well as an ongoing case investigating transportation habits are discussed.},
booktitle = {Proceedings of the Fourth International Conference on Future Energy Systems},
pages = {87–98},
numpages = {12},
keywords = {collective sensing, environmental behavior, smart control, anthropology, eco-feedback, evaluation methods},
location = {Berkeley, California, USA},
series = {e-Energy '13}
}

@inproceedings{10.1145/2487166.2487170,
author = {Ghai, Sunil Kumar and Charbiwala, Zainul and Mylavarapu, Swarnalatha and Seetharamakrishnan, Deva P. and Kunnath, Rajesh},
title = {DC Picogrids: A Case for Local Energy Storage for Uninterrupted Power to DC Appliances},
year = {2013},
isbn = {9781450320528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487166.2487170},
doi = {10.1145/2487166.2487170},
abstract = {An increasing number of appliances now operate on DC and providing uninterrupted power supply (UPS) to them through outages requires two conversions: first from an energy store, typically a DC battery, to AC mains and then from AC mains to the DC input required by the appliance. The energy storage and DC-to-AC inversion are usually centrally located and tied to existing AC distribution lines to amortize costs and battery capacity. In this paper, we argue that adding energy storage locally to each DC appliance and managing it intelligently can lead to higher efficiency and lower average cost. We term this topology a DC picogrid as it mimics a scaled down independent microgrid. Our contribution is the design and evaluation of a smart picogrid controller that a) identifies the power source and b) decides on battery charging or discharging based on the power source. As we expect DC picogrids to co-exist with AC UPSes, we must ensure that the DC picogrid does not draw power from the UPS's battery but charges from the macrogrid when available. To accomplish this, we exploit the fact that AC distribution from the macrogrid exhibits sufficiently distinct characteristics compared to an AC UPS or a diesel generator. Our picogrid controller uses a Hidden Markov Model for state estimation that uses temporally correlated fluctuations in line voltage and frequency for discrimination. We show through data from four settings that the controller can identify its supply source with over 90% accuracy, and that efficiency recovered from conversion losses could result in 30% reduction in energy consumption.},
booktitle = {Proceedings of the Fourth International Conference on Future Energy Systems},
pages = {27–38},
numpages = {12},
keywords = {conversion losses, DC picogrid, power source identification},
location = {Berkeley, California, USA},
series = {e-Energy '13}
}

@article{10.1145/2463579.2463581,
author = {Oliveira, Rodrigo De and Cherubini, Mauro and Oliver, Nuria},
title = {Influence of Personality on Satisfaction with Mobile Phone Services},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/2463579.2463581},
doi = {10.1145/2463579.2463581},
abstract = {We propose a conceptual model that explains the relationship between the users' personality profile and their satisfaction with basic mobile phone services (calls, messages, and simple GPRS/3G services). The model captures direct and indirect effects on satisfaction by means of two variables: actual mobile phone usage and perceived usability of the related services. We empirically validate the model with data gathered from 603 customers of a telecommunication operator, and find that: (1) extroversion, conscientiousness, and intellect have a significant impact on customer satisfaction—positively for the first two traits and negatively for the latter; (2) extroversion positively influences mobile phone usage; and (3) extroversion and conscientiousness positively influence the users' perceived usability of mobile services. Interestingly, usability has the strongest positive impact on satisfaction, whereas mobile phone usage has a negative impact on satisfaction. We discuss key findings of this model and propose several implications for the design of mobile phone services.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = may,
articleno = {10},
numpages = {23},
keywords = {structural equation modeling, Big five, usability}
}

@inproceedings{10.1145/2504335.2504353,
author = {Huang, Ming-Chun and Xu, Wenyao and Liu, Jason and Samy, Lauren and Vajid, Amir and Alshurafa, Nabil and Sarrafzadeh, Majid},
title = {Inconspicuous On-Bed Respiratory Rate Monitoring},
year = {2013},
isbn = {9781450319737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2504335.2504353},
doi = {10.1145/2504335.2504353},
abstract = {The monitoring of human respiratory rate is essential in many clinical applications including the detection and monitoring of sleep disorders, the monitoring of newborns for Sudden Infant Death Syndrome (SIDS), and identifying patients at high risk up to 24 hours before an adverse event like stroke and cardiac arrest [1]. Traditional noninvasive respiratory rate measurements in a hospital setting rely on clinical staff to visually track a patient's chest movement for a period of time to derive the respiratory rate from the number of movements observed. Failure to perform continuous and quantified measurements of respiratory rate could result in an inability to rescue a patient exhibiting respiratory distress. Severe after effects hinder recovery and result in loss of time, cost, or even life. This paper proposes an e-textile pressure sensitive bed sheet to non-invasively and accurately measure respiratory rate by analyzing time-stamped pressure distribution sequences. The bed sheet provides a 24/7 quantified on-bed respiratory rate monitoring service. It is made of e-textile and is similar to a regular bed sheet in comfort. As a result, it can seamlessly fit in common clinical or home environments, reducing the possible interference with a patient's regular sleeping habits and resulting in a type of inconspicuous monitoring.},
booktitle = {Proceedings of the 6th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {18},
numpages = {8},
keywords = {breath, respiratory rate, bedsheet, inconspicuous, pressure sensor array},
location = {Rhodes, Greece},
series = {PETRA '13}
}

@inproceedings{10.1145/2504335.2504407,
author = {Bieber, Gerald and Haescher, Marian and Vahl, Matthias},
title = {Sensor Requirements for Activity Recognition on Smart Watches},
year = {2013},
isbn = {9781450319737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2504335.2504407},
doi = {10.1145/2504335.2504407},
abstract = {The new generation of watches is smart. Smart watches are connected to the internet and provide sensor functionality that allows an enhanced human-computer-interaction. Smart watches provide a gesture interaction and a permanent monitoring of physical activities. In comparison to other electronic home consumer devices with integrated sensors, Smart watches provide monitoring data for 24h per day, many watches are water resistant and can be worn constantly. The integrated sensors are varying in performance and are not intended to distinguish between different states of activity and inactivity. This paper reports on identified requirements on sensors of smart watches for detection of activity, inactivity as well as sleep detection. Hereby a new measurement quantity is introduced and applications of heart beat detection or wearing situation are presented.},
booktitle = {Proceedings of the 6th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {67},
numpages = {6},
keywords = {watch, mobile computing, acceleration, recognition, smart watch, activity monitoring, sleep, quantify self, smart, wrist, sensor, user interfaces, inactivity},
location = {Rhodes, Greece},
series = {PETRA '13}
}

@inproceedings{10.1145/2504335.2504346,
author = {Mehner, Stefan and Klauck, Ronny and Koenig, Hartmut},
title = {Location-Independent Fall Detection with Smartphone},
year = {2013},
isbn = {9781450319737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2504335.2504346},
doi = {10.1145/2504335.2504346},
abstract = {Due to demographic changes in developed industrial countries and a better medical care system, the number of elderly people who still live in their home environment is rapidly growing because there they feel more comfortable and independent as in a clinical environment or in a residential care home. The elderly often live alone and receive only irregular visits. Due to impaired physical skills the probability of falls significantly increases. The detection of falls is a crucial aspect in the care of elderly. Falls are often detected very late with severe consequential damages. There are existing approaches for automatic fall detection. They usually deploy special external devices. Elderly people often do not accept these devices because they expose their frailty. In this paper, we present a location-independent fall detection method implemented as a smartphone application for an inconspicuous use in nearly every situation of the daily life. The difficulty of our approach is in the low resolution range of integrated acceleration sensors and the limited energy supply of the smartphone. As solution, we apply a modular threshold-based algorithm which uses the acceleration sensor with moderate energy consumption. Its fall detection rate is in the average of current relevant research.},
booktitle = {Proceedings of the 6th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {11},
numpages = {8},
keywords = {accelerometer, smartphone, fall detection},
location = {Rhodes, Greece},
series = {PETRA '13}
}

@inproceedings{10.1145/2463209.2488854,
author = {Park, Sangyoung and Kim, Younghyun and Chang, Naehyuck},
title = {Hybrid Energy Storage Systems and Battery Management for Electric Vehicles},
year = {2013},
isbn = {9781450320719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463209.2488854},
doi = {10.1145/2463209.2488854},
abstract = {Electric vehicles (EV) are considered as a strong alternative of internal combustion engine vehicles expecting lower carbon emission. However, their actual benefits are not yet clearly verified while the energy efficiency can be improved in many ways. The carbon emission benefits from EV is largely diminished if we charge EV with electricity from petroleum power plants due to power loss during generation, transmission, conversion and charging. On the other hand, regenerative braking is direct power conversion from the wheel to battery and one of the most important processes that can enhance energy efficiency of EV. Power loss during regenerative braking can be reduced by hybrid energy storage system (HESS) such that supercapacitors accept high power as batteries have small rate capability.Conventional charge management does not systematically exchange charge between the supercapacitor and battery. However, asymmetry in acceleration and deceleration as well as battery charging and discharging capability make the supercapacitor state of charge (SoC) management override the efficiency optimization. Unlike previous works, we show how charge migration during idle and cruise/stopping time can be beneficial in terms of energy efficiency and cruise range. Systematic charge migration decouples SoC management and charging efficiency optimization giving a higher degree of freedom to charging efficiency optimization. We demonstrate the proposed charge migration between the supercapacitor and battery improves energy efficiency by 19.4%.},
booktitle = {Proceedings of the 50th Annual Design Automation Conference},
articleno = {97},
numpages = {6},
keywords = {regenerative braking, electric vehicle, charging/discharging asymmetry, battery-supercapacitor hybrid},
location = {Austin, Texas},
series = {DAC '13}
}

@article{10.1145/2505420.2505424,
author = {Leit\~{a}o, Lu\'{\i}s and Calado, P\'{a}vel},
title = {An Automatic Blocking Strategy for XML Duplicate Detection},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2505420.2505424},
doi = {10.1145/2505420.2505424},
abstract = {Duplicate detection consists in finding objects that, although having different representations in a database, correspond to the same real world entity. This is typically achieved by comparing all objects to each other, which can be unfeasible for large datasets. Blocking strategies have been devised to reduce the number of objects to compare, at the cost of loosing some duplicates. However, these strategies typically rely on user knowledge to discover a set of parameters that optimize the comparisons, while minimizing the loss. Also, they do not usually optimize the comparison between each pair of objects. In this paper, we propose a blocking method of combining two optimization strategies: one to select which objects to compare and another to optimize pair-wise object comparisons. In addition, we propose a machine learning approach to determine the required parameters, without the need of user intervention. Experiments performed on several datasets show that not only we are able to effectively determine the optimization parameters, but also to significantly improve efficiency, while maintaining an acceptable loss of recall.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jun,
pages = {42–53},
numpages = {12},
keywords = {XML, blocking, data quality, duplicate detection, optimization}
}

@article{10.1145/2518140.2518141,
title = {Open Source Column: OpenIMAJ - Intelligent Multimedia Analysis in Java},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/2518140.2518141},
doi = {10.1145/2518140.2518141},
abstract = {Multimedia analysis is an exciting and fast-moving research area. Unfortunately, historically there has been a lack of software solutions in a common programming language for performing scalable integrated analysis of all modalities of media (images, videos, audio, text, web-pages, etc). For example, in the image analysis world, OpenCV and Matlab are commonly used by researchers, whilst many common Natural Language Processing tools are built using Java. The lack of coherency between these tools and languages means that it is often difficult to research and develop rational, comprehensible and repeatable software implementations of algorithms for performing multimodal multimedia analysis. These problems are also exacerbated by the lack of any principled software engineering (separation of concerns, minimised code repetition, maintainability, understandability, premature optimisation and over optimisation) often found in research code. OpenIMAJ is a set of libraries and tools for multimedia content analysis and content generation that aims to fill this gap and address the concerns. OpenIMAJ provides a coherent interface to a very broad range of techniques, and contains everything from stateof- the-art computer vision (e.g. SIFT descriptors, salient region detection, face detection and description, etc.) and advanced data clustering and hashing, through to software that performs analysis on the content, layout and structure of webpages. A full list of the all the modules and an overview of their functionalities for the latest OpenIMAJ release can be found here. OpenIMAJ is primarily written in Java and, as such is completely platform independent. The video-capture and hardware libraries contain some native code but Linux, OSX and Windows are supported out of the box (under both 32 and 64 bit JVMs; ARM processors are also supported under Linux). It is possible to write programs that use the libraries in any JVM language that supports Java interoperability, for example Groovy and Scala. OpenIMAJ can even be run on Android phones and tablets. As it's written using Java, you can run any application built using OpenIMAJ on any of the supported platforms without even having to recompile the code.},
journal = {SIGMultimedia Rec.},
month = jun,
pages = {3–8},
numpages = {6}
}

@inproceedings{10.1145/2488608.2488655,
author = {Thorup, Mikkel},
title = {Bottom-k and Priority Sampling, Set Similarity and Subset Sums with Minimal Independence},
year = {2013},
isbn = {9781450320290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488608.2488655},
doi = {10.1145/2488608.2488655},
abstract = {We consider bottom-k sampling for a set X, picking a sample Sk(X) consisting of the k elements that are smallest according to a given hash function h. With this sample we can estimate the relative size f=|Y|/|X| of any subset Y as |Sk(X) intersect Y|/k. A standard application is the estimation of the Jaccard similarity f=|A intersect B|/|A union B| between sets A and B. Given the bottom-k samples from A and B, we construct the bottom-k sample of their union as Sk(A union B)=Sk(Sk(A) union Sk(B)), and then the similarity is estimated as |Sk(A union B) intersect Sk(A) intersect Sk(B)|/k.We show here that even if the hash function is only 2-independent, the expected relative error is O(1√(fk)). For fk=Omega(1) this is within a constant factor of the expected relative error with truly random hashing.For comparison, consider the classic approach of kxmin-wise where we use k hash independent functions h1,...,hk, storing the smallest element with each hash function. For kxmin-wise there is an at least constant bias with constant independence, and it is not reduced with larger k. Recently Feigenblat et al. showed that bottom-k circumvents the bias if the hash function is 8-independent and k is sufficiently large. We get down to 2-independence for any k. Our result is based on a simply union bound, transferring generic concentration bounds for the hashing scheme to the bottom-k sample, e.g., getting stronger probability error bounds with higher independence.For weighted sets, we consider priority sampling which adapts efficiently to the concrete input weights, e.g., benefiting strongly from heavy-tailed input. This time, the analysis is much more involved, but again we show that generic concentration bounds can be applied.},
booktitle = {Proceedings of the Forty-Fifth Annual ACM Symposium on Theory of Computing},
pages = {371–380},
numpages = {10},
keywords = {independence, estimation, sampling},
location = {Palo Alto, California, USA},
series = {STOC '13}
}

@inproceedings{10.1145/2491568.2491572,
author = {Alt, Florian and Schneegass, Stefan and Girgis, Michael and Schmidt, Albrecht},
title = {Cognitive Effects of Interactive Public Display Applications},
year = {2013},
isbn = {9781450320962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491568.2491572},
doi = {10.1145/2491568.2491572},
abstract = {Many public displays are nowadays equipped with different types of sensors. Such displays allow engaging and persistent user experiences to be created, e.g., in the form of gesture-controlled games or content exploration using direct touch at the display. However, as digital displays replace traditional posters and billboards, display owners are reluctant to deploy interactive content, but rather adapt traditional, non-interactive content. The main reason is, that the benefit of such interactive deployments is not obvious. Our hypothesis is that interactivity has a cognitive effect on users and therefore increases the ability to remember what they have seen on the screen -- which is beneficial both for the display owner and the user. In this paper we systematically investigate the impact of interactive content on public displays on the users' cognition in different situations. Our findings indicate that overall memorability is positively affected as users interact. Based on these findings we discuss design implications for interactive public displays.},
booktitle = {Proceedings of the 2nd ACM International Symposium on Pervasive Displays},
pages = {13–18},
numpages = {6},
keywords = {digital signage, public display, interactivity, recognition, recall},
location = {Mountain View, California},
series = {PerDis '13}
}

@inproceedings{10.1145/2466715.2466733,
author = {Romero, Andr\'{e}s and Gouiff\`{e}s, Mich\`{e}le and Lacassagne, Lionel},
title = {Enhanced Local Binary Covariance Matrices (ELBCM) for Texture Analysis and Object Tracking},
year = {2013},
isbn = {9781450320238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2466715.2466733},
doi = {10.1145/2466715.2466733},
abstract = {This paper presents a novel way to embed local binary texture information in the form of local binary patterns (LBP) into the covariance descriptor. Contrary to previous publications, our method is not based on the LBP decimal values where arithmetic operations have no texture meaning. Our method uses the angles described by the uniform LBP patterns and includes them into the set of features used to build the covariance descriptor. Our representation is not only more compact but more robust because it is less affected by noise and small neighborhood rotations. Experimental evaluations corroborate the performance of our descriptor for texture analysis and tracking applications. Our descriptor rivals with state-of-the-art methods and beats other covariance-based descriptors.},
booktitle = {Proceedings of the 6th International Conference on Computer Vision / Computer Graphics Collaboration Techniques and Applications},
articleno = {10},
numpages = {8},
keywords = {local log-euclidean, LBP, Riemannian manifolds, covariance descriptors},
location = {Berlin, Germany},
series = {MIRAGE '13}
}

@inproceedings{10.1145/2514601.2514611,
author = {Lauritsen, Marc},
title = {On Balance},
year = {2013},
isbn = {9781450320801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2514601.2514611},
doi = {10.1145/2514601.2514611},
abstract = {In the course of legal reasoning -- whether for purposes of deciding an issue, justifying a decision, predicting how an issue will be decided, or arguing for how it should be decided -- one often is required to reach (and assert) conclusions based on a balance of reasons that is not straightforwardly reducible to the application of rules. Recent AI &amp; Law work has modeled reason-balancing, both within and across cases, with set-theoretic and rule- or value-ordering approaches. This article explores how modeling it in 'choiceboxing' terms may yield new questions, insights, and tools.},
booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law},
pages = {83–91},
numpages = {9},
location = {Rome, Italy},
series = {ICAIL '13}
}

@inproceedings{10.1145/2479787.2479806,
author = {Hu, Yuh-Jong and Cheng, Kua-Ping and Huang, Ya-Ling},
title = {Crafting a Balance between Big Data Utility and Protection in the Semantic Data Cloud},
year = {2013},
isbn = {9781450318501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479787.2479806},
doi = {10.1145/2479787.2479806},
abstract = {Structured big data of Personal Identifiable Information (PII) are acquired from everywhere and stored as microdata in a statistical database. Given a statistical disclosure control method, big data analysis and protection are enacted for outsourcing data sources. We flexibly glean the data utility to achieve effective data-driven decision-making. However, we still comply with the privacy protection principles while applying data analysis. In this paper, we propose three types of semantics-enabled policies for controlling access, handling data, and releasing data to craft a balance between data utility and protection. Structured big data are tagged with semantic metadata to enable semantics-enabled policy's direct processing and interpretation. Finally, we demonstrate how to craft a balance between data utility and protection with these types of semantics-enabled policies, combined with various statistical disclosure control methods.},
booktitle = {Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics},
articleno = {18},
numpages = {12},
keywords = {data utility, world wide web, data protection, statistical disclosure control, big data, semantics-enabled policy, semantic data cloud},
location = {Madrid, Spain},
series = {WIMS '13}
}

@inproceedings{10.1145/2462410.2462411,
author = {Baracaldo, Nathalie and Joshi, James},
title = {Beyond Accountability: Using Obligations to Reduce Risk Exposure and Deter Insider Attacks},
year = {2013},
isbn = {9781450319508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462410.2462411},
doi = {10.1145/2462410.2462411},
abstract = {Recently, the importance of including obligations as part of access control systems for privilege management, for example, in healthcare information systems, has been well recognized. In an access control system, an a posteriori obligation states which actions need to be performed by a user after he has accessed a resource. There is no guarantee that a user will fulfill a posteriori obligations. Not fulfilling these obligations may incur financial loss, or loss of goodwill and productivity to the organization. In this paper, we propose a trust-and-obligation based framework that reduces the risk exposure of an organization associated with a posteriori obligations. We propose a methodology to assign trust values to users to indicate how trustworthy they are with regards to fulfilling their obligations. When access requests that trigger a posteriori obligations are evaluated, the requesting users' trust values and the criticality of the associated obligations are used. Our framework detects and mitigates insider attacks and unintentional damages that may result from violating a posteriori obligations. Our framework also provides mechanisms to determine misconfigurations of obligation policies. We evaluate our framework through simulations and demonstrate its effectiveness.},
booktitle = {Proceedings of the 18th ACM Symposium on Access Control Models and Technologies},
pages = {213–224},
numpages = {12},
keywords = {access control, obligations, RBAC, insider threat, trust, risk},
location = {Amsterdam, The Netherlands},
series = {SACMAT '13}
}

@inproceedings{10.1145/2491956.2462179,
author = {Sankaranarayanan, Sriram and Chakarov, Aleksandar and Gulwani, Sumit},
title = {Static Analysis for Probabilistic Programs: Inferring Whole Program Properties from Finitely Many Paths},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462179},
doi = {10.1145/2491956.2462179},
abstract = {We propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyber-physical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volume-bound computations. Each path yields interval bounds that can be summed up with a "coverage" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {447–458},
numpages = {12},
keywords = {volume bounding, program verification, monte-carlo sampling, symbolic execution, probabilistic programming},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@article{10.1145/2499370.2462179,
author = {Sankaranarayanan, Sriram and Chakarov, Aleksandar and Gulwani, Sumit},
title = {Static Analysis for Probabilistic Programs: Inferring Whole Program Properties from Finitely Many Paths},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2499370.2462179},
doi = {10.1145/2499370.2462179},
abstract = {We propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyber-physical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volume-bound computations. Each path yields interval bounds that can be summed up with a "coverage" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.},
journal = {SIGPLAN Not.},
month = jun,
pages = {447–458},
numpages = {12},
keywords = {volume bounding, symbolic execution, program verification, probabilistic programming, monte-carlo sampling}
}

@inproceedings{10.1145/2479724.2479730,
author = {Bertot, John Carlo and Choi, Heeyoon},
title = {Big Data and E-Government: Issues, Policies, and Recommendations},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479730},
doi = {10.1145/2479724.2479730},
abstract = {The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From "smart" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {1–10},
numpages = {10},
keywords = {open government, big data},
location = {Quebec, Canada},
series = {dg.o '13}
}

@inproceedings{10.1145/2465848.2465849,
author = {Dede, Elif and Govindaraju, Madhusudhan and Gunter, Daniel and Canon, Richard Shane and Ramakrishnan, Lavanya},
title = {Performance Evaluation of a MongoDB and Hadoop Platform for Scientific Data Analysis},
year = {2013},
isbn = {9781450319799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465848.2465849},
doi = {10.1145/2465848.2465849},
abstract = {Scientific facilities such as the Advanced Light Source (ALS) and Joint Genome Institute and projects such as the Materials Project have an increasing need to capture, store, and analyze dynamic semi-structured data and metadata. A similar growth of semi-structured data within large Internet service providers has led to the creation of NoSQL data stores for scalable indexing and MapReduce for scalable parallel analysis. MapReduce and NoSQL stores have been applied to scientific data. Hadoop, the most popular open source implementation of MapReduce, has been evaluated, utilized and modified for addressing the needs of different scientific analysis problems. ALS and the Materials Project are using MongoDB, a document oriented NoSQL store. However, there is a limited understanding of the performance trade-offs of using these two technologies together.In this paper we evaluate the performance, scalability and fault-tolerance of using MongoDB with Hadoop, towards the goal of identifying the right software environment for scientific data analysis.},
booktitle = {Proceedings of the 4th ACM Workshop on Scientific Cloud Computing},
pages = {13–20},
numpages = {8},
keywords = {distributed computing, MongoDB, MapReduce, NoSQL, scientific computing, Hadoop},
location = {New York, New York, USA},
series = {Science Cloud '13}
}

@inproceedings{10.1145/2465529.2465551,
author = {Lu, Lian and Tu, Jinlong and Chau, Chi-Kin and Chen, Minghua and Lin, Xiaojun},
title = {Online Energy Generation Scheduling for Microgrids with Intermittent Energy Sources and Co-Generation},
year = {2013},
isbn = {9781450319003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465529.2465551},
doi = {10.1145/2465529.2465551},
abstract = {Microgrids represent an emerging paradigm of future electric power systems that can utilize both distributed and centralized generations. Two recent trends in microgrids are the integration of local renewable energy sources (such as wind farms) and the use of co-generation (i.e., to supply both electricity and heat). However, these trends also bring unprecedented challenges to the design of intelligent control strategies for microgrids. Traditional generation scheduling paradigms rely on perfect prediction of future electricity supply and demand. They are no longer applicable to microgrids with unpredictable renewable energy supply and with co-generation (that needs to consider both electricity and heat demand). In this paper, we study online algorithms for the microgrid generation scheduling problem with intermittent renewable energy sources and co-generation, with the goal of maximizing the cost-savings with local generation. Based on the insights from the structure of the offline optimal solution, we propose a class of competitive online algorithms, called CHASE (Competitive Heuristic Algorithm for Scheduling Energy-generation), that track the offline optimal in an online fashion. Under typical settings, we show that CHASE achieves the best competitive ratio among all deterministic online algorithms, and the ratio is no larger than a small constant 3. We also extend our algorithms to intelligently leverage on limited prediction of the future, such as near-term demand or wind forecast. By extensive empirical evaluations using real-world traces, we show that our proposed algorithms can achieve near offline-optimal performance. In a representative scenario, CHASE leads to around 20% cost reduction with no future look-ahead, and the cost reduction increases with the future look-ahead window.},
booktitle = {Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems},
pages = {53–66},
numpages = {14},
keywords = {combined heat and power generation, online algorithm, energy generation scheduling, microgrids},
location = {Pittsburgh, PA, USA},
series = {SIGMETRICS '13}
}

@article{10.1145/2494232.2465551,
author = {Lu, Lian and Tu, Jinlong and Chau, Chi-Kin and Chen, Minghua and Lin, Xiaojun},
title = {Online Energy Generation Scheduling for Microgrids with Intermittent Energy Sources and Co-Generation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2494232.2465551},
doi = {10.1145/2494232.2465551},
abstract = {Microgrids represent an emerging paradigm of future electric power systems that can utilize both distributed and centralized generations. Two recent trends in microgrids are the integration of local renewable energy sources (such as wind farms) and the use of co-generation (i.e., to supply both electricity and heat). However, these trends also bring unprecedented challenges to the design of intelligent control strategies for microgrids. Traditional generation scheduling paradigms rely on perfect prediction of future electricity supply and demand. They are no longer applicable to microgrids with unpredictable renewable energy supply and with co-generation (that needs to consider both electricity and heat demand). In this paper, we study online algorithms for the microgrid generation scheduling problem with intermittent renewable energy sources and co-generation, with the goal of maximizing the cost-savings with local generation. Based on the insights from the structure of the offline optimal solution, we propose a class of competitive online algorithms, called CHASE (Competitive Heuristic Algorithm for Scheduling Energy-generation), that track the offline optimal in an online fashion. Under typical settings, we show that CHASE achieves the best competitive ratio among all deterministic online algorithms, and the ratio is no larger than a small constant 3. We also extend our algorithms to intelligently leverage on limited prediction of the future, such as near-term demand or wind forecast. By extensive empirical evaluations using real-world traces, we show that our proposed algorithms can achieve near offline-optimal performance. In a representative scenario, CHASE leads to around 20% cost reduction with no future look-ahead, and the cost reduction increases with the future look-ahead window.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {53–66},
numpages = {14},
keywords = {energy generation scheduling, combined heat and power generation, online algorithm, microgrids}
}

@inproceedings{10.1145/2479724.2479738,
author = {Daniel, Sylvie and Doran, Marie-Andree},
title = {GeoSmartCity: Geomatics Contribution to the Smart City},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479738},
doi = {10.1145/2479724.2479738},
abstract = {Information and Communication Technologies (ICTs) revolutionize the ways different urban actors communicate and interact. Geographic information technologies are of key importance too for the deployment and implementation of ICTs in the Smart City, because of the central role they may play as decision-making support tools. Indeed, they give quick access to different layers of information that may be combined and integrated to facilitate analysis of a situation and make the best decisions. However, such a central role is rarely acknowledged. In this paper, we will tackle the transversal involvement of geomatics in a Smart City. We propose also to define the extent of the concept of Smart Cities and some of the distinctive features that it should display to support its sustainability.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {65–71},
numpages = {7},
keywords = {geospatial information, smart city model, geospatial intelligence},
location = {Quebec, Canada},
series = {dg.o '13}
}

@inproceedings{10.1145/2479724.2479748,
author = {Kaschesky, Michael and Selmi, Luigi},
title = {Fusepool R5 Linked Data Framework: Concepts, Methodologies, and Tools for Linked Data},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479748},
doi = {10.1145/2479724.2479748},
abstract = {The work reported here was done within the applied research project Fusepool. Fusepool is an international project to develop a set of integrated software components to ease the publishing of open data based on an open-source Linked Data Platform and a set of associated best practices. This article provides researchers and laypersons interested in Linked Open Government Data an overview of the concepts, methodologies, and tools. It introduces the main concepts, such as open government and open government data along with enabling technological approaches such as the semantic web and linked data. It then presents the methodologies for publishing Linked Open Government Data along the R5 framework: Reveal, Refine, Reuse, Release, and Run. The final section discusses the potential impact of the Linked Data Platform as well as areas for further improvement.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {156–165},
numpages = {10},
keywords = {reliability, measurement, standardization, design, economics},
location = {Quebec, Canada},
series = {dg.o '13}
}

@inproceedings{10.1145/2465529.2465546,
author = {Li, Yongkun and Lee, Patrick P.C. and Lui, John C.S.},
title = {Stochastic Modeling of Large-Scale Solid-State Storage Systems: Analysis, Design Tradeoffs and Optimization},
year = {2013},
isbn = {9781450319003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465529.2465546},
doi = {10.1145/2465529.2465546},
abstract = {Solid state drives (SSDs) have seen wide deployment in mobiles, desktops,and data centers due to their high I/O performance and low energy consumption. As SSDs write data out-of-place, garbage collection (GC) is required to erase and reclaim space with invalid data. However, GC poses additional writes that hinder the I/O performance, while SSD blocks can only endure a finite number of erasures. Thus, there is a performance-durability tradeoff on the design space of GC. To characterize the optimal tradeoff, this paper formulates an analytical model that explores the full optimal design space of any GC algorithm. We first present a stochastic Markov chain model that captures the I/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive the asymptotic steady-state performance. We further prove the model convergence and generalize the model for all types of workload. Inspired by this model, we propose a randomized greedy algorithm (RGA) that can operate along the optimal tradeoff curve with a tunable parameter. Using trace-driven simulation on DiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to realize the performance-durability tradeoff.},
booktitle = {Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems},
pages = {179–190},
numpages = {12},
keywords = {cleaning cost, wear-leveling, stochastic modeling, garbage collection, mean field analysis, solid-state drives},
location = {Pittsburgh, PA, USA},
series = {SIGMETRICS '13}
}

@article{10.1145/2494232.2465546,
author = {Li, Yongkun and Lee, Patrick P.C. and Lui, John C.S.},
title = {Stochastic Modeling of Large-Scale Solid-State Storage Systems: Analysis, Design Tradeoffs and Optimization},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2494232.2465546},
doi = {10.1145/2494232.2465546},
abstract = {Solid state drives (SSDs) have seen wide deployment in mobiles, desktops,and data centers due to their high I/O performance and low energy consumption. As SSDs write data out-of-place, garbage collection (GC) is required to erase and reclaim space with invalid data. However, GC poses additional writes that hinder the I/O performance, while SSD blocks can only endure a finite number of erasures. Thus, there is a performance-durability tradeoff on the design space of GC. To characterize the optimal tradeoff, this paper formulates an analytical model that explores the full optimal design space of any GC algorithm. We first present a stochastic Markov chain model that captures the I/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive the asymptotic steady-state performance. We further prove the model convergence and generalize the model for all types of workload. Inspired by this model, we propose a randomized greedy algorithm (RGA) that can operate along the optimal tradeoff curve with a tunable parameter. Using trace-driven simulation on DiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to realize the performance-durability tradeoff.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {179–190},
numpages = {12},
keywords = {wear-leveling, solid-state drives, garbage collection, mean field analysis, cleaning cost, stochastic modeling}
}

@inproceedings{10.1145/2487766.2487768,
author = {Kolovos, Dimitrios S. and Rose, Louis M. and Matragkas, Nicholas and Paige, Richard F. and Guerra, Esther and Cuadrado, Jes\'{u}s S\'{a}nchez and De Lara, Juan and R\'{a}th, Istv\'{a}n and Varr\'{o}, D\'{a}niel and Tisi, Massimo and Cabot, Jordi},
title = {A Research Roadmap towards Achieving Scalability in Model Driven Engineering},
year = {2013},
isbn = {9781450321655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487766.2487768},
doi = {10.1145/2487766.2487768},
abstract = {As Model-Driven Engineering (MDE) is increasingly applied to larger and more complex systems, the current generation of modelling and model management technologies are being pushed to their limits in terms of capacity and efficiency. Additional research and development is imperative in order to enable MDE to remain relevant with industrial practice and to continue delivering its widely recognised productivity, quality, and maintainability benefits. Achieving scalability in modelling and MDE involves being able to construct large models and domain-specific languages in a systematic manner, enabling teams of modellers to construct and refine large models in a collaborative manner, advancing the state of the art in model querying and transformations tools so that they can cope with large models (of the scale of millions of model elements), and providing an infrastructure for efficient storage, indexing and retrieval of large models. This paper attempts to provide a research roadmap for these aspects of scalability in MDE and outline directions for work in this emerging research area.},
booktitle = {Proceedings of the Workshop on Scalability in Model Driven Engineering},
articleno = {2},
numpages = {10},
location = {Budapest, Hungary},
series = {BigMDE '13}
}

@inproceedings{10.1145/2482513.2482965,
author = {Ker, Andrew D. and Bas, Patrick and B\"{o}hme, Rainer and Cogranne, R\'{e}mi and Craver, Scott and Filler, Tom\'{a}\v{s} and Fridrich, Jessica and Pevn\'{y}, Tom\'{a}\v{s}},
title = {Moving Steganography and Steganalysis from the Laboratory into the Real World},
year = {2013},
isbn = {9781450320818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2482513.2482965},
doi = {10.1145/2482513.2482965},
abstract = {There has been an explosion of academic literature on steganography and steganalysis in the past two decades. With a few exceptions, such papers address abstractions of the hiding and detection problems, which arguably have become disconnected from the real world. Most published results, including by the authors of this paper, apply "in laboratory conditions" and some are heavily hedged by assumptions and caveats; significant challenges remain unsolved in order to implement good steganography and steganalysis in practice. This position paper sets out some of the important questions which have been left unanswered, as well as highlighting some that have already been addressed successfully, for steganography and steganalysis to be used in the real world.},
booktitle = {Proceedings of the First ACM Workshop on Information Hiding and Multimedia Security},
pages = {45–58},
numpages = {14},
keywords = {game theory, security models, optimal detection, steganography, minimal distortion, steganalysis},
location = {Montpellier, France},
series = {IH&amp;MMSec '13}
}

@inproceedings{10.1145/2493123.2462917,
author = {Costa, Paolo and Donnelly, Austin and O'Shea, Greg and Rowstron, Antony},
title = {CamCubeOS: A Key-Based Network Stack for 3D Torus Cluster Topologies},
year = {2013},
isbn = {9781450319102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493123.2462917},
doi = {10.1145/2493123.2462917},
abstract = {Cluster fabric interconnects that use 3D torus topologies are increasingly being deployed in data center clusters. In our prior work, we demonstrated that by using these topologies and letting applications implement custom routing protocols and perform operations on path, it is possible to increase performance and simplify development. However, these benefits cannot be achieved using mainstream point-to-point networking stacks such as TCP/IP or MPI, which hide the underlying topology and do not allow the implementation of any in-network operations.In this paper we describe CamCubeOS, a novel key-based communication stack, purposely designed from scratch for 3D torus fabric interconnects. We note that many of the applications used in clusters are key-based. Therefore, we designed CamCubeOS to natively support key-based operations. We select a virtual topology that perfectly matches the underlying physical topology and we use the keyspace to expose the physical locality, thus avoiding the typical overhead incurred by overlay-based approaches.We report on our experience in building several applications on top of CamCubeOS and we evaluate their performance and feasibility using a prototype and large-scale simulations.},
booktitle = {Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {73–84},
numpages = {12},
keywords = {data center clusters, 3D torus topologies, in-network processing, key-based routing},
location = {New York, New York, USA},
series = {HPDC '13}
}

@inproceedings{10.1145/2462902.2462917,
author = {Costa, Paolo and Donnelly, Austin and O'Shea, Greg and Rowstron, Antony},
title = {CamCubeOS: A Key-Based Network Stack for 3D Torus Cluster Topologies},
year = {2013},
isbn = {9781450319102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462902.2462917},
doi = {10.1145/2462902.2462917},
abstract = {Cluster fabric interconnects that use 3D torus topologies are increasingly being deployed in data center clusters. In our prior work, we demonstrated that by using these topologies and letting applications implement custom routing protocols and perform operations on path, it is possible to increase performance and simplify development. However, these benefits cannot be achieved using mainstream point-to-point networking stacks such as TCP/IP or MPI, which hide the underlying topology and do not allow the implementation of any in-network operations.In this paper we describe CamCubeOS, a novel key-based communication stack, purposely designed from scratch for 3D torus fabric interconnects. We note that many of the applications used in clusters are key-based. Therefore, we designed CamCubeOS to natively support key-based operations. We select a virtual topology that perfectly matches the underlying physical topology and we use the keyspace to expose the physical locality, thus avoiding the typical overhead incurred by overlay-based approaches.We report on our experience in building several applications on top of CamCubeOS and we evaluate their performance and feasibility using a prototype and large-scale simulations.},
booktitle = {Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {73–84},
numpages = {12},
keywords = {data center clusters, key-based routing, 3D torus topologies, in-network processing},
location = {New York, New York, USA},
series = {HPDC '13}
}

@inproceedings{10.1145/2479724.2479754,
author = {Niehaves, Bj\"{o}rn and K\"{o}ffer, Sebastian and Ortbach, Kevin},
title = {IT Consumerization under More Difficult Conditions: Insights from German Local Governments},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479754},
doi = {10.1145/2479724.2479754},
abstract = {Public sector organizations work under more difficult conditions, when intending to embrace IT consumerization, defined as the diffusion of consumer IT in the workplace. There is an observable lack in current literature with respect to particular factors that influence consumer IT adoption by local governments. Our paper aims to investigate these aspects in detail. We apply an inductive design for our study, in form of a multiple-case study with two German local governments. From the case data we were able to identify five major factors that influence the adoption and diffusion of consumer IT in the public sector. This paper contributes to a better understanding of the impact organizational characteristics may have on the adoption of consumer IT for work purposes. We think that our results provide promising insights into underlying factors that complicate the exploitations of IT consumerization as opportunity for both increasing work efficiency and creating more innovative work environments in the public sector.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {205–213},
numpages = {9},
keywords = {BYOD, qualitative research, case study, public sector, IT consumerization, local government},
location = {Quebec, Canada},
series = {dg.o '13}
}

@inproceedings{10.1145/2479724.2479752,
author = {Williams, Christine B. and Gulati, Girish J. "Jeff" and Yates, David J.},
title = {Predictors of On-Line Services and e-Participation: A Cross-National Comparison},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479752},
doi = {10.1145/2479724.2479752},
abstract = {Effective e-government creates an environment for citizens to have greater access to their government and, in theory, makes citizen-to-government contact more inclusive. Our research examines two distinct but related measures of e-government effectiveness, namely the online service index and the e-participation index, both reported in the 2010 e-government survey conducted by the United Nations. We analyze the impact of political structure, administrative culture and policy initiatives on both indices in more than 150 countries. Our multiple regression analysis shows that there is greater e-government capability in countries that have an administrative culture of sound governance and policies that advance the development and diffusion of information and communication technologies. More democratic institutions and processes, however, appear to have a negative impact on e-government. In addition, countries that practice effective governance and promote competition in the telecommunications sector demonstrate more extensive provision of e-participation. These results suggest that the path to e-government leverages different strategies depending on a nation's political structure, and that authoritarian countries may be utilizing e-government to maintain the status quo.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {190–197},
numpages = {8},
keywords = {e-participation, regulation, e-government, governance, telecommunications policy, democratic institutions},
location = {Quebec, Canada},
series = {dg.o '13}
}

@inproceedings{10.1145/2466627.2481205,
author = {Chu, Sharon Lynn and Quek, Francis and Gusukuma, Luke and Tanenbaum, Theresa Jean},
title = {The Effects of Physicality on the Child's Imagination},
year = {2013},
isbn = {9781450321501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2466627.2481205},
doi = {10.1145/2466627.2481205},
abstract = {This paper investigates the effects of physical objects as support for imagination in the context of enactive storytelling. More specifically, we target nine-year-old children because of their general disengagement from creative activity, a phenomenon known as the Fourth-grade Slump that arises from a demotivational spiral brought on by social awareness. We study how enactment using physical objects may allow the child to better engage in story imagination. Our study compares the richness of the imagination under three main enactment conditions with objects that have varying degrees of fidelity to referent objects: Cultural objects (physical visual resemblance); Physical objects (similar physical affordances); Arbitrary objects (minimal physical and visual affordances). We employ a mixed-methods analysis to gauge the child's level of broader imagination from three data sources: Enactment videos, drawings and interviews with the children. We found that the object types significantly differ in their support of the imagination, with the object of highest specificity being most effective. Our findings can inform the design of embodied creativity-support systems for children.},
booktitle = {Proceedings of the 9th ACM Conference on Creativity &amp; Cognition},
pages = {93–102},
numpages = {10},
keywords = {creativity, fourth-grade slump, physicality, imagination, children, storytelling},
location = {Sydney, Australia},
series = {C&amp;C '13}
}

@inproceedings{10.1145/2466627.2466643,
author = {Desjardins, Audrey and Wakkary, Ron},
title = {Manifestations of Everyday Design: Guiding Goals and Motivations},
year = {2013},
isbn = {9781450321501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2466627.2466643},
doi = {10.1145/2466627.2466643},
abstract = {This paper explores the relationship between goals, materials and competences in the practice of everyday design. Appropriations and creative uses of design artifacts are often reported in terms of outcomes and goals; however, we observe a gap in understanding how materials, tools, and competences are also involved in these processes. We conduct a multiple case study of three groups of everyday designers: families, hobbyist jewelers, and steampunk enthusiasts. We provide a description of the aspects of meaning, materials, and competences, as well as how they are interrelated, for each case. Our findings show that amongst these three aspects of the practice of everyday designers, it is the meaning of the practice that acts as the strongest motivator for practitioners. Materials, tools, and competences are hence largely determined accordingly. The implications of this study propose ways to design for practices with different types of meaning: foundational, aesthetic, and aspirational goals.},
booktitle = {Proceedings of the 9th ACM Conference on Creativity &amp; Cognition},
pages = {253–262},
numpages = {10},
keywords = {practice theory, DIY, everyday design, families, steampunk, jewelry, hobby, appropriation},
location = {Sydney, Australia},
series = {C&amp;C '13}
}

@inproceedings{10.1145/2465839.2465845,
author = {Feitoza Santos, Alysson and de Lacerda Fernandes, Stenio Flavio and Gomes Lopes J\'{u}nior, Petr\^{o}nio and Fawzi Hadj Sadok, Djamel and Szabo, Geza},
title = {Multi-Gigabit Traffic Identification on GPU},
year = {2013},
isbn = {9781450319812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465839.2465845},
doi = {10.1145/2465839.2465845},
abstract = {Traffic Identification is a crucial task performed by ISP administrators to evaluate and improve network service quality. Deep Packet Inspection (DPI) is a well-known technique used to identify networked traffic. DPI relies mostly on Regular Expressions (REs) evaluated by Finite Automata. Many previous studies have investigated the impacts on the classification accuracy of such systems when inspecting only a portion of the traffic. However, none have discussed the real impacts on the overall system throughput. This work presents a novel technique to perform DPI on Graphics Processing Units (GPU) called Flow-Based Traffic Identification (FBTI) and a proof-of-concept prototype analysis. Basically we want to increase DPI systems? performance on commodity platforms as well as their capacity to identify networked traffic on high speed links. By combining Deterministic Finite Automaton (DFA) for evaluating REs and flow-level packet sampling we achieve a raw performance of over 60 Gbps on GPUs. Our prototype solution could reach a real throughput of over 12 Gbps, measured as the identified volume of flows.},
booktitle = {Proceedings of the First Edition Workshop on High Performance and Programmable Networking},
pages = {39–44},
numpages = {6},
location = {New York, New York, USA},
series = {HPPN '13}
}

@inproceedings{10.1145/2465808.2465811,
author = {Ros-Giralt, Jordi and Rotsted, Bob and Commike, Alan},
title = {Overcoming Performance Collapse for 100Gbps Cyber Security},
year = {2013},
isbn = {9781450319843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465808.2465811},
doi = {10.1145/2465808.2465811},
abstract = {In this paper, we present a series of performance tests carried out on R-Scope Dominate-T (RDT), a 1U network security appliance configured with four Tilera Gx-36 processors and with an aggregated network IO capacity of 160Gbps. RDT is optimized with several high-performance computing techniques. On the software side, RDT runs Linux and a modified version of Bro--the open source network security monitor developed by the International Computer Science Institute--optimized with (1) intelligent IDS-aware packet queuing, (2) Bro-programmable packet shunting, (3) zero-locking IPC data structures, and (4) layer-4 packet prioritization. On the hardware side, the system leverages a many-core architecture with (1) 144 cores servicing 16 x 10Gbps network interfaces, (2) an on-chip ASIC-assisted engine delivering packets directly to Bro at wire rates, and (3) core-programmable zero-overhead/zero-interrupt Linux. The objective of this work is to make a contribution towards maximizing the amount of cyber security intelligence that a system can detect per unit of cost, where cost includes the processing time, space, energy, and capital equipment expenses incurred to perform such detection.},
booktitle = {Proceedings of the First Workshop on Changing Landscapes in HPC Security},
pages = {15–22},
numpages = {8},
keywords = {cyber-security},
location = {New York, New York, USA},
series = {CLHS '13}
}

@inproceedings{10.1145/2463676.2465273,
author = {Huang, Botong and Babu, Shivnath and Yang, Jun},
title = {Cumulon: Optimizing Statistical Data Analysis in the Cloud},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465273},
doi = {10.1145/2463676.2465273},
abstract = {We present Cumulon, a system designed to help users rapidly develop and intelligently deploy matrix-based big-data analysis programs in the cloud. Cumulon features a flexible execution model and new operators especially suited for such workloads. We show how to implement Cumulon on top of Hadoop/HDFS while avoiding limitations of MapReduce, and demonstrate Cumulon's performance advantages over existing Hadoop-based systems for statistical data analysis. To support intelligent deployment in the cloud according to time/budget constraints, Cumulon goes beyond database-style optimization to make choices automatically on not only physical operators and their parameters, but also hardware provisioning and configuration settings. We apply a suite of benchmarking, simulation, modeling, and search techniques to support effective cost-based optimization over this rich space of deployment plans.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
keywords = {data parallelism, cloud, statistical computing, linear algebra},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2484702.2484704,
author = {Alvanaki, Foteini and Ilieva, Evica and Michel, Sebastian and Stupar, Aleksandar},
title = {Interesting Event Detection through Hall of Fame Rankings},
year = {2013},
isbn = {9781450321914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484702.2484704},
doi = {10.1145/2484702.2484704},
abstract = {Everything is relative. Cars are compared by gas per mile, websites by page rank, students based on GPA, scientists by number of publications, and celebrities by beauty or wealth. In this paper, we study the characteristics of such entity rankings based on a set of rankings obtained from a popular Web portal. The obtained insights are integrated in our approach, coined Pantheon. Pantheon maintains sets of top-k rankings and reports identified changes in a way that appeals to users, using a novel combination of different characteristics like competitiveness, information entropy, and scale of change. Entity rankings are assembled by combining entity type attributes with data-driven categorical constraints and sorting criteria on numeric attributes. We report on the results of an experimental evaluation using real-world data obtained from a basketball statistics website.},
booktitle = {Proceedings of the ACM SIGMOD Workshop on Databases and Social Networks},
pages = {7–12},
numpages = {6},
keywords = {events, reporting, entity rankings, hall of fame, top-k rankings},
location = {New York, New York},
series = {DBSocial '13}
}

@inproceedings{10.1145/2463676.2465308,
author = {Elmore, Aaron J. and Das, Sudipto and Pucher, Alexander and Agrawal, Divyakant and El Abbadi, Amr and Yan, Xifeng},
title = {Characterizing Tenant Behavior for Placement and Crisis Mitigation in Multitenant DBMSs},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465308},
doi = {10.1145/2463676.2465308},
abstract = {A multitenant database management system (DBMS) in the cloud must continuously monitor the trade-off between efficient resource sharing among multiple application databases (tenants) and their performance. Considering the scale of attn{hundreds to} thousands of tenants in such multitenant DBMSs, manual approaches for continuous monitoring are not tenable. A self-managing controller of a multitenant DBMS faces several challenges. For instance, how to characterize a tenant given its variety of workloads, how to reduce the impact of tenant colocation, and how to detect and mitigate a performance crisis where one or more tenants' desired service level objective (SLO) is not achieved.We present Delphi, a self-managing system controller for a multitenant DBMS, and Pythia, a technique to learn behavior through observation and supervision using DBMS-agnostic database level performance measures. Pythia accurately learns tenant behavior even when multiple tenants share a database process, learns good and bad tenant consolidation plans (or packings), and maintains a pertenant history to detect behavior changes. Delphi detects performance crises, and leverages Pythia to suggests remedial actions using a hill-climbing search algorithm to identify a new tenant placement strategy to mitigate violating SLOs. Our evaluation using a variety of tenant types and workloads shows that Pythia can learn a tenant's behavior with more than 92% accuracy and learn the quality of packings with more than 86% accuracy. During a performance crisis, Delphi is able to reduce 99th percentile latencies by 80%, and can consolidate 45% more tenants than a greedy baseline, which balances tenant load without modeling tenant behavior.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {517–528},
numpages = {12},
keywords = {database consolidation, elastic data management, tenant characterization, multitenancy, shared nothing architectures},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2463676.2463708,
author = {Larson, Per-Ake and Clinciu, Cipri and Fraser, Campbell and Hanson, Eric N. and Mokhtar, Mostafa and Nowakiewicz, Michal and Papadimos, Vassilis and Price, Susan L. and Rangarajan, Srikumar and Rusanu, Remus and Saubhasik, Mayukh},
title = {Enhancements to SQL Server Column Stores},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463708},
doi = {10.1145/2463676.2463708},
abstract = {SQL Server 2012 introduced two innovations targeted for data warehousing workloads: column store indexes and batch (vectorized) processing mode. Together they greatly improve performance of typical data warehouse queries, routinely by 10X and in some cases by a 100X or more. The main limitations of the initial version are addressed in the upcoming release. Column store indexes are updatable and can be used as the base storage for a table. The repertoire of batch mode operators has been expanded, existing operators have been improved, and query optimization has been enhanced. This paper gives an overview of SQL Server's column stores and batch processing, in particular the enhancements introduced in the upcoming release.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1159–1168},
numpages = {10},
keywords = {olap, data warehousing, columnar storage, index, column store},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2463676.2465278,
author = {Vlachou, Akrivi and Doulkeridis, Christos and N\o{}rv\r{a}g, Kjetil and Kotidis, Yannis},
title = {Branch-and-Bound Algorithm for Reverse Top-k Queries},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465278},
doi = {10.1145/2463676.2465278},
abstract = {Top-k queries return to the user only the k best objects based on the individual user preferences and comprise an essential tool for rank-aware query processing. Assuming a stored data set of user preferences, reverse top-k queries have been introduced for retrieving the users that deem a given database object as one of their top-k results. Reverse top-k queries have already attracted significant interest in research, due to numerous real-life applications such as market analysis and product placement. Currently, the most efficient algorithm for computing the reverse top-k set is RTA. RTA has two main drawbacks when processing a reverse top-k query: (i) it needs to access all stored user preferences, and (ii) it cannot avoid executing a top-k query for each user preference that belongs to the result set. To address these limitations, in this paper, we identify useful properties for processing reverse top-k queries without accessing each user's individual preferences nor executing the top-k query. We propose an intuitive branch-and-bound algorithm for processing reverse top-k queries efficiently and discuss novel optimizations to boost its performance. Our experimental evaluation demonstrates the efficiency of the proposed algorithm that outperforms RTA by a large margin.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {481–492},
numpages = {12},
keywords = {branch-and-bound algorithm, reverse top-k query},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2463676.2465281,
author = {Papadopoulos, Stavros and Cormode, Graham and Deligiannakis, Antonios and Garofalakis, Minos},
title = {Lightweight Authentication of Linear Algebraic Queries on Data Streams},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465281},
doi = {10.1145/2463676.2465281},
abstract = {We consider a stream outsourcing setting, where a data owner delegates the management of a set of disjoint data streams to an untrusted server. The owner authenticates his streams via signatures. The server processes continuous queries on the union of the streams for clients trusted by the owner. Along with the results, the server sends proofs of result correctness derived from the owner's signatures, which are easily verifiable by the clients. We design novel constructions for a collection of fundamental problems over streams represented as linear algebraic queries. In particular, our basic schemes authenticate dynamic vector sums and dot products, as well as dynamic matrix products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group by queries, joins, in-network aggregation, similarity matching, and event processing. All our schemes are very lightweight, and offer strong cryptographic guarantees derived from formal definitions and proofs. We experimentally confirm the practicality of our schemes.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {881–892},
numpages = {12},
keywords = {query authentication, data streams, data integrity},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2485922.2485944,
author = {Wu, Lisa and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Navigating Big Data with High-Throughput, Energy-Efficient Data Partitioning},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485944},
doi = {10.1145/2485922.2485944},
abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {249–260},
numpages = {12},
keywords = {microarchitecture, streaming data, accelerator, specialized functional unit, data partitioning},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485944,
author = {Wu, Lisa and Barker, Raymond J. and Kim, Martha A. and Ross, Kenneth A.},
title = {Navigating Big Data with High-Throughput, Energy-Efficient Data Partitioning},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485944},
doi = {10.1145/2508148.2485944},
abstract = {The global pool of data is growing at 2.5 quintillion bytes per day, with 90% of it produced in the last two years alone [24]. There is no doubt the era of big data has arrived. This paper explores targeted deployment of hardware accelerators to improve the throughput and energy efficiency of large-scale data processing. In particular, data partitioning is a critical operation for manipulating large data sets. It is often the limiting factor in database performance and represents a significant fraction of the overall runtime of large data queries.To accelerate partitioning, this paper describes a hardware accelerator for range partitioning, or HARP, and a hardware-software data streaming framework. The streaming framework offers a seamless execution environment for streaming accelerators such as HARP. Together, HARP and the streaming framework provide an order of magnitude improvement in partitioning performance and energy. A detailed analysis of a 32nm physical design shows 7.8 times the throughput of a highly optimized and optimistic software implementation, while consuming just 6.9% of the area and 4.3% of the power of a single Xeon core in the same technology generation.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {249–260},
numpages = {12},
keywords = {microarchitecture, accelerator, streaming data, data partitioning, specialized functional unit}
}

@inproceedings{10.1145/2485922.2485945,
author = {Chung, Eric S. and Davis, John D. and Lee, Jaewon},
title = {LINQits: Big Data on Little Clients},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485945},
doi = {10.1145/2485922.2485945},
abstract = {We present LINQits, a flexible hardware template that can be mapped onto programmable logic or ASICs in a heterogeneous system-on-chip for a mobile device or server. Unlike fixed-function accelerators, LINQits accelerates a domain-specific query language called LINQ. LINQits does not provide coverage for all possible applications---however, existing applications (re-)written with LINQ in mind benefit extensively from hardware acceleration. Furthermore, the LINQits framework offers a graceful and transparent migration path from software to hardware.LINQits is prototyped on a 2W heterogeneous SoC called the ZYNQ processor, which combines dual ARM A9 processors with an FPGA on a single die in 28nm silicon technology. Our physical measurements show that LINQits improves energy efficiency by 8.9 to 30.6 times and performance by 10.7 to 38.1 times compared to optimized, multithreaded C programs running on conventional ARM A9 processors.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {261–272},
numpages = {12},
keywords = {mobile, co-processor accelerator, query language, FPGA, database, big data, ASIC},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485945,
author = {Chung, Eric S. and Davis, John D. and Lee, Jaewon},
title = {LINQits: Big Data on Little Clients},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485945},
doi = {10.1145/2508148.2485945},
abstract = {We present LINQits, a flexible hardware template that can be mapped onto programmable logic or ASICs in a heterogeneous system-on-chip for a mobile device or server. Unlike fixed-function accelerators, LINQits accelerates a domain-specific query language called LINQ. LINQits does not provide coverage for all possible applications---however, existing applications (re-)written with LINQ in mind benefit extensively from hardware acceleration. Furthermore, the LINQits framework offers a graceful and transparent migration path from software to hardware.LINQits is prototyped on a 2W heterogeneous SoC called the ZYNQ processor, which combines dual ARM A9 processors with an FPGA on a single die in 28nm silicon technology. Our physical measurements show that LINQits improves energy efficiency by 8.9 to 30.6 times and performance by 10.7 to 38.1 times compared to optimized, multithreaded C programs running on conventional ARM A9 processors.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {261–272},
numpages = {12},
keywords = {database, query language, mobile, FPGA, co-processor accelerator, big data, ASIC}
}

@inproceedings{10.1145/2485922.2485974,
author = {Yang, Hailong and Breslow, Alex and Mars, Jason and Tang, Lingjia},
title = {Bubble-Flux: Precise Online QoS Management for Increased Utilization in Warehouse Scale Computers},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485974},
doi = {10.1145/2485922.2485974},
abstract = {Ensuring the quality of service (QoS) for latency-sensitive applications while allowing co-locations of multiple applications on servers is critical for improving server utilization and reducing cost in modern warehouse-scale computers (WSCs). Recent work relies on static profiling to precisely predict the QoS degradation that results from performance interference among co-running applications to increase the number of "safe" co-locations. However, these static profiling techniques have several critical limitations: 1) a priori knowledge of all workloads is required for profiling, 2) it is difficult for the prediction to capture or adapt to phase or load changes of applications, and 3) the prediction technique is limited to only two co-running applications.To address all of these limitations, we present Bubble-Flux, an integrated dynamic interference measurement and online QoS management mechanism to provide accurate QoS control and maximize server utilization. Bubble-Flux uses a Dynamic Bubble to probe servers in real time to measure the instantaneous pressure on the shared hardware resources and precisely predict how the QoS of a latency-sensitive job will be affected by potential co-runners. Once "safe" batch jobs are selected and mapped to a server, Bubble-Flux uses an Online Flux Engine to continuously monitor the QoS of the latency-sensitive application and control the execution of batch jobs to adapt to dynamic input, phase, and load changes to deliver satisfactory QoS. Batch applications remain in a state of flux throughout execution. Our results show that the utilization improvement achieved by Bubble-Flux is up to 2.2x better than the prior static approach.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {607–618},
numpages = {12},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485974,
author = {Yang, Hailong and Breslow, Alex and Mars, Jason and Tang, Lingjia},
title = {Bubble-Flux: Precise Online QoS Management for Increased Utilization in Warehouse Scale Computers},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485974},
doi = {10.1145/2508148.2485974},
abstract = {Ensuring the quality of service (QoS) for latency-sensitive applications while allowing co-locations of multiple applications on servers is critical for improving server utilization and reducing cost in modern warehouse-scale computers (WSCs). Recent work relies on static profiling to precisely predict the QoS degradation that results from performance interference among co-running applications to increase the number of "safe" co-locations. However, these static profiling techniques have several critical limitations: 1) a priori knowledge of all workloads is required for profiling, 2) it is difficult for the prediction to capture or adapt to phase or load changes of applications, and 3) the prediction technique is limited to only two co-running applications.To address all of these limitations, we present Bubble-Flux, an integrated dynamic interference measurement and online QoS management mechanism to provide accurate QoS control and maximize server utilization. Bubble-Flux uses a Dynamic Bubble to probe servers in real time to measure the instantaneous pressure on the shared hardware resources and precisely predict how the QoS of a latency-sensitive job will be affected by potential co-runners. Once "safe" batch jobs are selected and mapped to a server, Bubble-Flux uses an Online Flux Engine to continuously monitor the QoS of the latency-sensitive application and control the execution of batch jobs to adapt to dynamic input, phase, and load changes to deliver satisfactory QoS. Batch applications remain in a state of flux throughout execution. Our results show that the utilization improvement achieved by Bubble-Flux is up to 2.2x better than the prior static approach.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {607–618},
numpages = {12}
}

@inproceedings{10.1145/2485922.2485949,
author = {Cook, Henry and Moreto, Miquel and Bird, Sarah and Dao, Khanh and Patterson, David A. and Asanovic, Krste},
title = {A Hardware Evaluation of Cache Partitioning to Improve Utilization and Energy-Efficiency While Preserving Responsiveness},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485949},
doi = {10.1145/2485922.2485949},
abstract = {Computing workloads often contain a mix of interactive, latency-sensitive foreground applications and recurring background computations. To guarantee responsiveness, interactive and batch applications are often run on disjoint sets of resources, but this incurs additional energy, power, and capital costs. In this paper, we evaluate the potential of hardware cache partitioning mechanisms and policies to improve efficiency by allowing background applications to run simultaneously with interactive foreground applications, while avoiding degradation in interactive responsiveness. We evaluate these tradeoffs using commercial x86 multicore hardware that supports cache partitioning, and find that real hardware measurements with full applications provide different observations than past simulation-based evaluations. Co-scheduling applications without LLC partitioning leads to a 10% energy improvement and average throughput improvement of 54% compared to running tasks separately, but can result in foreground performance degradation of up to 34% with an average of 6%. With optimal static LLC partitioning, the average energy improvement increases to 12% and the average throughput improvement to 60%, while the worst case slowdown is reduced noticeably to 7% with an average slowdown of only 2%. We also evaluate a practical low-overhead dynamic algorithm to control partition sizes, and are able to realize the potential performance guarantees of the optimal static approach, while increasing background throughput by an additional 19%.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {308–319},
numpages = {12},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485949,
author = {Cook, Henry and Moreto, Miquel and Bird, Sarah and Dao, Khanh and Patterson, David A. and Asanovic, Krste},
title = {A Hardware Evaluation of Cache Partitioning to Improve Utilization and Energy-Efficiency While Preserving Responsiveness},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485949},
doi = {10.1145/2508148.2485949},
abstract = {Computing workloads often contain a mix of interactive, latency-sensitive foreground applications and recurring background computations. To guarantee responsiveness, interactive and batch applications are often run on disjoint sets of resources, but this incurs additional energy, power, and capital costs. In this paper, we evaluate the potential of hardware cache partitioning mechanisms and policies to improve efficiency by allowing background applications to run simultaneously with interactive foreground applications, while avoiding degradation in interactive responsiveness. We evaluate these tradeoffs using commercial x86 multicore hardware that supports cache partitioning, and find that real hardware measurements with full applications provide different observations than past simulation-based evaluations. Co-scheduling applications without LLC partitioning leads to a 10% energy improvement and average throughput improvement of 54% compared to running tasks separately, but can result in foreground performance degradation of up to 34% with an average of 6%. With optimal static LLC partitioning, the average energy improvement increases to 12% and the average throughput improvement to 60%, while the worst case slowdown is reduced noticeably to 7% with an average slowdown of only 2%. We also evaluate a practical low-overhead dynamic algorithm to control partition sizes, and are able to realize the potential performance guarantees of the optimal static approach, while increasing background throughput by an additional 19%.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {308–319},
numpages = {12}
}

@inproceedings{10.1145/2485922.2485948,
author = {Bacha, Anys and Teodorescu, Radu},
title = {Dynamic Reduction of Voltage Margins by Leveraging On-Chip ECC in Itanium II Processors},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485948},
doi = {10.1145/2485922.2485948},
abstract = {Lowering supply voltage is one of the most effective approaches for improving the energy efficiency of microprocessors. Unfortunately, technology limitations, such as process variability and circuit aging, are forcing microprocessor designers to add larger voltage guardbands to their chips. This makes supply voltage increasingly difficult to scale with technology. This paper presents a new mechanism for dynamically reducing voltage margins while maintaining the chip operating frequency constant. Unlike previous approaches that rely on special hardware to detect and recover from timing violations caused by low-voltage execution, our solution is firmware-based and does not require additional hardware. Instead, it relies on error correction mechanisms already built into modern processors. The system dynamically reduces voltage margins and uses correctable error reports raised by the hardware to identify the lowest, safe operating voltage. The solution adapts to core-to-core variability by tailoring supply voltage to each core's safe operating level. In addition, it exploits variability in workload vulnerability to low voltage execution. The system was prototyped on an HP Integrity Server that uses Intel's Itanium 9560 processors. Evaluation using SPECjbb2005 and SPEC CPU2000 workloads shows core power savings ranging from 18% to 23%, with minimal performance impact.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {297–307},
numpages = {11},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485948,
author = {Bacha, Anys and Teodorescu, Radu},
title = {Dynamic Reduction of Voltage Margins by Leveraging On-Chip ECC in Itanium II Processors},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485948},
doi = {10.1145/2508148.2485948},
abstract = {Lowering supply voltage is one of the most effective approaches for improving the energy efficiency of microprocessors. Unfortunately, technology limitations, such as process variability and circuit aging, are forcing microprocessor designers to add larger voltage guardbands to their chips. This makes supply voltage increasingly difficult to scale with technology. This paper presents a new mechanism for dynamically reducing voltage margins while maintaining the chip operating frequency constant. Unlike previous approaches that rely on special hardware to detect and recover from timing violations caused by low-voltage execution, our solution is firmware-based and does not require additional hardware. Instead, it relies on error correction mechanisms already built into modern processors. The system dynamically reduces voltage margins and uses correctable error reports raised by the hardware to identify the lowest, safe operating voltage. The solution adapts to core-to-core variability by tailoring supply voltage to each core's safe operating level. In addition, it exploits variability in workload vulnerability to low voltage execution. The system was prototyped on an HP Integrity Server that uses Intel's Itanium 9560 processors. Evaluation using SPECjbb2005 and SPEC CPU2000 workloads shows core power savings ranging from 18% to 23%, with minimal performance impact.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {297–307},
numpages = {11}
}

@inproceedings{10.1145/2485922.2485943,
author = {Basu, Arkaprava and Gandhi, Jayneel and Chang, Jichuan and Hill, Mark D. and Swift, Michael M.},
title = {Efficient Virtual Memory for Big Memory Servers},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485943},
doi = {10.1145/2485922.2485943},
abstract = {Our analysis shows that many "big-memory" server workloads, such as databases, in-memory caches, and graph analytics, pay a high cost for page-based virtual memory. They consume as much as 10% of execution cycles on TLB misses, even using large pages. On the other hand, we find that these workloads use read-write permission on most pages, are provisioned not to swap, and rarely benefit from the full flexibility of page-based virtual memory.To remove the TLB miss overhead for big-memory workloads, we propose mapping part of a process's linear virtual address space with a direct segment, while page mapping the rest of the virtual address space. Direct segments use minimal hardware---base, limit and offset registers per core---to map contiguous virtual memory regions directly to contiguous physical memory. They eliminate the possibility of TLB misses for key data structures such as database buffer pools and in-memory key-value stores. Memory mapped by a direct segment may be converted back to paging when needed.We prototype direct-segment software support for x86-64 in Linux and emulate direct-segment hardware. For our workloads, direct segments eliminate almost all TLB misses and reduce the execution time wasted on TLB misses to less than 0.5%.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {237–248},
numpages = {12},
keywords = {tanslation lookaside buffer, virtual memory},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485943,
author = {Basu, Arkaprava and Gandhi, Jayneel and Chang, Jichuan and Hill, Mark D. and Swift, Michael M.},
title = {Efficient Virtual Memory for Big Memory Servers},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485943},
doi = {10.1145/2508148.2485943},
abstract = {Our analysis shows that many "big-memory" server workloads, such as databases, in-memory caches, and graph analytics, pay a high cost for page-based virtual memory. They consume as much as 10% of execution cycles on TLB misses, even using large pages. On the other hand, we find that these workloads use read-write permission on most pages, are provisioned not to swap, and rarely benefit from the full flexibility of page-based virtual memory.To remove the TLB miss overhead for big-memory workloads, we propose mapping part of a process's linear virtual address space with a direct segment, while page mapping the rest of the virtual address space. Direct segments use minimal hardware---base, limit and offset registers per core---to map contiguous virtual memory regions directly to contiguous physical memory. They eliminate the possibility of TLB misses for key data structures such as database buffer pools and in-memory key-value stores. Memory mapped by a direct segment may be converted back to paging when needed.We prototype direct-segment software support for x86-64 in Linux and emulate direct-segment hardware. For our workloads, direct segments eliminate almost all TLB misses and reduce the execution time wasted on TLB misses to less than 0.5%.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {237–248},
numpages = {12},
keywords = {tanslation lookaside buffer, virtual memory}
}

@inproceedings{10.1145/2485922.2485935,
author = {Parashar, Angshuman and Pellauer, Michael and Adler, Michael and Ahsan, Bushra and Crago, Neal and Lustig, Daniel and Pavlov, Vladimir and Zhai, Antonia and Gambhir, Mohit and Jaleel, Aamer and Allmon, Randy and Rayess, Rachid and Maresh, Stephen and Emer, Joel},
title = {Triggered Instructions: A Control Paradigm for Spatially-Programmed Architectures},
year = {2013},
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485922.2485935},
doi = {10.1145/2485922.2485935},
abstract = {In this paper, we present triggered instructions, a novel control paradigm for arrays of processing elements (PEs) aimed at exploiting spatial parallelism. Triggered instructions completely eliminate the program counter and allow programs to transition concisely between states without explicit branch instructions. They also allow efficient reactivity to inter-PE communication traffic. The approach provides a unified mechanism to avoid over-serialized execution, essentially achieving the effect of techniques such as dynamic instruction reordering and multithreading, which each require distinct hardware mechanisms in a traditional sequential architecture.Our analysis shows that a triggered-instruction based spatial accelerator can achieve 8X greater area-normalized performance than a traditional general-purpose processor. Further analysis shows that triggered control reduces the number of static and dynamic instructions in the critical paths by 62% and 64% respectively over a program-counter style spatial baseline, resulting in a speedup of 2.0X.},
booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
pages = {142–153},
numpages = {12},
keywords = {reconfigurable accelerators, spatial programming},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@article{10.1145/2508148.2485935,
author = {Parashar, Angshuman and Pellauer, Michael and Adler, Michael and Ahsan, Bushra and Crago, Neal and Lustig, Daniel and Pavlov, Vladimir and Zhai, Antonia and Gambhir, Mohit and Jaleel, Aamer and Allmon, Randy and Rayess, Rachid and Maresh, Stephen and Emer, Joel},
title = {Triggered Instructions: A Control Paradigm for Spatially-Programmed Architectures},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2508148.2485935},
doi = {10.1145/2508148.2485935},
abstract = {In this paper, we present triggered instructions, a novel control paradigm for arrays of processing elements (PEs) aimed at exploiting spatial parallelism. Triggered instructions completely eliminate the program counter and allow programs to transition concisely between states without explicit branch instructions. They also allow efficient reactivity to inter-PE communication traffic. The approach provides a unified mechanism to avoid over-serialized execution, essentially achieving the effect of techniques such as dynamic instruction reordering and multithreading, which each require distinct hardware mechanisms in a traditional sequential architecture.Our analysis shows that a triggered-instruction based spatial accelerator can achieve 8X greater area-normalized performance than a traditional general-purpose processor. Further analysis shows that triggered control reduces the number of static and dynamic instructions in the critical paths by 62% and 64% respectively over a program-counter style spatial baseline, resulting in a speedup of 2.0X.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {142–153},
numpages = {12},
keywords = {reconfigurable accelerators, spatial programming}
}

@inproceedings{10.1145/2479440.2479448,
author = {Subramonian, Ramesh and Gopalakrishna, Kishore and Surlaker, Kapil and Schulman, Bob and Gandhi, Mihir and Topiwala, Sajid and Zhang, David and Zhang, Zhen},
title = {In Data Veritas: Data Driven Testing for Distributed Systems},
year = {2013},
isbn = {9781450321518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479440.2479448},
doi = {10.1145/2479440.2479448},
abstract = {The increasing deployment of distributed systems to solve large data and computational problems has not seen a concomitant increase in tools and techniques to test these systems. In this paper, we propose a data driven approach to testing. We translate our intuitions and expectations about how the system should behave into invariants, the truth of which can be verified from data emitted by the system. Our particular implementation of the invariants uses Q, a high-performance analytical database, programmed with a vector language.To show the practical value of this approach, we describe how it was used to test Helix, a distributed cluster manager deployed at LinkedIn. We make the case that looking at testing as an exercise in data analytics has the following benefits. It (a) increases the expressivity of the tests (b) decreases their fragility and (c) suggests additional, insightful ways to understand the system under test.As the title of the paper suggests, there is truth in the data --- we only need to look for it.},
booktitle = {Proceedings of the Sixth International Workshop on Testing Database Systems},
articleno = {4},
numpages = {6},
location = {New York, New York},
series = {DBTest '13}
}

@inproceedings{10.1145/2462456.2464449,
author = {LiKamWa, Robert and Liu, Yunxin and Lane, Nicholas D. and Zhong, Lin},
title = {MoodScope: Building a Mood Sensor from Smartphone Usage Patterns},
year = {2013},
isbn = {9781450316729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462456.2464449},
doi = {10.1145/2462456.2464449},
abstract = {We report a first-of-its-kind smartphone software system, MoodScope, which infers the mood of its user based on how the smartphone is used. Compared to smartphone sensors that measure acceleration, light, and other physical properties, MoodScope is a "sensor" that measures the mental state of the user and provides mood as an important input to context-aware computing. We run a formative statistical mood study with smartphone-logged data collected from 32 participants over two months. Through the study, we find that by analyzing communication history and application usage patterns, we can statistically infer a user's daily mood average with an initial accuracy of 66%, which gradu-ally improves to an accuracy of 93% after a two-month personal-ized training period. Motivated by these results, we build a service, MoodScope, which analyzes usage history to act as a sensor of the user's mood. We provide a MoodScope API for developers to use our system to create mood-enabled applications. We further create and deploy a mood-sharing social application.},
booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {389–402},
numpages = {14},
keywords = {mobile systems, mood, smartphone usage, machine learning, affective computing},
location = {Taipei, Taiwan},
series = {MobiSys '13}
}

@inproceedings{10.1145/2462456.2464440,
author = {Simoens, Pieter and Xiao, Yu and Pillai, Padmanabhan and Chen, Zhuo and Ha, Kiryong and Satyanarayanan, Mahadev},
title = {Scalable Crowd-Sourcing of Video from Mobile Devices},
year = {2013},
isbn = {9781450316729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462456.2464440},
doi = {10.1145/2462456.2464440},
abstract = {We propose a scalable Internet system for continuous collection of crowd-sourced video from devices such as Google Glass. Our hybrid cloud architecture, GigaSight, is effectively a Content Delivery Network (CDN) in reverse. It achieves scalability by decentralizing the collection infrastructure using cloudlets based on virtual machines~(VMs). Based on time, location, and content, privacy sensitive information is automatically removed from the video. This process, which we refer to as denaturing, is executed in a user-specific VM on the cloudlet. Users can perform content-based searches on the total catalog of denatured videos. Our experiments reveal the bottlenecks for video upload, denaturing, indexing, and content-based search. They also provide insight on how parameters such as frame rate and resolution impact scalability.},
booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {139–152},
numpages = {14},
keywords = {mobile computing, computer vision, smartphone, denaturing, cloud computing, virtual machines, google glass, cloudlet},
location = {Taipei, Taiwan},
series = {MobiSys '13}
}

@inproceedings{10.1145/2462456.2464446,
author = {Nirjon, Shahriar and Dickerson, Robert F. and Asare, Philip and Li, Qiang and Hong, Dezhi and Stankovic, John A. and Hu, Pan and Shen, Guobin and Jiang, Xiaofan},
title = {Auditeur: A Mobile-Cloud Service Platform for Acoustic Event Detection on Smartphones},
year = {2013},
isbn = {9781450316729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462456.2464446},
doi = {10.1145/2462456.2464446},
abstract = {Auditeur is a general-purpose, energy-efficient, and context-aware acoustic event detection platform for smartphones. It enables app developers to have their app register for and get notified on a wide variety of acoustic events. Auditeur is backed by a cloud service to store user contributed sound clips and to generate an energy-efficient and context-aware classification plan for the phone. When an acoustic event type has been registered, the smartphone instantiates the necessary acoustic processing modules and wires them together to execute the plan. The phone then captures, processes, and classifies acoustic events locally and efficiently. Our analysis on user-contributed empirical data shows that Auditeur's energy-aware acoustic feature selection algorithm is capable of increasing the device lifetime by 33.4%, sacrificing less than 2% of the maximum achievable accuracy. We implement seven apps with Auditeur, and deploy them in real-world scenarios to demonstrate that Auditeur is versatile, 11.04% - 441.42% less power hungry, and 10.71% - 13.86% more accurate in detecting acoustic events, compared to state-of-the-art techniques. We present a user study to demonstrate that novice programmers can implement the core logic of interesting apps with Auditeur in less than 30 minutes, using only 15 - 20 lines of Java code.},
booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {403–416},
numpages = {14},
keywords = {auditeur, soundlets, acoustics},
location = {Taipei, Taiwan},
series = {MobiSys '13}
}

@inproceedings{10.1145/2462456.2464457,
author = {Parate, Abhinav and Chiu, Meng-Chieh and Ganesan, Deepak and Marlin, Benjamin M.},
title = {Leveraging Graphical Models to Improve Accuracy and Reduce Privacy Risks of Mobile Sensing},
year = {2013},
isbn = {9781450316729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462456.2464457},
doi = {10.1145/2462456.2464457},
abstract = {The proliferation of sensors on mobile phones and wearables has led to a plethora of context classifiers designed to sense the individual's context. We argue that a key missing piece in mobile inference is a layer that fuses the outputs of several classifiers to learn deeper insights into an individual's habitual patterns and associated correlations between contexts, thereby enabling new systems optimizations and opportunities. In this paper, we design CQue, a dynamic bayesian network that operates over classifiers for individual contexts, observes relations across these outputs across time, and identifies opportunities for improving energy-efficiency and accuracy by taking advantage of relations. In addition, such a layer provides insights into privacy leakage that might occur when seemingly innocuous user context revealed to different applications on a phone may be combined to reveal more information than originally intended. In terms of system architecture, our key contribution is a clean separation between the detection layer and the fusion layer, enabling classifiers to solely focus on detecting the context, and leverage temporal smoothing and fusion mechanisms to further boost performance by just connecting to our higher-level inference engine. To applications and users, CQue provides a query interface, allowing a) applications to obtain more accurate context results while remaining agnostic of what classifiers/sensors are used and when, and b) users to specify what contexts they wish to keep private, and only allow information that has low leakage with the private context to be revealed. We implemented CQue in Android, and our results show that CQue can i) improve activity classification accuracy up to 42%, ii) reduce energy consumption in classifying social, location and activity contexts with high accuracy(&gt;90%) by reducing the number of required classifiers by at least 33%, and iii) effectively detect and suppress contexts that reveal private information.},
booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {83–96},
numpages = {14},
keywords = {mobile computing, energy-accuracy-privacy optimizations, continuous context-sensing},
location = {Taipei, Taiwan},
series = {MobiSys '13}
}

@inproceedings{10.1145/2557595.2557601,
author = {Bonner, John Vh and Peebles, David},
title = {Rhetorical Considerations for Innovative Approaches to Performance and Audience Engagement},
year = {2013},
isbn = {9781450325813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2557595.2557601},
doi = {10.1145/2557595.2557601},
abstract = {This research explores how digital media could be used to enhance engagement between an audience and a presenter, orator, or lecturer. Our aim is to think creatively about what a presentation could become and how the use of digital media technologies could form an intrinsic part of the presenter/audience experience. To help anchor this concept, we devised a development framework, which is briefly described. The framework was used to develop a potential component element of a performance presentation: the use of multiple video projections that would supplement a presentation and form a connected but non-linear narrative to produce an element of a performance presentation. To contextualize the framework and assess its effectiveness, an evaluation study of a multiple video projection project was evaluated. The study revealed the importance of audience priming within the rhetorical considerations of the development framework.},
booktitle = {Proceedings of the 2013 Inputs-Outputs Conference: An Interdisciplinary Conference on Engagement in HCI and Performance},
articleno = {6},
numpages = {4},
keywords = {performance presentations, performance arts, audience engagement, interaction design},
location = {Brighton, United Kingdom},
series = {Inputs-Outputs '13}
}

@inproceedings{10.1145/2482991.2483007,
author = {Foth, Marcus and Satchell, Christine and Seeburger, Jan and Russell-Bennett, Rebekah},
title = {Social and Mobile Interaction Design to Increase the Loyalty Rates of Young Blood Donors},
year = {2013},
isbn = {9781450321044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2482991.2483007},
doi = {10.1145/2482991.2483007},
abstract = {Young adults represent the largest group of first time donors to the Australian Red Cross Blood Service, but they are also the least loyal group and often do not return after their first donation. At the same time, many young people use the internet and various forms of social media on a daily basis. Web and mobile based technological practices and communication patterns change the way that young people interact with one another, with their families, and communities. Combining these two points of departure, this study seeks to identify best practices of employing mobile apps and social media in order to enhance the loyalty rates of young blood donors. The findings reported in this paper are based on a qualitative approach presenting a nuanced understanding of the different factors that motivate young people to donate blood in the first place, as well as the obstacles or issues that prevent them from returning. The paper discusses work in progress with a view to inform the development of interactive prototypes trialling three categories of features: personal services (such as scheduling); social media (such as sharing the donation experience with friends to raise awareness); and data visualisations (such as local blood inventory levels). We discuss our translation of research findings into design implications.},
booktitle = {Proceedings of the 6th International Conference on Communities and Technologies},
pages = {64–73},
numpages = {10},
keywords = {urban informatics, blood donation, persuasive technology, mobile applications, interaction design, social media},
location = {Munich, Germany},
series = {C&amp;T '13}
}

@inproceedings{10.1145/2482991.2482992,
author = {Ploderer, Bernd and Smith, Wally and Howard, Steve and Pearce, Jon and Borland, Ron},
title = {Patterns of Support in an Online Community for Smoking Cessation},
year = {2013},
isbn = {9781450321044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2482991.2482992},
doi = {10.1145/2482991.2482992},
abstract = {Social support offers various benefits for health and behaviour change. However, previous work has shown that individuals are typically reluctant to ask for support on social network sites, unless they can present a changed, healthier identity. To examine the relationship between stage of change and social support we conducted a thematic analysis of messages posted in a public Facebook support group for people trying to quit smoking. Our findings show that the kind of support exchanged online is related to participants' stage of change. Contrary to our expectations, supportive responses and leadership in the support group came mainly from users who just started their change process rather than people who had already changed. We discuss contributions to theories of online participation and impression management as well as implications for practitioners who seek to establish support groups.},
booktitle = {Proceedings of the 6th International Conference on Communities and Technologies},
pages = {26–35},
numpages = {10},
keywords = {Facebook, social network sites, smoking cessation, online participation, behaviour change, social support},
location = {Munich, Germany},
series = {C&amp;T '13}
}

@inproceedings{10.1145/2543882.2543886,
author = {Korhonen, Ari and Naps, Thomas and Boisvert, Charles and Crescenzi, Pilu and Karavirta, Ville and Mannila, Linda and Miller, Bradley and Morrison, Briana and Rodger, Susan H. and Ross, Rocky and Shaffer, Clifford A.},
title = {Requirements and Design Strategies for Open Source Interactive Computer Science EBooks},
year = {2013},
isbn = {9781450326650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2543882.2543886},
doi = {10.1145/2543882.2543886},
abstract = {Online education supported by digital courseware will radically alter higher education in ways that we cannot predict. New technologies such as MOOCs and Khan Academy have generated interest in new models for knowledge delivery. The nature of Computer Science content provides special opportunities for computer-supported delivery in both traditional and online classes. Traditional CS textbooks are likely to be replaced by online materials that tightly integrate content with visualizations and automatically assessed exercises. We refer to these new textbook-like artifacts as icseBooks (pronounced ice books"), for interactive computer science electronic books. IcseBook technology will in turn impact the pedagogy used in CS courses. This report surveys the state of the field, addresses new use cases for CS pedagogy with icseBooks, and lays out a series of research questions for future study.},
booktitle = {Proceedings of the ITiCSE Working Group Reports Conference on Innovation and Technology in Computer Science Education-Working Group Reports},
pages = {53–72},
numpages = {20},
keywords = {automated assessment, interactive eBook, digital education, algorithm visualization, hypertext},
location = {Canterbury, England, United Kingdom},
series = {ITiCSE -WGR '13}
}

@inproceedings{10.1145/2482991.2482999,
author = {M\"{u}ller-Birn, Claudia and Dobusch, Leonhard and Herbsleb, James D.},
title = {Work-to-Rule: The Emergence of Algorithmic Governance in Wikipedia},
year = {2013},
isbn = {9781450321044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2482991.2482999},
doi = {10.1145/2482991.2482999},
abstract = {Research has shown the importance of a functioning governance system for the success of peer production communities. It particularly highlights the role of human coordination and communication within the governance regime. In this article, we extend this line of research by differentiating two categories of governance mechanisms. The first category is based primarily on communication, in which social norms emerge that are often formalized by written rules and guidelines. The second category refers to the technical infrastructure that enables users to access artifacts, and that allows the community to communicate and coordinate their collective actions to create those artifacts. We collected qualitative and quantitative data from Wikipedia in order to show how a community's consensus gradually converts social mechanisms into algorithmic mechanisms. In detail, we analyze algorithmic governance mechanisms in two embedded cases: the software extension "flagged revisions" and the bot "xqbot". Our insights point towards a growing relevance of algorithmic governance in the realm of governing large-scale peer production communities. This extends previous research, in which algorithmic governance is almost absent. Further research is needed to unfold, understand, and also modify existing interdependencies between social and algorithmic governance mechanisms.},
booktitle = {Proceedings of the 6th International Conference on Communities and Technologies},
pages = {80–89},
numpages = {10},
keywords = {software, bots, Wikipedia, qualitative, governance, wiki},
location = {Munich, Germany},
series = {C&amp;T '13}
}

@article{10.1145/2483669.2483676,
author = {Burrows, Steven and Potthast, Martin and Stein, Benno},
title = {Paraphrase Acquisition via Crowdsourcing and Machine Learning},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483676},
doi = {10.1145/2483669.2483676},
abstract = {To paraphrase means to rewrite content while preserving the original meaning. Paraphrasing is important in fields such as text reuse in journalism, anonymizing work, and improving the quality of customer-written reviews. This article contributes to paraphrase acquisition and focuses on two aspects that are not addressed by current research: (1) acquisition via crowdsourcing, and (2) acquisition of passage-level samples. The challenge of the first aspect is automatic quality assurance; without such a means the crowdsourcing paradigm is not effective, and without crowdsourcing the creation of test corpora is unacceptably expensive for realistic order of magnitudes. The second aspect addresses the deficit that most of the previous work in generating and evaluating paraphrases has been conducted using sentence-level paraphrases or shorter; these short-sample analyses are limited in terms of application to plagiarism detection, for example. We present the Webis Crowd Paraphrase Corpus 2011 (Webis-CPC-11), which recently formed part of the PAN 2010 international plagiarism detection competition. This corpus comprises passage-level paraphrases with 4067 positive samples and 3792 negative samples that failed our criteria, using Amazon's Mechanical Turk for crowdsourcing. In this article, we review the lessons learned at PAN 2010, and explain in detail the method used to construct the corpus. The empirical contributions include machine learning experiments to explore if passage-level paraphrases can be identified in a two-class classification problem using paraphrase similarity features, and we find that a k-nearest-neighbor classifier can correctly distinguish between paraphrased and nonparaphrased samples with 0.980 precision at 0.523 recall. This result implies that just under half of our samples must be discarded (remaining 0.477 fraction), but our cost analysis shows that the automation we introduce results in a 18% financial saving and over 100 hours of time returned to the researchers when repeating a similar corpus design. On the other hand, when building an unrelated corpus requiring, say, 25% training data for the automated component, we show that the financial outcome is cost neutral, while still returning over 70 hours of time to the researchers. The work presented here is the first to join the paraphrasing and plagiarism communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {corpus, Mechanical Turk, Paraphrase generation, plagiarism, cost analysis}
}

@article{10.1145/2483669.2483679,
author = {Schuster, Daniel and Rosi, Alberto and Mamei, Marco and Springer, Thomas and Endler, Markus and Zambonelli, Franco},
title = {Pervasive Social Context: Taxonomy and Survey},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483679},
doi = {10.1145/2483669.2483679},
abstract = {As pervasive computing meets social networks, there is a fast growing research field called pervasive social computing. Applications in this area exploit the richness of information arising out of people using sensor-equipped pervasive devices in their everyday life combined with intense use of different social networking services. We call this set of information pervasive social context. We provide a taxonomy to classify pervasive social context along the dimensions space, time, people, and information source (STiPI) as well as commenting on the type and reason for creating such context. A survey of recent research shows the applicability and usefulness of the taxonomy in classifying and assessing applications and systems in the area of pervasive social computing. Finally, we present some research challenges in this area and illustrate how they affect the systems being surveyed.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {46},
numpages = {22},
keywords = {social networks, Pervasive computing, taxonomy, context awareness, survey}
}

@article{10.1145/2483852.2483871,
author = {Marathe, Madhav and Vullikanti, Anil Kumar S.},
title = {Computational Epidemiology},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2483852.2483871},
doi = {10.1145/2483852.2483871},
abstract = {The challenge of developing and using computer models to understand and control the diffusion of disease through populations.},
journal = {Commun. ACM},
month = jul,
pages = {88–96},
numpages = {9}
}

@article{10.1145/2483710.2483711,
author = {Isom\"{o}tt\"{o}nen, Ville and Tirronen, Ville},
title = {Teaching Programming by Emphasizing Self-Direction: How Did Students React to the Active Role Required of Them?},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
url = {https://doi.org/10.1145/2483710.2483711},
doi = {10.1145/2483710.2483711},
abstract = {Lecturing is known to be a controversial form of teaching. With massed classrooms, in particular, it tends to constrain the active participation of students. One of the remedies applied to programming education is to use technology that can vitalize interaction in the classroom, while another is to base teaching increasingly on programming activities. In this article, we present the first results of an exploratory study, in which we teach programming without lectures, exams, or grades, by heavily emphasizing programming activity, and, in a pedagogical sense, student self-direction. This article investigates how students reacted to the active role required of them and what issues emerged in this setting where self-direction was required. The results indicate three issues that should be taken into account when designing a student-driven course: the challenge of supporting students' theoretical synthesis of the topics to be learned, the individual's opportunities for self-direction in a group work setting, and mismatch between individual learning processes and academic course scheduling.},
journal = {ACM Trans. Comput. Educ.},
month = jul,
articleno = {6},
numpages = {21},
keywords = {programming education, Self-direction}
}

@inproceedings{10.1145/2462476.2465589,
author = {McDermott, Roger and Pirie, Iain and Cajander, \r{A}sa and Daniels, Mats and Laxer, Cary},
title = {Investigation into the Personal Epistemology of Computer Science Students},
year = {2013},
isbn = {9781450320788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462476.2465589},
doi = {10.1145/2462476.2465589},
abstract = {In this paper, we investigate the personal epistemology of computing students, that is, their conceptions of knowledge and learning. We review some models of personal epistemological development and describe one of the questionnaire tools that have been used to assess the epistemological beliefs of students studying in other disciplines. We describe an experiment that uses one of these tools, together with exploratory factor analysis, to determine the dimensions of epistemological beliefs of a cohort of computing students and compare the results with that reported in other contexts. The results, while not reproducing the details of previous work, do seem to suggest that there are indeed multiple dimensions to personal epistemology, and that these can be identified, to a large extent, with those recognised by other researchers. Finally, we make some observations about the importance of personal epistemology for learning in Computer Science and outline further work in this area.},
booktitle = {Proceedings of the 18th ACM Conference on Innovation and Technology in Computer Science Education},
pages = {231–236},
numpages = {6},
keywords = {pedagogy, learning, factor analysis, epistemological beliefs, personal epistemology},
location = {Canterbury, England, UK},
series = {ITiCSE '13}
}

@inproceedings{10.1145/2489793.2489796,
author = {Adams, Sam and Bhattacharya, Suparna and Friedlander, Bob and Gerken, John and Kimelman, Doug and Kraemer, Jim and Ossher, Harold and Richards, John and Ungar, David and Wegman, Mark},
title = {Enterprise Context: A Rich Source of Requirements for Context-Oriented Programming},
year = {2013},
isbn = {9781450320405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2489793.2489796},
doi = {10.1145/2489793.2489796},
abstract = {We introduce the domain of enterprise context, as opposed to personal or execution context, and we present requirements for context-oriented programming technology arising out of this broader notion of context.We illustrate enterprise context with scenarios in which data from across an enterprise, as well as data from outside an enterprise, are all brought to bear as context in any situation where they are relevant and can factor into making better decisions and achieving better outcomes.We suggest enterprise context as a rich source of requirements for context-oriented programming models, languages, and virtual machines. In particular, we raise issues such as scale, integration, relevance, temporality, protection, privacy, provenance, policy in general, and valuation. And, for this workshop, we propose enterprise context as one perspective for discussion of new language and VM features: How do proposed features support such a domain?},
booktitle = {Proceedings of the 5th International Workshop on Context-Oriented Programming},
articleno = {3},
numpages = {7},
location = {Montpellier, France},
series = {COP'13}
}

@inproceedings{10.1145/2489837.2489848,
author = {Stucki, Sandro and Amin, Nada and Jonnalagedda, Manohar and Rompf, Tiark},
title = {What Are the Odds? Probabilistic Programming in Scala},
year = {2013},
isbn = {9781450320641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2489837.2489848},
doi = {10.1145/2489837.2489848},
abstract = {Probabilistic programming is a powerful high-level paradigm for probabilistic modeling and inference. We present Odds, a small domain-specific language (DSL) for probabilistic programming, embedded in Scala. Odds provides first-class support for random variables and probabilistic choice, while reusing Scala's abstraction and modularity facilities for composing probabilistic computations and for executing deterministic program parts. Odds accurately represents possibly dependent random variables using a probability monad that models committed choice. This monadic representation of probabilistic models can be combined with a range of inference procedures. We present engines for exact inference, rejection sampling and importance sampling with look-ahead, but other types of solvers are conceivable as well. We evaluate Odds on several non-trivial probabilistic programs from the literature and we demonstrate how the basic probabilistic primitives can be used to build higher-level abstractions, such as rule-based logic programming facilities, using advanced Scala features.},
booktitle = {Proceedings of the 4th Workshop on Scala},
articleno = {11},
numpages = {9},
keywords = {probabilistic programming, probabilistic inference, probability monad, Scala, EDSL},
location = {Montpellier, France},
series = {SCALA '13}
}

@article{10.1145/2480741.2480742,
author = {Manshaei, Mohammad Hossein and Zhu, Quanyan and Alpcan, Tansu and Bac\c{s}ar, Tamer and Hubaux, Jean-Pierre},
title = {Game Theory Meets Network Security and Privacy},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2480741.2480742},
doi = {10.1145/2480741.2480742},
abstract = {This survey provides a structured and comprehensive overview of research on security and privacy in computer and communication networks that use game-theoretic approaches. We present a selected set of works to highlight the application of game theory in addressing different forms of security and privacy problems in computer networks and mobile applications. We organize the presented works in six main categories: security of the physical and MAC layers, security of self-organizing networks, intrusion detection systems, anonymity and privacy, economics of network security, and cryptography. In each category, we identify security problems, players, and game models. We summarize the main results of selected works, such as equilibrium analysis and security mechanism designs. In addition, we provide a discussion on the advantages, drawbacks, and future direction of using game theory in this field. In this survey, our goal is to instill in the reader an enhanced understanding of different research approaches in applying game-theoretic methods to network security. This survey can also help researchers from various fields develop game-theoretic solutions to current and emerging security problems in computer networking.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {25},
numpages = {39},
keywords = {cryptography, wireless security, Game theory, location privacy, revocation, intrusion detection system, multiparty computation, network security and privacy}
}

@article{10.1145/2485984.2485988,
author = {Wouhaybi, Rita H. and Yarvis, Mark D. and Sharma, Sangita and Muse, Philip and Wan, Chieh-Yih and Prasad, Sai and Durham, Lenitra and Sahni, Ritu and Norton, Robert and Curry, Merlin and Jimison, Holly and Harper, Richard and Lowe, Robert A.},
title = {Experiences with Context Management in Emergency Medicine},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/2485984.2485988},
doi = {10.1145/2485984.2485988},
abstract = {In emergency medicine, patient care is intense and stressful, often requiring paramedics to consult with remote physicians to convey the patient's condition. We present a framework for context-management in telemedicine developed in collaboration between engineers, physicians, and paramedics. We describe a mobile platform and embedded wireless sensors to capture physiological and audio context into a comprehensive patient record, accessible locally and remotely. We describe a first evaluation of this technology by trained paramedics in simulated scenarios and evaluate key aspects of system performance. Early results suggest that wireless sensing can provide reliable and low latency data both locally and to remote physicians. In addition, audio context capture is a promising approach to capturing a comprehensive patient record, with a low rate of medically important errors.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jul,
articleno = {100},
numpages = {22},
keywords = {Telemedicine, emergency medicine application, sensor networks, context-aware systems, open platform}
}

@inproceedings{10.1145/2463372.2463548,
author = {Fraser, Gordon and Arcuri, Andrea and McMinn, Phil},
title = {Test Suite Generation with Memetic Algorithms},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463548},
doi = {10.1145/2463372.2463548},
abstract = {Genetic Algorithms have been successfully applied to the generation of unit tests for classes, and are well suited to create complex objects through sequences of method calls. However, because the neighborhood in the search space for method sequences is huge, even supposedly simple optimizations on primitive variables (e.g., numbers and strings) can be ineffective or unsuccessful. To overcome this problem, we extend the global search applied in the EVOSUITE test generation tool with local search on the individual statements of method sequences. In contrast to previous work on local search, we also consider complex datatypes including strings and arrays. A rigorous experimental methodology has been applied to properly evaluate these new local search operators. In our experiments on a set of open source classes of different kinds (e.g., numerical applications and text processing), the resulting test data generation technique increased branch coverage by up to 32% on average over the normal Genetic Algorithm.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1437–1444},
numpages = {8},
keywords = {search-based software engineering, evolutionary testing, evosuite, object-oriented},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@inproceedings{10.5555/2557668.2557679,
author = {Salamati, Farzaneh and Pasek, Zbigniew J.},
title = {Modeling for Personal Well-Being: Time for Paradigm Change},
year = {2013},
isbn = {9781627482752},
publisher = {Society for Modeling &amp; Simulation International},
address = {Vista, CA},
abstract = {The growing demand for health services tied to exploding costs are a growing concern for all national health care systems, which are reaching their limits of volume and case complexities. In the long run reliance on institutional health care delivery systems is not sustainable and may require shifting responsibilities of health management to individuals. Such a shift would require major paradigm change by focusing on individual's well-being and providing tools for its management. For example, by 2015, it is expected that one of every three people worldwide will be overweight and one in ten -- obese. Considering that while preventable, obesity is responsible for over 60% of all leading death causes in developed societies, it is critical to understand the mechanisms that may lead to its control. The obesity model presented in the paper was developed using system dynamics and is based on three-factor models combined into an energy balance equation. The results were tested with weight loss clinic data. Tools for quantified self-tracking may provide an automated source for data collection needed to refine the model and estimated individual characteristics needs as model parameters.},
booktitle = {Proceedings of the 2013 Grand Challenges on Modeling and Simulation Conference},
articleno = {11},
numpages = {5},
keywords = {individualized health care, system dynamics, obesity, self-tracking},
location = {Toronto, Ontario, Canada},
series = {GCMS '13}
}

@inproceedings{10.5555/2557696.2557735,
author = {Bruzzone, Agostino G. and Massei, Marina and Solis, Adriano O. and Poggi, Simonluca and Bartolucci, Christian and Capponi, Lorenzo D'Agostino},
title = {Serious Games as Enablers for Training and Education on Operations on Ships and Off-Shore Platforms},
year = {2013},
isbn = {9781627482769},
publisher = {Society for Modeling &amp; Simulation International},
address = {Vista, CA},
abstract = {This paper is focused on design and development of an advanced serious game for the maritime domain. The authors propose an innovative approach to developing training and educational support for navy vessels and/or off-shore platforms. This serious game is a distributed and multi-user simulation framework that allows the reduction of time, costs, and risks of training, by introducing the crew to operating procedures before having to carry out real tests at sea. The research is based on development of educational programs and virtual frameworks supporting cooperative training for complex operations management.},
booktitle = {Proceedings of the 2013 Summer Computer Simulation Conference},
articleno = {36},
numpages = {8},
keywords = {serious games, interoperable simulation, simulation for training},
location = {Toronto, Ontario, Canada},
series = {SCSC '13}
}

@inproceedings{10.5555/2557696.2557716,
author = {Christodoulou, Smaragda and Michael, Despina and Gregoriades, Andreas and Pampaka, Maria},
title = {Design of a 3D Interactive Simulator for Driver Behavior Analysis},
year = {2013},
isbn = {9781627482769},
publisher = {Society for Modeling &amp; Simulation International},
address = {Vista, CA},
abstract = {Diagnosing the causes of road accidents and the development of effective countermeasures to reduce accident rates is of key importance in road safety. Human error is one of the principal influencing factors that lead to road accidents, and is attributed to increased mental workload induced by distractions. Workload, however, is characterized by intrinsic properties that are difficult to observe. Hence, phenotype behaviors, such as lane deviations, could act as good predictors of driver workload. Driving simulators emerged as a promising technology for the analysis of driving conditions and road users' behavior in an attempt to tackle the problem of road accidents. However, the cost of designing or owning a simulator to conduct a safety analysis is prohibitive for many government agencies. The work presented herein demonstrates the design and development of a driving simulator, using a 3D game engine that aims to contribute towards evaluating black spots in road networks by promoting rapid design of realistic models and facilitating the specification of test scenarios. The developed simulator was employed to conduct a set of preliminary experiments that analyzed driving behaviors of local road users for a chosen black spot in a road network in Limassol-Cyprus. Data collected from the experiments are analyzed, results are presented and conclusions are drawn.},
booktitle = {Proceedings of the 2013 Summer Computer Simulation Conference},
articleno = {17},
numpages = {8},
keywords = {distractions, human error, driving simulator, workload},
location = {Toronto, Ontario, Canada},
series = {SCSC '13}
}

@article{10.1145/2522968.2522979,
author = {Sakr, Sherif and Liu, Anna and Fayoumi, Ayman G.},
title = {The Family of Mapreduce and Large-Scale Data Processing Systems},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2522968.2522979},
doi = {10.1145/2522968.2522979},
abstract = {In the last two decades, the continuous increase of computational power has produced an overwhelming flow of data which has called for a paradigm shift in the computing architecture and large-scale data processing mechanisms. MapReduce is a simple and powerful programming model that enables easy development of scalable parallel applications to process vast amounts of data on large clusters of commodity machines. It isolates the application from the details of running a distributed program such as issues on data distribution, scheduling, and fault tolerance. However, the original implementation of the MapReduce framework had some limitations that have been tackled by many research efforts in several followup works after its introduction. This article provides a comprehensive survey for a family of approaches and mechanisms of large-scale data processing mechanisms that have been implemented based on the original idea of the MapReduce framework and are currently gaining a lot of momentum in both research and industrial communities. We also cover a set of introduced systems that have been implemented to provide declarative programming interfaces on top of the MapReduce framework. In addition, we review several large-scale data processing systems that resemble some of the ideas of the MapReduce framework for different purposes and application scenarios. Finally, we discuss some of the future research directions for implementing the next generation of MapReduce-like solutions.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {11},
numpages = {44},
keywords = {MapReduce, large-scale data processing, big data}
}

@article{10.1145/2522968.2522981,
author = {Silva, Jonathan A. and Faria, Elaine R. and Barros, Rodrigo C. and Hruschka, Eduardo R. and Carvalho, Andr\'{e} C. P. L. F. de and Gama, Jo\~{a}o},
title = {Data Stream Clustering: A Survey},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2522968.2522981},
doi = {10.1145/2522968.2522981},
abstract = {Data stream mining is an active research area that has recently emerged to discover knowledge from large amounts of continuously generated data. In this context, several data stream clustering algorithms have been proposed to perform unsupervised learning. Nevertheless, data stream clustering imposes several challenges to be addressed, such as dealing with nonstationary, unbounded data that arrive in an online fashion. The intrinsic nature of stream data requires the development of algorithms capable of performing fast and incremental processing of data objects, suitably addressing time and memory limitations. In this article, we present a survey of data stream clustering algorithms, providing a thorough discussion of the main design components of state-of-the-art algorithms. In addition, this work addresses the temporal aspects involved in data stream clustering, and presents an overview of the usually employed experimental methodologies. A number of references are provided that describe applications of data stream clustering in different domains, such as network intrusion detection, sensor networks, and stock market analysis. Information regarding software packages and data repositories are also available for helping researchers and practitioners. Finally, some important issues and open questions that can be subject of future research are discussed.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {13},
numpages = {31},
keywords = {online clustering, Data stream clustering}
}

@article{10.1145/2483969.2483974,
author = {Klebanov, Beata Beigman and Burstein, Jill and Madnani, Nitin},
title = {Sentiment Profiles of Multiword Expressions in Test-Taker Essays: The Case of Noun-Noun Compounds},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1550-4875},
url = {https://doi.org/10.1145/2483969.2483974},
doi = {10.1145/2483969.2483974},
abstract = {The property of idiomaticity vs. compositionality of a multiword expression traditionally pertains to the semantic interpretation of the expression. In this article, we consider this property as it applies to the expression's sentiment profile (relative degree of positivity, negativity, and neutrality). Thus, while heart attack is idiomatic in terms of semantic interpretation, the sentiment profile of the expression (strongly negative) can, in fact, be determined from the strongly negative profile of the head word. In this article, we (1) propose a way to measure compositionality of a multiword expression's sentiment profile, and perform the measurement on noun-noun compounds; (2) evaluate the utility of using sentiment profiles of noun-noun compounds in a sentence-level sentiment classification task. We find that the sentiment profiles of noun-noun compounds in test-taker essays tend to be highly compositional and that their incorporation improves the performance of a sentiment classification system.},
journal = {ACM Trans. Speech Lang. Process.},
month = jul,
articleno = {12},
numpages = {15},
keywords = {positive, idiom, sentiment, sentiment polarity, sentiment analysis, multi-word expression, negative, neutral, compound, Discourse, noun-noun compound, MWE}
}

