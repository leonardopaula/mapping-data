@inproceedings{10.1145/2037556.2037607,
author = {Kaschesky, Michael and Sobkowicz, Pawel and Bouchard, Guillaume},
title = {Opinion Mining in Social Media: Modeling, Simulating, and Visualizing Political Opinion Formation in the Web},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037556.2037607},
doi = {10.1145/2037556.2037607},
abstract = {Affordable and ubiquitous online communications (social media) provide the means for flows of ideas and opinions and play an increasing role for the transformation and cohesion of society - yet little is understood about how online opinions emerge, diffuse, and gain momentum. To address this problem, an opinion formation framework based on content analysis of social media and sociophysical system modeling is proposed. Based on prior research and own projects, three building blocks of online opinion tracking and simulation are described: (1) automated topic and opinion detection in real-time, (2) topic and opinion modeling and agent-based simulation, and (3) visualizations of topic and opinion networks. Finally, two application scenarios are presented to illustrate the framework and motivate further research.},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
pages = {317–326},
numpages = {10},
keywords = {design, economics, standardization, measurement, reliability, languages, experimentation, management, human factors},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/2037556.2037581,
author = {Linders, Dennis},
title = {We-Government: An Anatomy of Citizen Coproduction in the Information Age},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037556.2037581},
doi = {10.1145/2037556.2037581},
abstract = {This paper examines whether the tools of the Information Age---principally but not exclusively the Internet---make citizen coproduction of government services more viable and effective. The paper first discusses the re-emergence of citizen coproduction as a fashionable policy option in the face of persistent budget deficits, the rise of "government by network," and the advent of mass "peer-production." Finding a plethora of competing labels, models, and concepts for Internet-facilitated coproduction, the paper proposes a formal taxonomy to provide a more robust framework for systematic analysis. The paper then applies this framework to evaluate the impact of the tools of the Information Age on citizen coproduction. Its findings cautiously support the claim that the Information Age enables and advances new forms of citizen coproduction, namely large-scale "Do-It-Yourself Government" and "Government as a Platform." The paper concludes with a discussion of the potential implications for public administration, including the possible emergence of a new social contract that empowers the public to play a far more active role in the functioning of their government.},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
pages = {167–176},
numpages = {10},
keywords = {service delivery, information age, public administration, e-government, government as platform, collaborative governance, crowdsourcing, digital government, coproduction},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/1989323.1989441,
author = {Silberstein, Adam E. and Sears, Russell and Zhou, Wenchao and Cooper, Brian Frank},
title = {A Batch of PNUTS: Experiences Connecting Cloud Batch and Serving Systems},
year = {2011},
isbn = {9781450306614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1989323.1989441},
doi = {10.1145/1989323.1989441},
abstract = {Cloud data management systems are growing in prominence, particularly at large Internet companies like Google, Yahoo!, and Amazon, which prize them for their scalability and elasticity. Each of these systems trades off between low-latency serving performance and batch processing throughput. In this paper, we discuss our experience running batch-oriented Hadoop on top of Yahoo's serving-oriented PNUTS system instead of the standard HDFS file system. Though PNUTS is optimized for and primarily used for serving, a number of applications at Yahoo! must run batch-oriented jobs that read or write data that is stored in PNUTS.Combining these systems reveals several key areas where the fundamental properties of each system are mismatched. We discuss our approaches to accommodating these mismatches, by either bending the batch and serving abstractions, or inventing new ones. Batch systems like Hadoop provide coarse task-level recovery, while serving systems like PNUTS provide finer record or transaction-level recovery. We combine both types to log record-level errors, while detecting and recovering from large-scale errors. Batch systems optimize for read and write throughput of large requests, while serving systems use indexing to provide low latency access to individual records. To improve latency-insensitive write throughput to PNUTS, we introduce a batch write path. The systems provide conflicting consistency models, and we discuss techniques to isolate them from one another.},
booktitle = {Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data},
pages = {1101–1112},
numpages = {12},
keywords = {hadoop, serving, bulk load, hybrid, PNUTS, batch},
location = {Athens, Greece},
series = {SIGMOD '11}
}

@inproceedings{10.1145/1998076.1998145,
author = {Lagoze, Carl and Patzke, Karin},
title = {A Research Agenda for Data Curation Cyberinfrastructure},
year = {2011},
isbn = {9781450307444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1998076.1998145},
doi = {10.1145/1998076.1998145},
abstract = {In 2008, the National Science Foundation released the DataNet solicitation, which presents an ambitious vision for a comprehensive data curation cyberinfrastructure in support of fourth paradigm science. The program subsequently funded two projects, DataONE and the Data Conservancy. The authors put forth an uncertainty framework for understanding the larger socio-cultural issues that influence the progress of DataNet projects and cyberinfrastructure projects in general. This framework highlights the key technical, organizational, scientific, and institutional contexts that the projects must consider as they mature.},
booktitle = {Proceedings of the 11th Annual International ACM/IEEE Joint Conference on Digital Libraries},
pages = {373–382},
numpages = {10},
keywords = {cyberinfrastructure, data curation},
location = {Ottawa, Ontario, Canada},
series = {JCDL '11}
}

@inproceedings{10.1145/1998196.1998203,
author = {Aanjaneya, Mridul and Chazal, Frederic and Chen, Daniel and Glisse, Marc and Guibas, Leonidas J. and Morozov, Dmitriy},
title = {Metric Graph Reconstruction from Noisy Data},
year = {2011},
isbn = {9781450306829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1998196.1998203},
doi = {10.1145/1998196.1998203},
abstract = {Many real-world data sets can be viewed of as noisy samples of special types of metric spaces called metric graphs [16]. Building on the notions of correspondence and Gromov-Hausdorff distance in metric geometry, we describe a model for such data sets as an approximation of an underlying metric graph. We present a novel algorithm that takes as an input such a data set, and outputs the underlying metric graph with guarantees. We also implement the algorithm, and evaluate its performance on a variety of real world data sets.},
booktitle = {Proceedings of the Twenty-Seventh Annual Symposium on Computational Geometry},
pages = {37–46},
numpages = {10},
keywords = {reconstruction, metric graph, noise, inference},
location = {Paris, France},
series = {SoCG '11}
}

@inproceedings{10.1145/2527031.2527046,
author = {White, Su and Croitoru, Madalina and Bazan, St\'{e}phane and Cerri, Stefano and Davis, Hugh C. and Folgieri, Raffaella and Jonquet, Clement and Scharffe, Fran\c{c}ois and Staab, Steffan and Tiropanis, Thanassis and Vafopoulos, Michalis},
title = {Negotiating the Web Science Curriculum through Shared Educational Artefacts},
year = {2011},
isbn = {9781450308557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2527031.2527046},
doi = {10.1145/2527031.2527046},
abstract = {The far-reaching impact of the Web on society is widely recognised. The interdisciplinary study of this impact has crystallised in the field of study known as Web Science. However, defining an agreed, shared understanding of what constitutes web science requires complex negotiation and translations of understandings across component disciplines, national cultures and educational traditions. Some individual institutions have already established particular curricula, and discussions in the Web Science Curriculum Workshop series have marked the territory to some extent. This paper reports on a process being adopted across a consortium of partners to systematically create a shared understanding of what constitutes web science. It records and critiques the processes instantiated to agree a common curriculum, and presents a framework for future discussion and development.},
booktitle = {Proceedings of the 3rd International Web Science Conference},
articleno = {11},
numpages = {8},
keywords = {web science curriculum, co-creation, negotiated curriculum, web science education, educational repository},
location = {Koblenz, Germany},
series = {WebSci '11}
}

@inproceedings{10.5555/2132890.2132905,
author = {Baumann, Timo and Schlangen, David},
title = {Predicting the Micro-Timing of User Input for an Incremental Spoken Dialogue System That Completes a User's Ongoing Turn},
year = {2011},
isbn = {9781937284107},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present the novel task of predicting temporal features of continuations of user input, while that input is still ongoing. We show that the remaining duration of an ongoing word, as well as the duration of the next can be predicted reasonably well, and we put this information to use in a system that synchronously completes a user's speech. While we focus on collaborative completions, the techniques presented here may also be useful for the alignment of back-channels and immediate turn-taking in an incremental SDS, or to synchronously monitor the user's speech fluency for other reasons.},
booktitle = {Proceedings of the SIGDIAL 2011 Conference},
pages = {120–129},
numpages = {10},
location = {Portland, Oregon},
series = {SIGDIAL '11}
}

@inproceedings{10.5555/2002736.2002754,
author = {Reddy, Sravana and Knight, Kevin},
title = {Unsupervised Discovery of Rhyme Schemes},
year = {2011},
isbn = {9781932432886},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation.},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2},
pages = {77–82},
numpages = {6},
location = {Portland, Oregon},
series = {HLT '11}
}

@inproceedings{10.1145/2000259.2000269,
author = {Hauck, Michael and Kuperberg, Michael and Huber, Nikolaus and Reussner, Ralf},
title = {Ginpex: Deriving Performance-Relevant Infrastructure Properties through Goal-Oriented Experiments},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000269},
doi = {10.1145/2000259.2000269},
abstract = {In software performance engineering, the infrastructure on which an application is running plays a crucial role when predicting the performance of the application. Thus, to yield accurate prediction results, performance-relevant properties and behaviour of the infrastructure have to be integrated into performance models. However, capturing these properties is a cumbersome and error-prone task, as it requires carefully engineered measurements and experiments. Existing approaches for creating infrastructure performance models require manual coding of these experiments, or ignore the detailed properties in the models. The contribution of this paper is the Ginpex approach, which introduces goal-oriented and model-based specification and generation of executable performance experiments for detecting and quantifying performance relevant infrastructure properties. Ginpex provides a metamodel for experiment specification and comes with pre-defined experiment templates that provide automated experiment execution on the target platform and also automate the evaluation of the experiment results. We evaluate Ginpex using two case studies, where experiments are executed to detect the operating system scheduler timeslice length, and to quantify the CPU virtualization overhead for an application executed in a virtualized environment.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {53–62},
numpages = {10},
keywords = {experiments, metamodel, measurements, performance prediction, infrastructure},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/2181101.2181127,
author = {Di Leva, Ciro and Guardascione, Isaia and Newton, Frederick},
title = {International Mission in Kosovo: Program, Project and Knowledge Management},
year = {2011},
isbn = {9781450307833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2181101.2181127},
doi = {10.1145/2181101.2181127},
abstract = {This document is based on the meaningful experience of the authors who are experienced in International Peacekeeping Missions. Its aim is to help improve the skills related to Project and Knowledge Management disciplines required during the reconstruction phase of a country, like Kosovo, which is trying to take on a new lease of life after many years of internal conflict.With this knowledge it is hoped to de-conflict organizational and practice overlaps incurred by the International donor community and other stakeholders in their short, middle and long term investment plans.},
booktitle = {Proceedings of the 12th International Conference on Product Focused Software Development and Process Improvement},
pages = {120–127},
numpages = {8},
keywords = {mission, knowledge management, strategic plan, program and project management, intelligence},
location = {Torre Canne, Brindisi, Italy},
series = {Profes '11}
}

@inproceedings{10.1145/1999030.1999038,
author = {Lamberty, K. K. and Adams, Stephen and Biatek, Jason and Froiland, Katherine and Lapham, Jay},
title = {Using a Large Display in the Periphery to Support Children Learning through Design},
year = {2011},
isbn = {9781450307512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999030.1999038},
doi = {10.1145/1999030.1999038},
abstract = {Learners benefit from creating personally meaningful artifacts for an audience, especially when those artifacts embody the concepts that the learners aim to understand. Our past work examined artistic design as an anchor for mathematical discussions between learners as they explored concepts like symmetry and fractions. In this paper, we present the work from two field studies where we explored ways to expand opportunities for mathematical discussions with a larger audience (beyond learners seated next to each other) by incorporating structured ways to share finished and in-progress work on a large display that all participants could view peripherally. We observed that the large display provided support for sharing designs with children anywhere in the room. The large display increased the students' awareness of their peers' designs, and the displayed designs became an anchor for their discussions. More work remains to increase the frequency of discussions about mathematical aspects of their artifacts.},
booktitle = {Proceedings of the 10th International Conference on Interaction Design and Children},
pages = {62–71},
numpages = {10},
keywords = {ambient, awareness, large display, collaboration, math, reflection, children, engagement, learning, peripheral display},
location = {Ann Arbor, Michigan},
series = {IDC '11}
}

@inproceedings{10.1145/2347504.2347533,
author = {Karahano\u{g}lu, Arma\u{g}an and Erbu\u{g}, \c{C}i\u{g}dem},
title = {Perceived Qualities of Smart Wearables: Determinants of User Acceptance},
year = {2011},
isbn = {9781450312806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2347504.2347533},
doi = {10.1145/2347504.2347533},
abstract = {Wearable computers are one of the new technologies that are expected to be a part of users' lives extensively in near future. While some of the users have positive attitudes towards these new products, some users may reject to use them due to different reasons. User experience is subjective, and effected by various parameters. Among these the first impression, namely the perceived qualities has an important impact on product acceptance. This paper aims to explore the perceived qualities of wearables and define the relations between them. An empirical study is conducted, to find out the hierarchy and meaningful relationships between the perceived qualities of smart wearables. The study is based on personal construct theory and data is presented by Cross-Impact Analysis. The patterns behind affection and affected qualities are explored to understand the design requirements for the best integration of wearables into daily lives.},
booktitle = {Proceedings of the 2011 Conference on Designing Pleasurable Products and Interfaces},
articleno = {26},
numpages = {8},
keywords = {human values, smart wearables, experience design, perceived qualities, user preferences},
location = {Milano, Italy},
series = {DPPI '11}
}

@inproceedings{10.1145/2347504.2347511,
author = {Zhang, Xiao and Wakkary, Ron},
title = {Design Analysis: Understanding e-Waste Recycling by Generation Y},
year = {2011},
isbn = {9781450312806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2347504.2347511},
doi = {10.1145/2347504.2347511},
abstract = {This paper aims to understand e-waste recycling behavior of Generation Y. It presents a pilot study that explores this generation's e-waste recycling practices, their attitudes towards e-waste recycling, and the barriers to e-waste recycling. The findings reveal the complexity of the actual e-waste recycling behavior, many participants in this study hold a positive attitude towards e-waste recycling, yet there is a shortage of convenient recycling options and e-waste recycling information. Based on the Motivation-Opportunity-Abilities model, this paper also uncovers the decision-making process involved in each recycling action. We use these findings to present a preliminary analysis of design implications to provoke design ideas and services that support e-waste recycling, and discuss our further research direction.},
booktitle = {Proceedings of the 2011 Conference on Designing Pleasurable Products and Interfaces},
articleno = {6},
numpages = {8},
keywords = {recycling action, design, recycling behavior, e-waste, recycling, attitude},
location = {Milano, Italy},
series = {DPPI '11}
}

@inproceedings{10.5555/2021153.2021157,
author = {Kolya, Anup Kumar and Das, Dipankar and Ekbal, Asif and Bandyopadhyay, Sivaji},
title = {Identifying Event: Sentiment Association Using Lexical Equivalence and Co-Reference Approaches},
year = {2011},
isbn = {9781932432985},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we have identified event and sentiment expressions at word level from the sentences of TempEval-2010 corpus and evaluated their association in terms of lexical equivalence and co-reference. A hybrid approach that consists of Conditional Random Field (CRF) based machine learning framework in conjunction with several rule based strategies has been adopted for event identification within the TimeML framework. The strategies are based on semantic role labeling, WordNet relations and some handcrafted rules. The sentiment expressions are identified simply based on the cues that are available in the sentiment lexicons such as Subjectivity Wordlist, SentiWordNet and WordNet Affect. The identification of lexical equivalence between event and sentiment expressions based on the part-of-speech (POS) categories is straightforward. The emotional verbs from VerbNet have also been employed to improve the coverage of lexical equivalence. On the other hand, the association of sentiment and event has been analyzed using the notion of co-reference. The parsed dependency relations along with basic rhetoric knowledge help to identify the co-reference between event and sentiment expressions. Manual evaluation on the 171 sentences of TempEval-2010 dataset yields the precision, recall and F-Score values of 61.25%, 70.29% and 65.23% respectively.},
booktitle = {Proceedings of the ACL 2011 Workshop on Relational Models of Semantics},
pages = {19–27},
numpages = {9},
location = {Portland, Oregon},
series = {RELMS '11}
}

@inproceedings{10.5555/2107653.2107678,
author = {G\^{\i}nsc\u{A}, Alexandru-Lucian and Boro\c{s}, Emanuela and Iftene, Adrian and Trandab\u{A}\c{t}, Diana and Toader, Mihai and Cor\^{\i}ci, Marius and Perez, Cenel-Augusto and Cristea, Dan},
title = {Sentimatrix: Multilingual Sentiment Analysis Service},
year = {2011},
isbn = {9781937284060},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper describes the preliminary results of a system for extracting sentiments opinioned with regard with named entities. It also combines rule-based classification, statistics and machine learning in a new method. The accuracy and speed of extraction and classification are crucial. The service oriented architecture permits the end-user to work with a flexible interface in order to produce applications that range from aggregating consumer feedback on commercial products to measuring public opinion on political issues from blog and forums. The experiment has two versions available for testing, one with concrete extraction results and sentiment calculus and the other with internal metrics validation results.},
booktitle = {Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis},
pages = {189–195},
numpages = {7},
location = {Portland, Oregon},
series = {WASSA '11}
}

@inproceedings{10.5555/2107653.2107673,
author = {Russo, Irene and Caselli, Tommaso and Rubino, Francesco and Boldrini, Ester and Mart\'{\i}nez-Barco, Patricio},
title = {EMOCause: An Easy-Adaptable Approach to Emotion Cause Contexts},
year = {2011},
isbn = {9781937284060},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper we present a method to automatically identify linguistic contexts which contain possible causes of emotions or emotional states from Italian newspaper articles (La Repubblica Corpus). Our methodology is based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge on emotional states and emotion eliciting situations. Our approach has been evaluated with respect to manually annotated data. The results obtained so far are satisfying and support the validity of the methodology proposed.},
booktitle = {Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis},
pages = {153–160},
numpages = {8},
location = {Portland, Oregon},
series = {WASSA '11}
}

@inproceedings{10.1145/1999747.1999817,
author = {Feaster, Yvon and Segars, Luke and Wahba, Sally K. and Hallstrom, Jason O.},
title = {Teaching CS Unplugged in the High School (with Limited Success)},
year = {2011},
isbn = {9781450306973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999747.1999817},
doi = {10.1145/1999747.1999817},
abstract = {CS Unplugged is a set of active learning activities designed to introduce fundamental computer science principles without the use of computers. The program has gained significant momentum in recent years, with proponents citing deep engagement and enjoyment benefits. With these benefits in mind, we initiated a one-year outreach program involving a local high school, using the CS Unplugged program as the foundation. To our disappointment, the results were at odds with our enthusiasm --- significantly. In this paper, we describe our approach to adapting the CS Unplugged materials for use at the high school level, present our experiences teaching it, and summarize the results of our evaluation.},
booktitle = {Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education},
pages = {248–252},
numpages = {5},
keywords = {experimental evaluation, high school curriculum, cs unplugged, computer science outreach},
location = {Darmstadt, Germany},
series = {ITiCSE '11}
}

@inproceedings{10.1145/1999747.1999769,
author = {Wolz, Ursula and Milazzo, Michael and Stone, Meredith},
title = {Kinesthetic Learning of Computing via "off-Beat" Activities},
year = {2011},
isbn = {9781450306973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999747.1999769},
doi = {10.1145/1999747.1999769},
abstract = {To broaden participation in computing requires exposing students to a variety of experiences. CS Unplugged promotes learning through kinesthetic activities. This paper identifies three types of learning (1) problem solving, (2) creative construction, and (3) open-ended invention, that lend themselves to activities that engage middle school students in physical movement to learn computing. Via surveys and a personality traits activity "True Colors" we examined whether self-identified personality types were predisposed to particular kinesthetic learning activities. Our results suggest that personality type as defined by True Colors does not predict selection of an activity type. Furthermore, the students in our summer Interactive Journalism Institute were significantly more predisposed to pick open-ended invention. These results suggest directions in which K-12 computing curriculum should take to reach the broadest constituency.},
booktitle = {Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education},
pages = {68–72},
numpages = {5},
keywords = {kinesthetic learning, cs unplugged, scratch, broadening participation in cs, interactive journalism, computational thinking},
location = {Darmstadt, Germany},
series = {ITiCSE '11}
}

@inproceedings{10.1145/1999916.1999923,
author = {Falaki, Hossein and Mahajan, Ratul and Estrin, Deborah},
title = {SystemSens: A Tool for Monitoring Usage in Smartphone Research Deployments},
year = {2011},
isbn = {9781450307406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999916.1999923},
doi = {10.1145/1999916.1999923},
abstract = {By deploying several research applications, we found that capturing usage context (e.g., CPU and memory) is highly valuable for debugging and interpreting results, even if that context information appears unrelated to the application. We have developed a general tool called SystemSens to help researchers capture usage context in their deployments in an extendible way. This paper describes and motivates the design choices underlying our tool and evaluates its overheads in terms of phone resources and data.},
booktitle = {Proceedings of the Sixth International Workshop on MobiArch},
pages = {25–30},
numpages = {6},
keywords = {deployment, smartphone, android},
location = {Bethesda, Maryland, USA},
series = {MobiArch '11}
}

@inproceedings{10.1145/1999995.2000000,
author = {Ra, Moo-Ryong and Sheth, Anmol and Mummert, Lily and Pillai, Padmanabhan and Wetherall, David and Govindan, Ramesh},
title = {Odessa: Enabling Interactive Perception Applications on Mobile Devices},
year = {2011},
isbn = {9781450306430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1999995.2000000},
doi = {10.1145/1999995.2000000},
abstract = {Resource constrained mobile devices need to leverage computation on nearby servers to run responsive applications that recognize objects, people, or gestures from real-time video. The two key questions that impact performance are what computation to offload, and how to structure the parallelism across the mobile device and server. To answer these questions, we develop and evaluate three interactive perceptual applications. We find that offloading and parallelism choices should be dynamic, even for a given application, as performance depends on scene complexity as well as environmental factors such as the network and device capabilities. To this end we develop Odessa, a novel, lightweight, runtime that automatically and adaptively makes offloading and parallelism decisions for mobile interactive perception applications. Our evaluation shows that the incremental greedy strategy of Odessa converges to an operating point that is close to an ideal offline partitioning. It provides more than a 3x improvement in application performance over partitioning suggested by domain experts. Odessa works well across a variety of execution environments, and is agile to changes in the network, device and application inputs.},
booktitle = {Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services},
pages = {43–56},
numpages = {14},
keywords = {offloading, incremental partitioning, mobile perception application, video processing, parallel processing},
location = {Bethesda, Maryland, USA},
series = {MobiSys '11}
}

@inproceedings{10.1145/2159365.2159374,
author = {Canossa, Alessandro and Drachen, Anders and S\o{}rensen, Janus Rau M\o{}ller},
title = {Arrrgghh!!! Blending Quantitative and Qualitative Methods to Detect Player Frustration},
year = {2011},
isbn = {9781450308045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2159365.2159374},
doi = {10.1145/2159365.2159374},
abstract = {Frustration, in small, calibrated doses, can be integral to an enjoyable game experience, but it is a very delicate balance: just a slightly excessive amount of frustration could compel players to terminate prematurely the experience. Another factor with high relevance when analyzing player frustration is the difference in personality between players: some are less willing to endure frustration and might give up on the game earlier than others. This article seeks to identify patterns of behavior that could point to potential frustration before players resolve to quit a game. The method should be applicable independently from the personalities of different players. Furthermore, in order for this method to be relevant during game production, it has been decided to avoid relying on large numbers of players, and instead depend on highly granular data and both qualitative approaches (direct observation of players) and quantitative research (data mining gameplay metrics). The result is a computational model of player frustration that, although applied to a single game (Kane &amp; Lynch 2), is able to raise a red flag whenever a sequence of actions in the game could be interpreted as possible player frustration.},
booktitle = {Proceedings of the 6th International Conference on Foundations of Digital Games},
pages = {61–68},
numpages = {8},
keywords = {qualitative and quantitative research methods, patterns of behavior, game development, user experience, game design, small sample size, gameplay metrics, player frustration},
location = {Bordeaux, France},
series = {FDG '11}
}

@inproceedings{10.1145/2000119.2000126,
author = {Lino, Natasha and Siebra, Clauirton and Ara\'{u}jo, Jonatas and Anabuki, Davi and Patricio, Jos\'{e} and Batista, Marcelle and N\'{o}brega, Ramon and Amaro, Manoel and Lemos, Guido},
title = {Knowledge Tv},
year = {2011},
isbn = {9781450306027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000119.2000126},
doi = {10.1145/2000119.2000126},
abstract = {While iTV content is aimed at human manipulation, it does not appropriately support the performance of computational processes, such as search engines. The principal problem is that the meaning of multimedia documents cannot be explored, reducing the actuation limit of services and applications. In this context, this project proposes the definition of a semantic layer to the iTV platform, based on the Semantic Web concepts, which organizes its content and offers intelligent services, such as semantic queries and recommendation. Thus, this project tries to avoid the same problems that are currently presented by the Web platform, following the Semantic Web trend and increasing the potential to the development of more advanced applications. This technology is being developed in accordance with the Ginga project, which is the official specification for the Brazilian iTV middleware.},
booktitle = {Proceedings of the 9th European Conference on Interactive TV and Video},
pages = {29–38},
numpages = {10},
keywords = {kdd, semantic web, semantic services},
location = {Lisbon, Portugal},
series = {EuroITV '11}
}

@inproceedings{10.1145/2103354.2103356,
author = {Burke, Moira and Settles, Burr},
title = {Plugged in to the Community: Social Motivators in Online Goal-Setting Groups},
year = {2011},
isbn = {9781450308243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103354.2103356},
doi = {10.1145/2103354.2103356},
abstract = {At personal goal-setting websites, people join others in committing to a challenging goal, such as losing ten pounds or writing a novel in a month. Despite the popularity of these online communities, we know little about whether or how they improve goal performance. Based on theories of goal-setting and group attachment, we examine the influence of two social factors in an online "songwriting challenge" community: early feedback evoking a shared social identity, and one-on-one collaborations with other members. Combining five years of longitudinal behavioral data with member surveys, we find that users who engage in these social features perform better on their goals than those who are non-social. Furthermore, these early social experiences are associated with strong community-centric behaviors in the long term, including donating money and providing feedback to others.},
booktitle = {Proceedings of the 5th International Conference on Communities and Technologies},
pages = {1–10},
numpages = {10},
keywords = {feedback, social facilitation, collaboration, goal-setting},
location = {Brisbane, Australia},
series = {C&amp;T '11}
}

@article{10.1145/1970378.1970379,
author = {Lindtner, Silvia and Chen, Judy and Hayes, Gillian R. and Dourish, Paul},
title = {Towards a Framework of Publics: Re-Encountering Media Sharing and Its User},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/1970378.1970379},
doi = {10.1145/1970378.1970379},
abstract = {Design and evaluation of user-generated media production and sharing in Human-Computer Interaction (HCI) often focus on formal and informal media sharing, such as communication within social networks, automatic notifications of activities, and the exchange of digital artifacts. However, conceptual tools for understanding how people relate to the audiences they reach through these systems are limited. The increasing interest in user-generated content in HCI demands the infusion of new methods and theories that explicitly engage the construction and use of media within and among large groups of individuals and systems. In this paper, we suggest that the notion of “publics,” drawn from media theory, provides useful insights into user-driven, social, and cultural forms of technology use and digital content creation. We illustrate this by employing the notion of publics to the findings from a two-month deployment of a mobile photo sharing platform in a youth housing community. The results of this empirical work coupled with a theoretical examination of publics stimulate reflection on prevailing interpretations of user-designer-reader roles. The paper provides an outlook for potentially new and productive ways of understanding interdependencies within those activities. Implications that can be drawn from this work concern the role of digital media creation and sharing for the formation of collectives and how people position themselves collectively in relation to larger social groups and societal norms. The analysis suggests fruitful crossovers among HCI, Media Theory and New Media Research by approaching the user as both consumer and producer of digital content.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jul,
articleno = {5},
numpages = {23},
keywords = {Public, media sharing, media theory, social networking, photo sharing}
}

@inproceedings{10.1145/2016551.2016552,
author = {Ferrari, Laura and Mamei, Marco and Zambonelli, Franco},
title = {"All-about" Diaries: Concepts and Experiences},
year = {2011},
isbn = {9781450305600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016551.2016552},
doi = {10.1145/2016551.2016552},
abstract = {Smart phones and pervasive computing technologies enable the vision of all-about diaries: tools for recording, in a browsable and machine-processable format, the everyday activities and events of people, communities, objects and places. Diaries offer a wealth of opportunities for consumers and industries. Yet, while proposals exist indicating promising approaches to implement parts of them, several challenges still have to be faced to produce fully-edged working systems. In this paper we discuss opportunities and technologies that enable such diaries to be created. Then, we present a prototype of a diary based on location data.},
booktitle = {Proceedings of the 5th International Conference on Communication System Software and Middleware},
articleno = {1},
numpages = {11},
keywords = {context awareness, pattern analysis location-based services, life logging, pervasive computing},
location = {Verona, Italy},
series = {COMSWARE '11}
}

@inproceedings{10.1145/2002259.2002299,
author = {Stojanovic, Nenad and Milenovic, Dejan and Xu, Yongchun and Stojanovic, Ljiljana and Anicic, Darko and Studer, Rudi},
title = {An Intelligent Event-Driven Approach for Efficient Energy Consumption in Commercial Buildings: Smart Office Use Case},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002299},
doi = {10.1145/2002259.2002299},
abstract = {In this paper we present a use case related to the intelligent processing of events coming from the conventional ("cheap") sensors in order to support better energy consumption in commercial buildings. The approach has been implemented using our iCEP framework and deployed in the office space of a real working environment. This research is a kind of the proof of the concept for a new technology that the industry partner would like to exploit.The results are very encouraging: smart decisions for the efficient usage of energy can be made by the intelligent processing of "cheap" sensor events.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {303–312},
numpages = {10},
keywords = {intelligent complex event processing, smart office, energy efficiency},
location = {New York, New York, USA},
series = {DEBS '11}
}

@inproceedings{10.1145/2002259.2002279,
author = {Engel, Yagil and Etzion, Opher},
title = {Towards Proactive Event-Driven Computing},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002279},
doi = {10.1145/2002259.2002279},
abstract = {Event driven architecture is a paradigm shift from traditional computing architectures which employ synchronous, request-response interactions. In this paper we introduce a conceptual architecture for what can be considered the next phase of that evolution: proactive event-driven computing. Proactivity refers to the ability to mitigate or eliminate undesired future events, or to identify and take advantage of future opportunities, by applying prediction and automated decision making technologies. We investigate an extension of the event processing conceptual model and architecture to support proactive event-driven applications, and propose the main building blocks of a novel architecture. We first describe several extensions to the existing event processing functionality that is required to support proactivity; next, we extend the event processing agent model to include two more type of agents: predictive agents that may derive future uncertain events based on prediction models, and proactive agents that compute the best proactive action that should be taken. Those building blocks are demonstrated through a comprehensive scenario that deals with proactive decision making, ensuring timely delivery of critical material for a production plant.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {125–136},
numpages = {12},
keywords = {event processing, proactive computing},
location = {New York, New York, USA},
series = {DEBS '11}
}

@inproceedings{10.1145/2002259.2002293,
author = {Dindar, Nihal and Fischer, Peter M. and Soner, Merve and Tatbul, Nesime},
title = {Efficiently Correlating Complex Events over Live and Archived Data Streams},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002293},
doi = {10.1145/2002259.2002293},
abstract = {Correlating complex events over live and archived data streams, which we call Pattern Correlation Queries (PCQs), provides many benefits for domains which need real time forecasting of events or identification of causal dependencies, while handling data at high rates and in massive amounts, like in financial or medical settings. Existing work has focused either on complex event processing over a single type of stream source (i.e., either live or archived), or on simple stream correlation queries (e.g., live events trigerring a database lookup). In this paper, we specifically focus on recency-based PCQs and provide clear, useful, and optimizable semantics for them. PCQs raise a number of challenges in optimizing data management and query processing, which we address in the setting of the DejaVu complex event processing system. More specifically, we propose three complementary optimizations including recent input buffering, query result caching, and join source ordering. Furthermore, we capture the relevant query processing tradeoffs in a cost model. An extensive performance study on synthetic and real-life data sets not only validates this cost model, but also shows that our optimizations are very effective, achieving more than two orders magnitude throughput improvement and much better scalability compared to a conventional approach.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {243–254},
numpages = {12},
keywords = {stream correlation, data streams, stream archiving, complex event processing, pattern matching},
location = {New York, New York, USA},
series = {DEBS '11}
}

@inproceedings{10.1145/2001576.2001609,
author = {Jones, Ben and Soltoggio, Andrea and Sendhoff, Bernhard and Yao, Xin},
title = {Evolution of Neural Symmetry and Its Coupled Alignment to Body Plan Morphology},
year = {2011},
isbn = {9781450305570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001576.2001609},
doi = {10.1145/2001576.2001609},
abstract = {Body morphology is thought to have heavily influenced the evolution of neural architecture. However, the extent of this interaction and its underlying principles are largely unclear. To help us elucidate these principles, we examine the artificial evolution of a hypothetical nervous system embedded in a fish-inspired animat. The aim is to observe the evolution of neural structures in relation to both body morphology and required motor primitives. Our investigations reveal that increasing the pressure to evolve a wider range of movements also results in higher levels of neural symmetry. We further examine how different body shapes affect the evolution of neural structure; we find that, in order to achieve optimal movements, the neural structure integrates and compensates for asymmetrical body morphology. Our study clearly indicates that different parts of the animat - specifically, nervous system and body plan - evolve in concert with and become highly functional with respect to the other parts. The autonomous emergence of morphological and neural computation in this model contributes to unveiling the surprisingly strong coupling of such systems in nature.},
booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
pages = {235–242},
numpages = {8},
keywords = {neuroevolution, motor primitives, artificial life, soft robotics, morphology, body symmetry},
location = {Dublin, Ireland},
series = {GECCO '11}
}

@inproceedings{10.1145/2396716.2396731,
author = {Pechau, J\"{o}rg},
title = {Rafting the Agile Waterfall: Value Based Conflicts of Agile Software Development},
year = {2011},
isbn = {9781450313025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396716.2396731},
doi = {10.1145/2396716.2396731},
abstract = {Agile software development projects executed in larger project environments often struggle to succeed, despite overall framework conditions being considered good. This often can be related to agile cultures clashing with nonagile cultures, thus leading agile and non-agile value systems into conflict. This paper is a result of my research regarding patterns of value based conflicts of agile software development projects. The patterns shall help to identify aforementioned conflicts and provide a short-term approach to aim for short-term improvements and a long-term approach, to address the value conflicts themselfes when using Agile approaches.},
booktitle = {Proceedings of the 16th European Conference on Pattern Languages of Programs},
articleno = {15},
numpages = {15},
keywords = {software development, agile, patterns, software project management, value systems},
location = {Irsee, Germany},
series = {EuroPLoP '11}
}

@article{10.1145/1978782.1978788,
author = {Bonifaci, Vincenzo and Korteweg, Peter and Marchetti-Spaccamela, Alberto and Stougie, Leen},
title = {Minimizing Flow Time in the Wireless Gathering Problem},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1549-6325},
url = {https://doi.org/10.1145/1978782.1978788},
doi = {10.1145/1978782.1978788},
abstract = {We address the problem of efficient data gathering in a wireless network through multihop communication. We focus on two objectives related to flow times, that is, the times spent by data packets in the system: minimization of the maximum flow time and minimization of the average flow time of the packets. For both problems we prove that, unless P=NP, no polynomial-time algorithm can approximate the optimal solution within a factor less than Ω(m1−ε) for any 0&lt;ε&lt;1, where m is the number of packets. We then assess the performance of two natural algorithms by proving that their cost remains within the optimal cost of the respective problem if we allow the algorithms to transmit data at a speed 5 times higher than that of the optimal solutions to which we compare them.},
journal = {ACM Trans. Algorithms},
month = jul,
articleno = {33},
numpages = {20},
keywords = {data gathering, approximation algorithms, Wireless networks, local algorithms}
}

@article{10.1145/2007183.2007189,
author = {Azmandian, Fatemeh and Moffie, Micha and Alshawabkeh, Malak and Dy, Jennifer and Aslam, Javed and Kaeli, David},
title = {Virtual Machine Monitor-Based Lightweight Intrusion Detection},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5980},
url = {https://doi.org/10.1145/2007183.2007189},
doi = {10.1145/2007183.2007189},
abstract = {As virtualization technology gains in popularity, so do attempts to compromise the security and integrity of virtualized computing resources. Anti-virus software and firewall programs are typically deployed in the guest virtual machine to detect malicious software. These security measures are effective in detecting known malware, but do little to protect against new variants of intrusions. Intrusion detection systems (IDSs) can be used to detect malicious behavior. Most intrusion detection systems for virtual execution environments track behavior at the application or operating system level, using virtualization as a means to isolate themselves from a compromised virtual machine.In this paper, we present a novel approach to intrusion detection of virtual server environments which utilizes only information available from the perspective of the virtual machine monitor (VMM). Such an IDS can harness the ability of the VMM to isolate and manage several virtual machines (VMs), making it possible to provide monitoring of intrusions at a common level across VMs. It also offers unique advantages over recent advances in intrusion detection for virtual machine environments. By working purely at the VMM-level, the IDS does not depend on structures or abstractions visible to the OS (e.g., file systems), which are susceptible to attacks and can be modified by malware to contain corrupted information (e.g., the Windows registry). In addition, being situated within the VMM provides ease of deployment as the IDS is not tied to a specific OS and can be deployed transparently below different operating systems.Due to the semantic gap between the information available to the VMM and the actual application behavior, we employ the power of data mining techniques to extract useful nuggets of knowledge from the raw, low-level architectural data. We show in this paper that by working entirely at the VMM-level, we are able to capture enough information to characterize normal executions and identify the presence of abnormal malicious behavior. Our experiments on over 300 real-world malware and exploits illustrate that there is sufficient information embedded within the VMM-level data to allow accurate detection of malicious attacks, with an acceptable false alarm rate.},
journal = {SIGOPS Oper. Syst. Rev.},
month = jul,
pages = {38–53},
numpages = {16},
keywords = {system, virtualization, virtual machine, virtual machine monitor, data mining, intrusion detection}
}

@inproceedings{10.1145/2016741.2016778,
author = {Demeler, Borries and Singh, Raminderjeet and Pierce, Marlon and Brookes, Emre H. and Marru, Suresh and Dubbs, Bruce},
title = {UltraScan Gateway Enhancements: In Collaboration with TeraGrid Advanced User Support},
year = {2011},
isbn = {9781450308885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016741.2016778},
doi = {10.1145/2016741.2016778},
abstract = {The Ultrascan gateway provides a user friendly web interface for evaluation of experimental analytical ultracentrifuge data using the UltraScan modeling software. The analysis tasks are executed on the TeraGrid and campus computational resources. The gateway is highly successful in providing the service to end users and consistently listed among the top five gateway community account usage. This continued growth and challenges of sustainability needed additional support to revisit the job management architecture.In this paper we describe the enhancements to the Ultrascan gateway middleware infrastructure provided through the TeraGrid Advanced User Support program. The advanced support efforts primarily focused on a) expanding the TeraGrid resources incorporate new machines; b) upgrading UltraScan's job management interfaces to use GRAM5 in place of the deprecated WS-GRAM; c) providing realistic usage scenarios to the GRAM5 and INCA resource testing and monitoring teams; d) creating general-purpose, resource-specific, and UltraScan-specific error handling and fault tolerance strategies; and e) providing forward and backward compatibility for the job management system between UltraScan's version 2 (currently in production) and version 3 (expected to be released mid-2011).},
booktitle = {Proceedings of the 2011 TeraGrid Conference: Extreme Digital Discovery},
articleno = {34},
numpages = {8},
keywords = {distributed application abstractions, application web services, scientific application management, service oriented architecture, scientific workflows, science gateways, grid computing, fault tolerance},
location = {Salt Lake City, Utah},
series = {TG '11}
}

@inproceedings{10.1145/2002951.2002961,
author = {Guo, Philip J.},
title = {Sloppy Python: Using Dynamic Analysis to Automatically Add Error Tolerance to Ad-Hoc Data Processing Scripts},
year = {2011},
isbn = {9781450308113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002951.2002961},
doi = {10.1145/2002951.2002961},
abstract = {Programmers and data analysts get frustrated when their long-running data processing scripts crash without producing results, due to either bugs in their code or inconsistencies in data sources. To alleviate this frustration, we developed a dynamic analysis technique that guarantees scripts will never crash: It converts all uncaught exceptions into special NA (Not Available) objects and continues executing rather than crashing. Thus, imperfect scripts will run to completion and produce partial results and an error log, which is more informative than simply crashing with no results. We implemented our technique as a "Sloppy" Python interpreter that automatically adds error tolerance to existing scripts without any programmer effort or run-time slowdown.},
booktitle = {Proceedings of the Ninth International Workshop on Dynamic Analysis},
pages = {35–40},
numpages = {6},
keywords = {scripting, fault tolerance, data processing},
location = {Toronto, Ontario, Canada},
series = {WODA '11}
}

@inproceedings{10.1145/2016904.2016910,
author = {Roveta, Francesco and Caviglia, Giorgio and Di Mario, Luca and Zanero, Stefano and Maggi, Federico and Ciuccarelli, Paolo},
title = {BURN: Baring Unknown Rogue Networks},
year = {2011},
isbn = {9781450306799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016904.2016910},
doi = {10.1145/2016904.2016910},
abstract = {Manual analysis of security-related events is still a necessity to investigate non-trivial cyber attacks. This task is particularly hard when the events involve slow, stealthy and large-scale activities typical of the modern cybercriminals' strategy. In this regard, visualization tools can effectively help analysts in their investigations. In this paper, we present BURN, an interactive visualization tool for displaying autonomous systems exhibiting rogue activity that helps at finding misbehaving networks through visual and interactive exploration. Up to seven values are displayed in a single visual element, while avoiding cumbersome and confusing maps. To this end, animations and alpha channels are leveraged to create simple views that highlight relevant activity patterns. In addition, BURN incorporates a simple algorithm to identify migrations of nefarious services across autonomous systems, which can support, for instance, root-cause analysis and law enforcement investigations.},
booktitle = {Proceedings of the 8th International Symposium on Visualization for Cyber Security},
articleno = {6},
numpages = {10},
location = {Pittsburgh, Pennsylvania, USA},
series = {VizSec '11}
}

@inproceedings{10.1145/2078827.2078830,
author = {Maurer, Max-Emanuel and De Luca, Alexander and Kempe, Sylvia},
title = {Using Data Type Based Security Alert Dialogs to Raise Online Security Awareness},
year = {2011},
isbn = {9781450309110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2078827.2078830},
doi = {10.1145/2078827.2078830},
abstract = {When browsing the Internet, users are likely to be exposed to security and privacy threats -- like fraudulent websites. Automatic browser mechanisms can protect them only to some extent. In other situations it is still important to raise the users' security awareness at the right moment. Passive indicators are mostly overlooked and blocking warnings are quickly dismissed by habituated users. In this work, we present a new concept of warnings that appear in-context, right next to data the user has just entered. Those dialogs are displayed whenever critical data types -- e.g. credit card data -- are entered by the users into online forms. Since they do not immediately interrupt the users' interaction but appear right in the users' focus, it is possible to place important security information in a way that it can be easily seen.We implemented the concept as a Firefox plugin and evaluated it in a row of studies including two lab studies, one focus group and one real world study. Results show that the concept is very well accepted by the users and that with the plugin, especially non-expert participants were more likely to identify fraudulent (or phishing) websites than using the standard browser warnings. Besides this, we were able to gather interesting findings on warning usage.},
booktitle = {Proceedings of the Seventh Symposium on Usable Privacy and Security},
articleno = {2},
numpages = {13},
keywords = {data type based, web browsing, security awareness, in-context},
location = {Pittsburgh, Pennsylvania},
series = {SOUPS '11}
}

@inproceedings{10.1145/2078827.2078831,
author = {Sotirakopoulos, Andreas and Hawkey, Kirstie and Beznosov, Konstantin},
title = {On the Challenges in Usable Security Lab Studies: Lessons Learned from Replicating a Study on SSL Warnings},
year = {2011},
isbn = {9781450309110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2078827.2078831},
doi = {10.1145/2078827.2078831},
abstract = {We replicated and extended a 2008 study conducted at CMU that investigated the effectiveness of SSL warnings. We adjusted the experimental design to mitigate some of the limitations of that prior study; adjustments include allowing participants to use their web browser of choice and recruiting a more representative user sample. However, during our study we observed a strong disparity between our participants actions during the laboratory tasks and their self-reported "would be" actions during similar tasks in everyday computer practices. Our participants attributed this disparity to the laboratory environment and the security it offered. In this paper we discuss our results and how the introduced changes to the initial study design may have affected them. Also, we discuss the challenges of observing natural behavior in a study environment, as well as the challenges of replicating previous studies given the rapid changes in web technology. We also propose alternatives to traditional laboratory study methodologies that can be considered by the usable security research community when investigating research questions involving sensitive data where trust may influence behavior.},
booktitle = {Proceedings of the Seventh Symposium on Usable Privacy and Security},
articleno = {3},
numpages = {18},
keywords = {study replication, study environment bias, SSL warnings, experimental design, usable security},
location = {Pittsburgh, Pennsylvania},
series = {SOUPS '11}
}

@inproceedings{10.1145/2078827.2078837,
author = {Jaferian, Pooya and Hawkey, Kirstie and Sotirakopoulos, Andreas and Velez-Rojas, Maria and Beznosov, Konstantin},
title = {Heuristics for Evaluating IT Security Management Tools},
year = {2011},
isbn = {9781450309110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2078827.2078837},
doi = {10.1145/2078827.2078837},
abstract = {The usability of IT security management (ITSM) tools is hard to evaluate by regular methods, making heuristic evaluation attractive. However, standard usability heuristics are hard to apply as IT security management occurs within a complex and collaborative context that involves diverse stakeholders. We propose a set of ITSM usability heuristics that are based on activity theory, are supported by prior research, and consider the complex and cooperative nature of security management. In a between-subjects study, we compared the employment of the ITSM and Nielsen's heuristics for evaluation of a commercial identity management system. Participants who used the ITSM set found more problems categorized as severe than those who used Nielsen's. As evaluators identified different types of problems with the two sets of heuristics, we recommend employing both the ITSM and Nielsen's heuristics during evaluation of ITSM tools.},
booktitle = {Proceedings of the Seventh Symposium on Usable Privacy and Security},
articleno = {7},
numpages = {20},
keywords = {IT security management, complex systems, heuristic evaluation, computer supported cooperative work},
location = {Pittsburgh, Pennsylvania},
series = {SOUPS '11}
}

@inproceedings{10.5555/2500753.2500778,
author = {Singh, Devinder Kaur Ajit and Palaniswamy, Vijayakumar and Raman, Vimal A./L. P. and Pearson, Hannah and Sien, Bong Pei and Rajaratnam, Bala S.},
title = {Can Exercises Using Virtual Reality Games Reduce Risk and Fear of Falls among Older Women?},
year = {2011},
publisher = {Singapore Therapeutic, Assistive &amp; Rehabilitative Technologies (START) Centre},
address = {Midview City, SGP},
abstract = {Background &amp; Aim: The objective of this study was to compare the risk and fear of falls among older women pre and post intervention using virtual reality games. Method: Thirty six community dwelling older women aged 56 and above were randomly divided into experimental (exercises using virtual reality games focus on improving balance) and control (conventional balance exercises) groups. Both groups attended an hour exercise session, twice a week for 6 weeks. Risk and fear of falls were measured using Physiological Profile Assessment (PPA) and Activity Specific Balance Scale (ABC-6). Pre and post intervention differences between the groups were examined. Results: Both the groups, virtual experiment and control group, had significant decrease in fall risk score after the interventions of exercises using virtual reality games and conventional balance exercise respectively (Z = - 3.38, p = 0.001 for experimental group; Z = - 2.50, p = 0.01 for control group). Significant difference was only demonstrated in the experiment group for ABC, t(17) = -2.78, p = 0.013. No significant differences were shown between the groups in risk and fear of falls post intervention. Conclusion: Practicing fun and interactive exercises using virtual reality games can focus on improving balance at home and reducing risk of falls in older adults.},
booktitle = {Proceedings of the 5th International Conference on Rehabilitation Engineering &amp; Assistive Technology},
articleno = {21},
numpages = {4},
keywords = {risk of falls, virtual reality, fear of falls, older women},
location = {Bangkok, Thailand},
series = {i-CREATe '11}
}

@inproceedings{10.1145/2009916.2009969,
author = {Feild, Henry Allen and Allan, James and Glatt, Joshua},
title = {CrowdLogging: Distributed, Private, and Anonymous Search Logging},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2009969},
doi = {10.1145/2009916.2009969},
abstract = {We describe CrowdLogging, an approach for distributed search log collection, storage, and mining, with the dual goals of preserving privacy and making the mined information broadly available. Most search log mining approaches and most privacy enhancing schemes have focused on centralized search logs and methods for disseminating them to third parties. In our approach, a user's search log is encrypted and shared in such a way that (a) the source of a search behavior artifact, such as a query, is unknown and (b) extremely rare artifacts---that is, artifacts more likely to contain private information---are not revealed. The approach works with any search behavior artifact that can be extracted from a search log, including queries, query reformulations, and query-click pairs. In this work, we: (1) present a distributed search log collection, storage, and mining framework; (2) compare several privacy policies, including differential privacy, showing the trade-offs between strong guarantees and the utility of the released data; (3) demonstrate the impact of our approach using two existing research query logs; and (4) describe a pilot study for which we implemented a version of the framework.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {375–384},
numpages = {10},
keywords = {distributed query logs, private query log analysis},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.5555/2145432.2145575,
author = {Hopkins, Mark and May, Jonathan},
title = {Tuning as Ranking},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO's scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {1352–1362},
numpages = {11},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/2145432.2145592,
author = {Martins, Andr\'{e} F. T. and Smith, Noah A. and Aguiar, Pedro M. Q. and Figueiredo, M\'{a}rio A. T.},
title = {Structured Sparsity in Structured Prediction},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Linear models have enjoyed great success in structured prediction in NLP. While a lot of progress has been made on efficient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved. Common approaches employ ad hoc filtering or L1-regularization; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge. We fill this gap by adopting regularizers that promote structured sparsity, along with efficient algorithms to handle them. Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {1500–1511},
numpages = {12},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/2145432.2145569,
author = {Bar-Haim, Roy and Dinur, Elad and Feldman, Ronen and Fresko, Moshe and Goldstein, Guy},
title = {Identifying and Following Expert Investors in Stock Microblogs},
year = {2011},
isbn = {9781937284114},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Information published in online stock investment message boards, and more recently in stock microblogs, is considered highly valuable by many investors. Previous work focused on aggregation of sentiment from all users. However, in this work we show that it is beneficial to distinguish expert users from non-experts. We propose a general framework for identifying expert investors, and use it as a basis for several models that predict stock rise from stock microblogging messages (stock tweets). In particular, we present two methods that combine expert identification and per-user unsupervised learning. These methods were shown to achieve relatively high precision in predicting stock rise, and significantly outperform our baseline. In addition, our work provides an in-depth analysis of the content and potential usefulness of stock tweets.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {1310–1319},
numpages = {10},
location = {Edinburgh, United Kingdom},
series = {EMNLP '11}
}

@inproceedings{10.5555/2132960.2133022,
author = {Costa-juss\`{a}, Marta R. and Banchs, Rafael E.},
title = {The BM-I2R Haitian-Cr\'{e}Ole-to-English Translation System Description for the WMT 2011 Evaluation Campaign},
year = {2011},
isbn = {9781937284121},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This work describes the Haitian-Cr\'{e}ole to English statistical machine translation system built by Barcelona Media Innovation Center (BM) and Institute for Infocomm Research (I2R) for the 6th Workshop on Statistical Machine Translation (WMT 2011). Our system carefully processes the available data and uses it in a standard phrase-based system enhanced with a source context semantic feature that helps conducting a better lexical selection and a feature orthogonalization procedure that helps making MERT optimization more reliable and stable. Our system was ranked first (among a total of 9 participant systems) by the conducted human evaluation.},
booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
pages = {452–456},
numpages = {5},
location = {Edinburgh, Scotland},
series = {WMT '11}
}

@inproceedings{10.5555/2132960.2132964,
author = {Callison-Burch, Chris and Koehn, Philipp and Monz, Christof and Zaidan, Omar F.},
title = {Findings of the 2011 Workshop on Statistical Machine Translation},
year = {2011},
isbn = {9781937284121},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.},
booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
pages = {22–64},
numpages = {43},
location = {Edinburgh, Scotland},
series = {WMT '11}
}

@article{10.14778/3402707.3402744,
author = {Chen, Rui and Mohammed, Noman and Fung, Benjamin C. M. and Desai, Bipin C. and Xiong, Li},
title = {Publishing Set-Valued Data via Differential Privacy},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402707.3402744},
doi = {10.14778/3402707.3402744},
abstract = {Set-valued data provides enormous opportunities for various data mining tasks. In this paper, we study the problem of publishing set-valued data for data mining tasks under the rigorous differential privacy model. All existing data publishing methods for set-valued data are based on partition-based privacy models, for example k-anonymity, which are vulnerable to privacy attacks based on background knowledge. In contrast, differential privacy provides strong privacy guarantees independent of an adversary's background knowledge and computational power. Existing data publishing approaches for differential privacy, however, are not adequate in terms of both utility and scalability in the context of set-valued data due to its high dimensionality.We demonstrate that set-valued data could be efficiently released under differential privacy with guaranteed utility with the help of context-free taxonomy trees. We propose a probabilistic top-down partitioning algorithm to generate a differentially private release, which scales linearly with the input data size. We also discuss the applicability of our idea to the context of relational data. We prove that our result is (∈, δ)-useful for the class of counting queries, the foundation of many data mining tasks. We show that our approach maintains high utility for counting queries and frequent itemset mining and scales to large datasets through extensive experiments on real-life set-valued datasets.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1087–1098},
numpages = {12}
}

@article{10.1145/1978542.1978562,
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek},
title = {An Overview of Business Intelligence Technology},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1978542.1978562},
doi = {10.1145/1978542.1978562},
abstract = {BI technologies are essential to running today's businesses and this technology is going through sea changes.},
journal = {Commun. ACM},
month = aug,
pages = {88–98},
numpages = {11}
}

@article{10.1145/1978542.1978568,
author = {Wright, David},
title = {Should Privacy Impact Assessments Be Mandatory?},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1978542.1978568},
doi = {10.1145/1978542.1978568},
abstract = {Privacy impact assessments should be integrated into the overall approach to risk management with other strategic planning instruments.This article considers the issue of whether privacy impact assessments (PIAs) should be mandatory. I will examine the benefits and disadvantages of PIAs, the case for and against mandatory PIAs, and ultimately conclude they should be mandatory. Even if they are made mandatory, however, other factors, such as independent audits, must be taken into account to make them truly effective.},
journal = {Commun. ACM},
month = aug,
pages = {121–131},
numpages = {11}
}

@inproceedings{10.1145/2147805.2147834,
author = {Soni, Ameet and Shavlik, Jude},
title = {Probabilistic Ensembles for Improved Inference in Protein-Structure Determination},
year = {2011},
isbn = {9781450307963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2147805.2147834},
doi = {10.1145/2147805.2147834},
abstract = {Protein X-ray crystallography -- the most popular method for determining protein structures -- remains a laborious process requiring a great deal of manual crystallographer effort to interpret low-quality protein images. Automating this process is critical in creating a high-throughput protein-structure determination pipeline. Previously, our group developed ACMI, a probabilistic framework for producing protein-structure models from electron-density maps produced via X-ray crystallography. ACMI uses a Markov Random Field to model the 3D location of each non-hydrogen atom in a protein. Calculating the best structure in this model is intractable, so ACMI uses approximate inference methods to estimate the optimal structure. While previous results have shown ACMI to be the state-of-the-art method on this task, its approximate inference algorithm remains computationally expensive and susceptible to errors. In this work, we develop Probabilistic Ensembles in ACMI (PEA), a framework for leveraging multiple, independent runs of approximate inference to produce estimates of protein structures. Our results show statistically significant improvements in the accuracy of inference resulting in more complete and accurate protein structures. In addition, PEA provides a general framework for advanced approximate inference methods in complex problem domains.},
booktitle = {Proceedings of the 2nd ACM Conference on Bioinformatics, Computational Biology and Biomedicine},
pages = {264–273},
numpages = {10},
keywords = {ensembles, statistical inference, computational biology, protein-structure determination},
location = {Chicago, Illinois},
series = {BCB '11}
}

@article{10.14778/3402755.3402759,
author = {Cohen, Jeffrey and Eshleman, John and Hagenbuch, Brian and Kent, Joy and Pedrotti, Christopher and Sherry, Gavin and Waas, Florian},
title = {Online Expansion of Large-Scale Data Warehouses},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402755.3402759},
doi = {10.14778/3402755.3402759},
abstract = {Modern data warehouses store exceedingly large amounts of data, generally considered the crown jewels of an enterprise. The amount of data maintained in such data warehouses increases significantly over time---often at a continuous pace, e.g., by gathering additional data or retaining data for longer periods to derive additional business value, but occasionally also precipitously, e.g., when consolidating disparate data warehouses and Data Marts into a single database. Having to expand a data warehouse with 100's of TB of data by a substantial portion, e.g., 100% or more is a complex and disruptive maintenance operation as it typically involves some sort of dumping and reloading of data which requires substantial downtime.In this paper we describe the methodology and mechanisms we developed in Greenplum Database to expand large-scale data warehouses in an online fashion, i.e., without noticeable downtime. At the core of our approach is a set of robust and transactionally consistent primitives that enable efficient data movement. Special emphasis was put on usability and control that lets an administrator tailor the expansion process to specific operational characteristics via priorities and schedules.We present a number of experiments to quantify the impact of an on-going expansion on query workloads.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1249–1259},
numpages = {11}
}

