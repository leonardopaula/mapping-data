@inproceedings{10.1145/1978942.1979395,
author = {Choe, Eun Kyoung and Consolvo, Sunny and Watson, Nathaniel F. and Kientz, Julie A.},
title = {Opportunities for Computing Technologies to Support Healthy Sleep Behaviors},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979395},
doi = {10.1145/1978942.1979395},
abstract = {Getting the right amount of quality sleep is a key aspect of good health, along with a healthy diet and regular exercise. Human-computer interaction (HCI) researchers have recently designed systems to support diet and exercise, but sleep has been relatively under-studied in the HCI community. We conducted a literature review and formative study aimed at uncovering opportunities for computing to support the important area of promoting healthy sleep. We present results from interviews with sleep experts, as well as a survey (N = 230) and interviews with potential users (N = 16) to indicate what people would find practical and useful for sleep. Based on these results, we identify a number of design considerations, challenges, and opportunities for using computing to support healthy sleep behaviors, as well as a design framework for mapping the design space of technologies for sleep.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3053–3062},
numpages = {10},
keywords = {wellness, design, health, health informatics, persuasive technology, qualitative study, sleep},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979070,
author = {Balaam, Madeline and Fitzpatrick, Geraldine and Good, Judith and Harris, Eric},
title = {Enhancing Interactional Synchrony with an Ambient Display},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979070},
doi = {10.1145/1978942.1979070},
abstract = {Nonverbal communication is an essential part of face-to-face social interaction, conveying information about emotion and interpersonal relationships. The rigorous sensing capabilities of pervasive technologies and the subtle nature of ambient technologies make them ideal to support the production of nonverbal communication in social interactions. In this paper we present a study using an ambient technology that supports nonverbal communication, and specifically nonverbal behaviours associated with rapport. We show that an ambient display can influence a participant's nonverbal behaviour, and that participants are not aware of this change in their behaviour. We discuss these findings in terms of the design and ethical issues that it raises, and define an agenda for future work.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {867–876},
numpages = {10},
keywords = {ambient display, interactional synchrony, face-to-face interaction, social interaction, biofeedback monitor for social interaction, rapport},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979288,
author = {Medler, Ben and John, Michael and Lane, Jeff},
title = {Data Cracker: Developing a Visual Game Analytic Tool for Analyzing Online Gameplay},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979288},
doi = {10.1145/1978942.1979288},
abstract = {Game analytics is a domain that focuses on the systems and methods used to analyze game-related data. In this paper we present how a visual game analytic tool can be developed to analyze player gameplay behavior. Our tool, Data Cracker, was built for monitoring gameplay in Dead Space 2, the latest game in the Dead Space franchise. We use Data Cracker as a case study to inform a larger discussion of designing a visual game analytic tool while working with a game team. Our design approach focuses on increasing the data literacy of a game team. This means getting an entire team interested and involved with game analytics. We found that building our tool during the early game development cycle, creating multiple early visual prototypes and branding the tool to the Dead Space team caused more team members to become interested in our tool. Increasing interest in analytics is also a means, we argue, for changing the common occurrence within the game industry to disband teams after a game is released. Instead, we promote the creation of "live" teams which stay attached to a game long after it is release in order to continue the analysis process. Additionally, we discuss the barriers one might face when developing game analytic tools, such as prejudice against analytics or the technical issues involved when collecting large data sets. All of these examples are presented as insights we gained while coupling analytic tool design to game development.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2365–2374},
numpages = {10},
keywords = {visual analytics, information visualization, game analytics, team communication, player behavior, game design},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979003,
author = {Purpura, Stephen and Schwanda, Victoria and Williams, Kaiton and Stubler, William and Sengers, Phoebe},
title = {Fit4life: The Design of a Persuasive Technology Promoting Healthy Behavior and Ideal Weight},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979003},
doi = {10.1145/1978942.1979003},
abstract = {This is a critical design paper offering a possible scenario of use intended to provoke reflection about values and politics of design in persuasive computing. We describe the design of a system - Fit4Life - that encourages individuals to address the larger goal of reducing obesity in society by promoting individual healthy behaviors. Using the Persuasive Systems Design Model [26], this paper outlines the Fit4Life persuasion context, the technology, its use of persuasive messages, and an experimental design to test the system's efficacy. We also contribute a novel discussion of the ethical and sociocultural considerations involved in our design, an issue that has remained largely unaddressed in the existing persuasive technologies literature [29].},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {423–432},
numpages = {10},
keywords = {social implications, weight loss, critical design, persuasive technology},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979446,
author = {Gjerlufsen, Tony and Klokmose, Clemens Nylandsted and Eagan, James and Pillias, Cl\'{e}ment and Beaudouin-Lafon, Michel},
title = {Shared Substance: Developing Flexible Multi-Surface Applications},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979446},
doi = {10.1145/1978942.1979446},
abstract = {This paper presents a novel middleware for developing flexible interactive multi-surface applications. Using a scenario-based approach, we identify the requirements for this type of applications. We then introduce Substance, a data-oriented framework that decouples functionality from data, and Shared Substance, a middleware implemented in Substance that provides powerful sharing abstractions. We describe our implementation of two applications with Shared Substance and discuss the insights gained from these experiments. Our finding is that the combination of a data-oriented programming model with middleware support for sharing data and functionality provides a flexible, robust solution with low viscosity at both design-time and run-time.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3383–3392},
numpages = {10},
keywords = {multi-surface interaction, data-oriented model, middleware},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979245,
author = {Mazurek, Michelle L. and Klemperer, Peter F. and Shay, Richard and Takabi, Hassan and Bauer, Lujo and Cranor, Lorrie Faith},
title = {Exploring Reactive Access Control},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979245},
doi = {10.1145/1978942.1979245},
abstract = {As users store and share more digital content at home, access control becomes increasingly important. One promising approach for helping non-expert users create accurate access policies is reactive policy creation, in which users can update their policy dynamically in response to access requests that would not otherwise succeed. An earlier study suggested reactive policy creation might be a good fit for file access control at home. To test this, we conducted an experience-sampling study in which participants used a simulated reactive access-control system for a week. Our results bolster the case for reactive policy creation as one mode by which home users specify access-control policy. We found both quantitative and qualitative evidence of dynamic, situational policies that are hard to implement using traditional models but that reactive policy creation can facilitate. While we found some clear disadvantages to the reactive model, they do not seem insurmountable.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2085–2094},
numpages = {10},
keywords = {human factors, access control, privacy, home computing},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979155,
author = {Romero, Mario and Vialard, Alice and Peponis, John and Stasko, John and Abowd, Gregory},
title = {Evaluating Video Visualizations of Human Behavior},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979155},
doi = {10.1145/1978942.1979155},
abstract = {Previously, we presented Viz-A-Vis, a VIsualiZation of Activity through computer VISion [17]. Viz-A-Vis visualizes behavior as aggregate motion over observation space. In this paper, we present two complementary user studies of Viz-A-Vis measuring its performance and discovery affordances. First, we present a controlled user study aimed at comparatively measuring behavioral analysis preference and performance for observation and search tasks. Second, we describe a study with architects measuring discovery affordances and potential impacts on their work practices. We conclude: 1) Viz-A-Vis significantly reduced search time; and 2) it increased the number and quality of insightful discoveries.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1441–1450},
numpages = {10},
keywords = {behavior, information visualization, user studies, video},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979249,
author = {Brush, A.J. Bernheim and Lee, Bongshin and Mahajan, Ratul and Agarwal, Sharad and Saroiu, Stefan and Dixon, Colin},
title = {Home Automation in the Wild: Challenges and Opportunities},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979249},
doi = {10.1145/1978942.1979249},
abstract = {Visions of smart homes have long caught the attention of researchers and considerable effort has been put toward enabling home automation. However, these technologies have not been widely adopted despite being available for over three decades. To gain insight into this state of affairs, we conducted semi-structured home visits to 14 households with home automation. The long term experience, both positive and negative, of the households we interviewed illustrates four barriers that need to be addressed before home automation becomes amenable to broader adoption. These barriers are high cost of ownership, inflexibility, poor manageability, and difficulty achieving security. Our findings also provide several directions for further research, which include eliminating the need for structural changes for installing home automation, providing users with simple security primitives that they can confidently configure, and enabling composition of home devices.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2115–2124},
numpages = {10},
keywords = {smart home, home automation, domestic technology},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978997,
author = {Solovey, Erin Treacy and Lalooses, Francine and Chauncey, Krysta and Weaver, Douglas and Parasi, Margarita and Scheutz, Matthias and Sassaroli, Angelo and Fantini, Sergio and Schermerhorn, Paul and Girouard, Audrey and Jacob, Robert J.K.},
title = {Sensing Cognitive Multitasking for a Brain-Based Adaptive User Interface},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978997},
doi = {10.1145/1978942.1978997},
abstract = {Multitasking has become an integral part of work environments, even though people are not well-equipped cognitively to handle numerous concurrent tasks effectively. Systems that support such multitasking may produce better performance and less frustration. However, without understanding the user's internal processes, it is difficult to determine optimal strategies for adapting interfaces, since all multitasking activity is not identical. We describe two experiments leading toward a system that detects cognitive multitasking processes and uses this information as input to an adaptive interface. Using functional near-infrared spectroscopy sensors, we differentiate four cognitive multitasking processes. These states cannot readily be distinguished using behavioral measures such as response time, accuracy, keystrokes or screen contents. We then present our human-robot system as a proof-of-concept that uses real-time cognitive state information as input and adapts in response. This prototype system serves as a platform to study interfaces that enable better task switching, interruption management, and multitasking.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {383–392},
numpages = {10},
keywords = {human-robot interaction, fnirs, interruption, near-infrared spectroscopy, brain computer interface, multitasking},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979046,
author = {Epp, Clayton and Lippold, Michael and Mandryk, Regan L.},
title = {Identifying Emotional States Using Keystroke Dynamics},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979046},
doi = {10.1145/1978942.1979046},
abstract = {The ability to recognize emotions is an important part of building intelligent computers. Emotionally-aware systems would have a rich context from which to make appropriate decisions about how to interact with the user or adapt their system response. There are two main problems with current system approaches for identifying emotions that limit their applicability: they can be invasive and can require costly equipment. Our solution is to determine user emotion by analyzing the rhythm of their typing patterns on a standard keyboard. We conducted a field study where we collected participants' keystrokes and their emotional states via self-reports. From this data, we extracted keystroke features, and created classifiers for 15 emotional states. Our top results include 2-level classifiers for confidence, hesitance, nervousness, relaxation, sadness, and tiredness with accuracies ranging from 77 to 88%. In addition, we show promise for anger and excitement, with accuracies of 84%.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {715–724},
numpages = {10},
keywords = {affective computing, keystroke dynamics, emotion sensing},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979108,
author = {Shklovski, Irina and Kotamraju, Nalini},
title = {Online Contribution Practices in Countries That Engage in Internet Blocking and Censorship},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979108},
doi = {10.1145/1978942.1979108},
abstract = {In this article we describe people's online contribution practices in contexts in which the government actively blocks access to or censors the Internet. We argue that people experience blocking as confusing, as a motivation for self-censorship online, as a cause of impoverishment of available content and as a real threat of personal persecution. Challenging ideas of blocking as a monolithic, abstract policy, we discuss five strategies with which Internet users navigate blocking: self-censorship, cultivating technical savvy, reliance on social ties to relay blocked content, use of already blocked sites for content production as a form of protection and practiced transparency. We also discuss strategies that forum owners and blogging platform providers employ to deal with and to avoid blocking. We conclude by advocating for more research that acknowledges the complexity of the contexts in which all Internet users contribute to the Internet and social media.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1109–1118},
numpages = {10},
keywords = {internet use, internet censorship, online communities, lurkers, blocking, social media, internet non-use, ethnography, motivation, contribution, government},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979438,
author = {Chen, Yunan and Ngo, Victor and Harrison, Sidney and Duong, Victoria},
title = {Unpacking Exam-Room Computing: Negotiating Computer-Use in Patient-Physician Interactions},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979438},
doi = {10.1145/1978942.1979438},
abstract = {The presence of computers - especially desktops - takes significant time and attention away from patients during medical visits. As a result, patients may feel disengaged and disregarded. In this study, we examined the impact of using "Computer-on-Wheels" (COWs) in exam-rooms. We found physicians constantly reorienting and resituating exam-room computers to different positions during the three stages of a medical visit: communication-intensive phase, lecturing phase and ordering phase. We refer to this behavior as micro-negotiation of computer-use. Analysis of its usage patterns, as well as physician and patient perceptions, show that micro-negotiations facilitate eye contact expression and encourage patient participation in medical visits. In addition, we identify two tensions and two unintended benefits resulting from micro-negotiations. These findings lead us to consider new modes of negotiation in the exam-room that could alleviate the tensions identified while enabling physicians to continue enjoying micro-negotiation benefits in their work practice.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3343–3352},
numpages = {10},
keywords = {exam-room computing, micro-negotiation, patient-physician interactions, electronic medical record (emr), eye contact},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979353,
author = {Sato, Daisuke and Zhu, Shaojian and Kobayashi, Masatomo and Takagi, Hironobu and Asakawa, Chieko},
title = {Sasayaki: Augmented Voice Web Browsing Experience},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979353},
doi = {10.1145/1978942.1979353},
abstract = {Auditory user interfaces have great Web-access potential for billions of people with visual impairments, with limited literacy, who are driving, or who are otherwise unable to use a visual interface. However a sequential speech-based representation can only convey a limited amount of information. In addition, typical auditory user interfaces lose the visual cues such as text styles and page structures, and lack effective feedback about the current focus. To address these limitations, we created Sasayaki (from whisper in Japanese), which augments the primary voice output with a secondary whisper of contextually relevant information, automatically or in response to user requests. It also offers new ways to jump to semantically meaningful locations. A prototype was implemented as a plug-in for an auditory Web browser. Our experimental results show that the Sasayaki can reduce the task completion times for finding elements in webpages and increase satisfaction and confidence.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2769–2778},
numpages = {10},
keywords = {web accessibility, voice augmented browsing, multiple voices, auditory interface, Sasayaki},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978944,
author = {Cramer, Meg and Hirano, Sen H. and Tentori, Monica and Yeganyan, Michael T. and Hayes, Gillian R.},
title = {Classroom-Based Assistive Technology: Collective Use of Interactive Visual Schedules by Students with Autism},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978944},
doi = {10.1145/1978942.1978944},
abstract = {vSked is an interactive and collaborative assistive technology for students with autism, combining visual schedules, choice boards, and a token-based reward system into an integrated classroom system. In this paper, we present the results of a study of three deployments of vSked over the course of a year in two autism classrooms. The results of our study demonstrate that vSked can promote student independence, reduce the quantity of educator-initiated prompts, encourage consistency and predictability, reduce the time required to transition from one activity to another. The findings from this study reveal practices surrounding the use of assistive technologies in classrooms and highlight important considerations for both the design and the evaluation of assistive technologies in the future, especially those destined for classroom use.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {visual schedules, assistive technology, autism},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979187,
author = {Zimmerman, John and Tomasic, Anthony and Garrod, Charles and Yoo, Daisy and Hiruncharoenvate, Chaya and Aziz, Rafae and Thiruvengadam, Nikhil Ravi and Huang, Yun and Steinfeld, Aaron},
title = {Field Trial of Tiramisu: Crowd-Sourcing Bus Arrival Times to Spur Co-Design},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979187},
doi = {10.1145/1978942.1979187},
abstract = {Crowd-sourcing social computing systems represent a new material for HCI designers. However, these systems are difficult to work with and to prototype, because they require a critical mass of participants to investigate social behavior. Service design is an emerging research area that focuses on how customers co-produce the services that they use, and thus it appears to be a great domain to apply this new material. To investigate this relationship, we developed Tiramisu, a transit information system where commuters share GPS traces and submit problem reports. Tiramisu processes incoming traces and generates real-time arrival time predictions for buses. We conducted a field trial with 28 participants. In this paper we report on the results and reflect on the use of field trials to evaluate crowd-sourcing prototypes and on how crowd sourcing can generate co-production between citizens and public services.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1677–1686},
numpages = {10},
keywords = {service design, transit, crowd-sourcing, field trial},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979368,
author = {Shroff, Geeta and Kam, Matthew},
title = {Towards a Design Model for Women's Empowerment in the Developing World},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979368},
doi = {10.1145/1978942.1979368},
abstract = {Pulitzer Prize-winning journalist Nicholas Kristof argues that in this century the paramount moral challenge will be the struggle for gender equality around the world. In this paper, we present a design model for empowering low-income women in the developing world, in ways that cut across individual application areas. Specifically, this model characterizes a possible trajectory for NGOs and women to engage with each other and among themselves potentially augmented by technology to help women escape from poverty. The fieldwork components in this study took place over 15 weeks in three phases, with a total of 47 NGO staff members and 35 socio-economically challenged women in rural and urban India. Interviews and co-design sessions with seven proof-of-concept prototypes showed that women appeared to belong to five distinct stages of growth in striving towards independence. We report the technology design lessons from our co-design sessions to illustrate how user readiness, relationship building at the community and family levels, and integration with state, national and international level programs, should be taken into account in the broader context of intervention design.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2867–2876},
numpages = {10},
keywords = {digital divide, gender, design for inclusion, development, ICT4D, ICTD, women empowerment, feminist interaction, developing countries, HCI4D},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978979,
author = {Bardzell, Jeffrey and Bardzell, Shaowen},
title = {Pleasure is Your Birthright: Digitally Enabled Designer Sex Toys as a Case of Third-Wave HCI},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978979},
doi = {10.1145/1978942.1978979},
abstract = {In the past decade, HCI has become increasingly preoccupied with the deeply subjective qualities of interaction: experience, embodiment, pleasure, intimacy, and so on, an agenda sometimes grouped under the heading of "third-wave HCI"."Analytically understanding and designing for such qualities has been an ongoing challenge to the field, in part because its established theories and methodologies are comparatively weak at understanding and being responsive to human subjectivity. In this paper, we present a case study of a group of designers who have, in the past few years, revolutionized their domain - sex toys - by combining embodied pleasure, intimate experience, health and wellness, emerging technologies, high-quality design processes, and social activism. We consider the implications this case could have for researchers innovating on especially third-wave HCI design theories, methodologies, and processes.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {257–266},
numpages = {10},
keywords = {criticism, HCI, human sexuality, activism},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979190,
author = {Woelfer, Jill Palzkill and Hendry, David G.},
title = {Homeless Young People and Living with Personal Digital Artifacts},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979190},
doi = {10.1145/1978942.1979190},
abstract = {This paper reports on an investigation of how homeless young people hold themselves in relation to personal digital artifacts. Twelve participants, aged 19-29, took part in semi-structured interviews. Participants answered questions about the acquisition and disposition of personal artifacts, digital and non-digital, including mobile phones, music players, and wallets. The analysis of the interview transcripts reveals that young people often part with their digital artifacts in order to meet immediate needs, including the need to create and reciprocate goodwill. This contingent holding of personal artifacts illuminates both the ordinary and extraordinary circumstances of homelessness. The paper concludes with a discussion of constraints and implications for the design of information systems for improving the welfare of homeless young people.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1697–1706},
numpages = {10},
keywords = {access, homeless young people, infrastructure, attachment, iPods, personal digital artifacts, mobile phones},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979204,
author = {Sharma, Nikhil},
title = {Role of Available and Provided Resources in Sensemaking},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979204},
doi = {10.1145/1978942.1979204},
abstract = {Making sense of a topic often involves appropriating information and organizing themes from various existing resources. We studied how sensemakers appropriated from available online resources as well as artifacts provided by another person directly. We found that both available and provided resources affect sensemaking activities. Sensemakers added more structure in their work when online resources were easily available, but added less structure and information when they were provided relevant sensemaking artifacts from another person. We also studied how early and mature artifacts provided by another person were appropriated differently and found that mature artifacts were rated better and used more but resulted in lesser structure and information being added by the recipient. These findings have implications for the support of sensemaking activities using resources available online as well as artifacts provided by others including co-workers and friends.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1807–1816},
numpages = {10},
keywords = {collaboration, handoffs, resources, sensemaking, structure, representations},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979445,
author = {Gerken, Jens and Jetter, Hans-Christian and Z\"{o}llner, Michael and Mader, Martin and Reiterer, Harald},
title = {The Concept Maps Method as a Tool to Evaluate the Usability of APIs},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979445},
doi = {10.1145/1978942.1979445},
abstract = {Application programming interfaces (APIs) are the interfaces to existing code structures, such as widgets, frameworks, or toolkits. Therefore, they very much do have an impact on the quality of the resulting system. So, ensuring that developers can make the most out of them is an important challenge. However standard usability evaluation methods as known from HCI have limitations in grasping the interaction between developer and API as most IDEs (essentially the GUI) capture only part of it. In this paper we present the Concept Map method to study the usability of an API over time. This allows us to elicit the mental model of a programmer when using an API and thereby identify usability issues and learning barriers and their development over time.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3373–3382},
numpages = {10},
keywords = {evaluation method, concept maps, API usability, longitudinal},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1978983,
author = {Vihavainen, Sami and Mate, Sujeet and Sepp\"{a}l\"{a}, Lassi and Cricri, Francesco and Curcio, Igor D.D.},
title = {We Want More: Human-Computer Collaboration in Mobile Social Video Remixing of Music Concerts},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1978983},
doi = {10.1145/1978942.1978983},
abstract = {Recording and publishing mobile video clips from music concerts is popular. There is a high potential to increase the concert's perceived value when producing video remixes from individual video clips and using them socially. A digital production of a video remix is an interactive process between human and computer. However, it is not clear what the collaboration implications between human and computer are.We present a case study where we compare the processes and products of manual and automatic mobile video remixing. We provide results from the first systematic real world study of the subject. We draw our observations from a user trial where fans recorded mobile video clips during a rock concert.The results reveal issues on heterogeneous interests of the stakeholders, unexpected uses of the raw material, the burden of editing, diverse quality requirements, motivations for remixing, the effect of understanding the logic of automation, and the collaborative use of manual and automatic remixing.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {287–296},
numpages = {10},
keywords = {automation, human factors, mobile, video, music, social},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1978942.1979104,
author = {Kwak, Haewoon and Chun, Hyunwoo and Moon, Sue},
title = {Fragile Online Relationship: A First Look at Unfollow Dynamics in Twitter},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979104},
doi = {10.1145/1978942.1979104},
abstract = {We analyze the dynamics of the behavior known as 'unfollow' in Twitter. We collected daily snapshots of the online relationships of 1.2 million Korean-speaking users for 51 days as well as all of their tweets. We found that Twitter users frequently unfollow. We then discover the major factors, including the reciprocity of the relationships, the duration of a relationship, the followees' informativeness, and the overlap of the relationships, which affect the decision to unfollow. We conduct interview with 22 Korean respondents to supplement the quantitative results.They unfollowed those who left many tweets within a short time, created tweets about uninteresting topics, or tweeted about the mundane details of their lives. To the best of our knowledge, this work is the first systematic study of the unfollow behavior in Twitter.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1091–1100},
numpages = {10},
keywords = {unfollow, online relationship, twitter, computer-mediated communication},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/1982143.1982149,
author = {Serva, Mark A. and Benamati, John and Blue, Jon and Baroudi, Jack},
title = {"In Times of Stress, Be Bold and Valiant": A Preliminary Exploration of the Psychosocial and Physiological Measures of Stress and Suggestions for Future MIS Research},
year = {2011},
isbn = {9781450306669},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982143.1982149},
doi = {10.1145/1982143.1982149},
abstract = {Stress is a fact of life. Previous MIS research has examined stress, but has focused on the psychosocial aspects of stress, which are usually measures using self-reported metrics. This preliminary study examines the possible contrast between psychosocial and physiological stress that is, the body's autonomic reaction to a potential threat. The study also examines the effectiveness of the Trier Social Stress Test (TSST), a protocol designed to induce stress in social situations. Using a sample of thirty-two students, the results indicate that the TSST protocol is an effectiveness mechanism for inducing both psychosocial and physiological stress. The results also indicate that psychosocial and physiological stress are indeed different metrics. The study concludes with recommendations for future MIS researchers to build on these findings.},
booktitle = {Proceedings of the 49th SIGMIS Annual Conference on Computer Personnel Research},
pages = {14–19},
numpages = {6},
keywords = {psychosocial stress, biometrics, stress, physiological stress},
location = {San Antonio, Texas, USA},
series = {SIGMIS-CPR '11}
}

@inproceedings{10.1145/1987875.1987911,
author = {Tsay, Jason and Wright, Hyrum K. and Perry, Dewayne E.},
title = {Experiences Mining Open Source Release Histories},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987911},
doi = {10.1145/1987875.1987911},
abstract = {Software releases form a critical part of the life cycle of a software project. Typically, each project produces releases in its own way, using various methods of versioning, archiving, announcing and publishing the release. Understanding the release history of a software project can shed light on the project history, as well as the release process used by that project, and how those processes change. However, many factors make automating the retrieval of release history information difficult, such as the many sources of data, a lack of relevant standards and a disparity of tools used to create releases.In spite of the large amount of raw data available, no attempt has been made to create a release history database of a large number of projects in the open source ecosystem. This paper presents our experiences, including the tools, techniques and pitfalls, in our early work to create a software release history database which will be of use to future researchers who want to study and model the release engineering process in greater depth.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {208–212},
numpages = {5},
keywords = {release engineering, data mining},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@inproceedings{10.1145/1985441.1985461,
author = {Pagano, Dennis and Maalej, Walid},
title = {How Do Developers Blog? An Exploratory Study},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985461},
doi = {10.1145/1985441.1985461},
abstract = {We report on an exploratory study, which aims at understanding how software developers use social media compared to conventional development infrastructures. We analyzed the blogging and the committing behavior of 1,100 developers in four large open source communities. We observed that these communities intensively use blogs with one new entry about every 8 hours. A blog entry includes 14 times more words than a commit message. When analyzing the content of the blogs, we found that most popular topics represent high-level concepts such as functional requirements and domain concepts. Source code related topics are covered in less than 15% of the posts. Our results also show that developers are more likely to blog after corrective engineering and management activities than after forward engineering and re-engineering activities. Our findings call for a hypothesis-driven research to further understand the role of social media in software engineering and integrate it into development processes and tools.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {123–132},
numpages = {10},
keywords = {social software, open source, data mining, blogs},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1145/1985793.1985883,
author = {Schmerl, Bradley and Garlan, David and Dwivedi, Vishal and Bigrigg, Michael W. and Carley, Kathleen M.},
title = {SORASCS: A Case Study in Soa-Based Platform Design for Socio-Cultural Analysis},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985883},
doi = {10.1145/1985793.1985883},
abstract = {An increasingly important class of software-based systems is platforms that permit integration of third-party components, services, and tools. Service-Oriented Architecture (SOA) is one such platform that has been successful in providing integration and distribution in the business domain, and could be effective in other domains (e.g., scientific computing, healthcare, and complex decision making). In this paper, we discuss our application of SOA to provide an integration platform for socio-cultural analysis, a domain that, through models, tries to understand, analyze and predict relationships in large complex social systems. In developing this platform, called SORASCS, we had to overcome issues we believe are generally applicable to any application of SOA within a domain that involves technically na\"{\i}ve users and seeks to establish a sustainable software ecosystem based on a common integration platform. We discuss these issues, the lessons learned about the kinds of problems that occur, and pathways toward a solution.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {643–652},
numpages = {10},
keywords = {platform design, socio-cultural analysis, service oriented architectures},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/1987875.1987893,
author = {Paasivaara, Maria and Lassenius, Casper},
title = {How Does an Agile Coaching Team Work? A Case Study},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987893},
doi = {10.1145/1987875.1987893},
abstract = {This paper presents a case study on building a successful agile coaching team focusing on distributed software development projects in a global software company. We describe how the team of eight coaches was built, how the coaches work as a team, how the coaches work with their customer projects, what the main benefits of coaching have been for the customer projects, and the main challenges on building the coaching activities.The data was gathered by 13 semi-structured interviews of the coaching team members, as well as the interviews with personnel from four coached customer projects.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {101–109},
numpages = {9},
keywords = {agile coaching, agile software development, global software development},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@inproceedings{10.1145/1987993.1987995,
author = {Winbladh, Kristina and Ziv, Hadar and Richardson, Debra J.},
title = {Evolving Requirements in Patient-Centered Software},
year = {2011},
isbn = {9781450305853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987993.1987995},
doi = {10.1145/1987993.1987995},
abstract = {The implications of an aging U.S. population indicate that a large portion of the population will receive limited access to the healthcare they need, unless clinical preventive services are provided. Patient-centered healthcare, in which patients gain more access to and control over their own health, is becoming an important part of clinical preventive services and so is software. Healthcare entails highly complex processes that require substantial communication between different healthcare professionals. A major concern for patient-centered software is that it must adapt to changing needs to support long-term wellbeing, i.e., new knowledge must be considered continuously as part of the software lifecycle. This position paper contends that research efforts should be directed toward software engineering solutions that consider evolution as a part of the software lifecycle and use a variety of feedback channels to direct evolution, and presents a research agenda integrated with an approach that addresses evolving needs through a continuous data-driven requirements engineering (RE) technique.},
booktitle = {Proceedings of the 3rd Workshop on Software Engineering in Health Care},
pages = {1–4},
numpages = {4},
keywords = {requirements evolution, patient-centered software, data-driven requirements engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {SEHC '11}
}

@inproceedings{10.1145/1984701.1984705,
author = {King, Abayomi and Lyons, Kelly},
title = {Automatic Status Updates in Distributed Software Development},
year = {2011},
isbn = {9781450305952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1984701.1984705},
doi = {10.1145/1984701.1984705},
abstract = {This study investigates how automatic, real-time, user-centered awareness information can help distributed software development teams. We created an Eclipse plugin that automatically determines a user's activity in their Eclipse IDE and publishes the activity information as the status of their instant messenger client. The status is updated in real-time every time the user changes his or her activities in their IDE. We evaluated this tool by demonstrating it to eighty-one academics and industry workers in the field of computer science and interviewing them about the perceived benefits and usefulness of the tool. The results reveal various factors that can impact a participant's desire for increased awareness information. Despite these factors there was a general desire to improve awareness of users' activities via the tool. There was also some indication that the tool might help with interruption management.},
booktitle = {Proceedings of the 2nd International Workshop on Web 2.0 for Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {status update, interruption management, distributed software development, awareness},
location = {Waikiki, Honolulu, HI, USA},
series = {Web2SE '11}
}

@inproceedings{10.1145/1988688.1988725,
author = {Elia, Annibale and Vellutino, Daniela and Marano, Federica and Langella, Alberto Maria and Napoli, Antonella},
title = {Semantic Web and Language Resources for E-Government: Linguistically Motivated Data Mining},
year = {2011},
isbn = {9781450301480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988688.1988725},
doi = {10.1145/1988688.1988725},
abstract = {Language Technologies (LT) perform well when they rely on a previous Language Resources (LR) development. Hence, in this paper we illustrate how to build an efficient data mining system based on a coherent formalization of natural language and on a lingware (in Machine-Readable Form) built on the universal concepts of "lexical unit", "meaning unit" and "morphosyntactic context". From a linguistic and semantic point of view we get more coherent results through a linguistically motivated data mining than a statistical approach. We developed LR for Natural Language Processing (NLP) applications, composed by electronic dictionaries made of terminological multiword-expressions (Machine-Readable Form) and by local grammars (in the form of finite-state automata and transducers -- FSA/FST). Both parts of this lingware were built and applied according to Lexicon-Grammar (LG) formalization principles and methods. The mentioned language resources form the basis to develop an Information Retrieval System for e-Government. This device is a Semantic Web (SW) application, which will automatically recognize a given set of frequently-asked questions (from here on, FAQs) on European Community Information, previously formalized as syntactic patterns inside local grammars.},
booktitle = {Proceedings of the International Conference on Web Intelligence, Mining and Semantics},
articleno = {31},
numpages = {9},
keywords = {local grammars, finite state automata, semantic web, NLP, IR, electronic dictionaries, multiword-expressions, linguistic resources, data mining},
location = {Sogndal, Norway},
series = {WIMS '11}
}

@inproceedings{10.1145/1988688.1988718,
author = {Orgaz, Gema Bello and Cajias, Raul and Camacho, David},
title = {A Study on the Impact of Crowd-Based Voting Schemes in the 'Eurovision' European Contest},
year = {2011},
isbn = {9781450301480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988688.1988718},
doi = {10.1145/1988688.1988718},
abstract = {The Eurovision contest has been the reference on european song contests for the past 50 years. Countries in the European Union can shows the rest of the participants their current music tendencies. This phenomena has been studied in domains like physic and social sciences to find correlations between contests and current political and socio-economy trends in EU. The inclusion of web and social technologies some years ago, have caused a disruption in the traditional voting system whereby the audience is encouraged to participate by casting votes for their favorite song. As a result, this system yields new, relevant information that may be extrapolated to social and political tendencies in Europe with a higher degree accuracy than by data collected using the previous jury-based system. This paper provides an initial data analysis in crowd behavior to assess the impact of the televote system, in the Eurovision voting dynamic, by focusing on two distinct five years periods that can successfully contrast each voting scheme. Analyzing these periods separately, we can observe results from the televoting contests and then compare to the jury to see if there is a change in voting patterns. Finally, we study the underlying community structure of the voting network using the Cluster Percolation Method and Edge Betweenness to discover stable core communities spanning a number of years in the contest. The clusters obtained using these algorithms are then used to compare how these stable communities have evolving during the considered periods.},
booktitle = {Proceedings of the International Conference on Web Intelligence, Mining and Semantics},
articleno = {25},
numpages = {9},
keywords = {social mining, web mining, CPM, eurovision, voting partnership, edge betweenness, network, televoting, graph based algorithms, data mining},
location = {Sogndal, Norway},
series = {WIMS '11}
}

@inproceedings{10.1145/2141622.2141633,
author = {Metsis, Vangelis and Galatas, Georgios and Papangelis, Alexandros and Kosmopoulos, Dimitrios and Makedon, Fillia},
title = {Recognition of Sleep Patterns Using a Bed Pressure Mat},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141633},
doi = {10.1145/2141622.2141633},
abstract = {The monitoring of sleep patterns is of major importance for various reason such as, the detection and treatment of sleep disorders, the assessment of the effect of different medical conditions or medications on the sleep quality and the assessment of mortality risks associated with sleeping patterns in adults and children. Sleep monitoring by itself is a difficult problem due to both privacy and technical considerations. The proposed system uses a bed pressure mat to assess and report sleep patterns. To evaluate our system we used real data collected in Heracleia Lab's assistive living apartment. Our method is non-invasive, as it does not disrupt the user's usual sleeping behavior and it can be used both at the clinic and at home with minimal cost.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {9},
numpages = {4},
keywords = {motion recognition, sleep patterns, machine learning, sleep disorders},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@inproceedings{10.1145/2141622.2141674,
author = {Bieber, Gerald and Luthardt, Andr\'{e} and Peter, Christian and Urban, Bodo},
title = {The Hearing Trousers Pocket: Activity Recognition by Alternative Sensors},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141674},
doi = {10.1145/2141622.2141674},
abstract = {In daily life, mobile phones accompany the user permanently and are worn often in the front pocket of the trousers. The sensors included in today's mobile phones can hence be used for ubiquitous assistance. For instance, the acceleration sensor could be used for analysis of the person's bodily activity, or the microphone can be used to analyze the environmental noise levels. A possible sensor fusion provides additional and assured environmental and context information.This work presents new methods of activity recognition by acceleration and sound sensors by means of sensors included in commercially available smart phones during everyday life. We could identify that sounds provide valuable additional information on a user's situation that allow to better asses a person's current context.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {44},
numpages = {6},
keywords = {sound sensor, activity monitoring, mobile phone, physical activity, assistive technologies, context, mental state, microphone, stress detection, acceleration, user state detection, situation},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@inproceedings{10.1145/2141622.2141678,
author = {Xefteris, Stefanos and Baboshin, Andrey and Tserpes, Konstantinos and Androulidakis, Aggelos and Glickman, Yuri and Varvarigou, Theodora and Haritou, Maria and D'Andria, Francesco},
title = {Enabling Risk Assessment and Analysis by Event Detection in Dementia Patients Using a Reconfigurable Rule Set},
year = {2011},
isbn = {9781450307727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141622.2141678},
doi = {10.1145/2141622.2141678},
abstract = {Chronic mental illnesses pose a great burden on the lives of citizens worldwide. In modern health-care, decentralization and enabling the self management of patients at home are crucial factors in improving the every-day lives of patients and the people close to them. People in general tend to dislike obtrusive monitoring on their daily activities, so how can we implement a platform that can provide clinicians with adequate and concise information on their patients health status and at the same time be unobtrusive and easy to use. Moreover, how can we make such an unobtrusive system capable of providing the doctor with high-impact warnings on the patient's health status only when it is needed, thus relieving him of unnecessary workload? In this paper, the authors present a reconfigurable Event Detection mechanism used in the ALADDIN platform for Risk Assessment and Analysis.},
booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {47},
numpages = {7},
keywords = {assistive infrastructures, dementia, quality of life, cognitive states, e-health, risk analysis, assisted living, non-obtrusive monitoring},
location = {Heraklion, Crete, Greece},
series = {PETRA '11}
}

@inproceedings{10.5555/2248467.2248480,
author = {Pizziol, Sergio and Dehais, Fr\'{e}d\'{e}ric and Tessier, Catherine},
title = {Towards Human Operator "State" Assessment},
year = {2011},
isbn = {9781450315067},
publisher = {IRIT Press},
address = {Toulouse, FRA},
abstract = {This paper focuses on an approach to estimate the symbolic "state" and detect the attentional tunneling of a human operator in the frame of a human-robot mission. The symbolic "state" results from a fuzzy aggregation of the operator's gaze position and heart rate.},
booktitle = {Proceedings of the 1st International Conference on Application and Theory of Automation in Command and Control Systems},
pages = {99–106},
numpages = {8},
keywords = {eye tracker, attentional tunneling, situation awareness, heart rate, authority conflict, human operator state},
location = {Barcelona, Spain},
series = {ATACCS '11}
}

@inproceedings{10.1145/1987816.1987828,
author = {Essary, David and Amer, Ahmed},
title = {Sustainable Predictive Storage Management: On-Line Grouping for Energy and Latency Reduction},
year = {2011},
isbn = {9781450307734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987816.1987828},
doi = {10.1145/1987816.1987828},
abstract = {The divergence of processor and storage system speeds is one of the most intensely investigated problems in computing. Yet the performance disparity remains, and further, storage energy consumption is rapidly becoming a new critical problem. While smarter caching and predictive techniques do much to alleviate this disparity, the problem persists, and data storage remains a growing contributor to latency and energy consumption. We present an online, block-level, context-based predictive engine utilizing opportunistic replication. We test this predictive engine on real-world workloads, gathering all necessary predictive metadata on the fly without any warm-up period, and show reductions of total disk activity by up to 65%. This reduction in logical movement equates to a reduction in physical mechanical movement of the drive by 80%, a reduction of perceived latency by up to 63%, and a reduction in measured energy consumed by mechanical acticvity of 52--71% on live hardware.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems and Storage},
articleno = {9},
numpages = {11},
keywords = {latency, replication, predictive metadata, power, storage systems, sustainable, prediction, energy, data layout, predictive engine},
location = {Haifa, Israel},
series = {SYSTOR '11}
}

@inproceedings{10.1145/1988796.1988807,
author = {Van Hensbergen, Eric and Shinde, Pravin and Evans, Noah},
title = {Brasil: Basic Resource Aggregation System Infrastructure Layer},
year = {2011},
isbn = {9781450307611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988796.1988807},
doi = {10.1145/1988796.1988807},
abstract = {Brasil is a self-contained service which can be deployed across a cluster to provide a dataflow workload distribution and communication aggregation mechanism. Together with our dataflow shell, named PUSH, it is intended to be used for the management of non-traditional super computing applications as well as provide a mechanism to manage in-situ analysis and vizualization of more traditional high performance computing simulations. This paper describes our experiences implementing and deploying a prototype of Brasil on a BlueGene/P supercomputer.},
booktitle = {Proceedings of the 1st International Workshop on Runtime and Operating Systems for Supercomputers},
pages = {73–80},
numpages = {8},
location = {Tucson, Arizona},
series = {ROSS '11}
}

@inproceedings{10.1145/1993498.1993551,
author = {Budi, Aditya and Lo, David and Jiang, Lingxiao and Lucia},
title = {<i>Kb</i>-Anonymity: A Model for Anonymized Behaviour-Preserving Test and Debugging Data},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993551},
doi = {10.1145/1993498.1993551},
abstract = {It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.For the purpose of releasing private data for testing and debugging, this paper proposes the kb-anonymity model, which combines the k-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, kb-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {447–457},
numpages = {11},
keywords = {k-anonymity, behavior preservation, third-party testing and debugging, symbolic execution},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993551,
author = {Budi, Aditya and Lo, David and Jiang, Lingxiao and Lucia},
title = {<i>Kb</i>-Anonymity: A Model for Anonymized Behaviour-Preserving Test and Debugging Data},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993551},
doi = {10.1145/1993316.1993551},
abstract = {It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.For the purpose of releasing private data for testing and debugging, this paper proposes the kb-anonymity model, which combines the k-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, kb-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.},
journal = {SIGPLAN Not.},
month = jun,
pages = {447–457},
numpages = {11},
keywords = {third-party testing and debugging, behavior preservation, k-anonymity, symbolic execution}
}

@inproceedings{10.1145/1993498.1993509,
author = {Jung, Changhee and Rus, Silvius and Railing, Brian P. and Clark, Nathan and Pande, Santosh},
title = {Brainy: Effective Selection of Data Structures},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993509},
doi = {10.1145/1993498.1993509},
abstract = {Data structure selection is one of the most critical aspects of developing effective applications. By analyzing data structures' behavior and their interaction with the rest of the application on the underlying architecture, tools can make suggestions for alternative data structures better suited for the program input on which the application runs. Consequently, developers can optimize their data structure usage to make the application conscious of an underlying architecture and a particular program input.This paper presents the design and evaluation of Brainy, a new program analysis tool that automatically selects the best data structure for a given program and its input on a specific microarchitecture. The data structure's interface functions are instrumented to dynamically monitor how the data structure interacts with the application for a given input. The instrumentation records traces of various runtime characteristics including underlying architecture-specific events. These generated traces are analyzed and fed into an offline model, constructed using machine learning, to select the best data structure. That is, Brainy exploits runtime feedback of data structures to model the situation an application runs on, and selects the best data structure for a given application/input/architecture combination based on the constructed model. The empirical evaluation shows that this technique is highly accurate across several real-world applications with various program input sets on two different state-of-the-art microarchitectures. Consequently, Brainy achieved an average performance improvement of 27% and 33% on both microarchitectures, respectively.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {86–97},
numpages = {12},
keywords = {application generator, data structure selection, performance counters, training framework},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993509,
author = {Jung, Changhee and Rus, Silvius and Railing, Brian P. and Clark, Nathan and Pande, Santosh},
title = {Brainy: Effective Selection of Data Structures},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993509},
doi = {10.1145/1993316.1993509},
abstract = {Data structure selection is one of the most critical aspects of developing effective applications. By analyzing data structures' behavior and their interaction with the rest of the application on the underlying architecture, tools can make suggestions for alternative data structures better suited for the program input on which the application runs. Consequently, developers can optimize their data structure usage to make the application conscious of an underlying architecture and a particular program input.This paper presents the design and evaluation of Brainy, a new program analysis tool that automatically selects the best data structure for a given program and its input on a specific microarchitecture. The data structure's interface functions are instrumented to dynamically monitor how the data structure interacts with the application for a given input. The instrumentation records traces of various runtime characteristics including underlying architecture-specific events. These generated traces are analyzed and fed into an offline model, constructed using machine learning, to select the best data structure. That is, Brainy exploits runtime feedback of data structures to model the situation an application runs on, and selects the best data structure for a given application/input/architecture combination based on the constructed model. The empirical evaluation shows that this technique is highly accurate across several real-world applications with various program input sets on two different state-of-the-art microarchitectures. Consequently, Brainy achieved an average performance improvement of 27% and 33% on both microarchitectures, respectively.},
journal = {SIGPLAN Not.},
month = jun,
pages = {86–97},
numpages = {12},
keywords = {application generator, training framework, data structure selection, performance counters}
}

@inproceedings{10.1145/1993498.1993501,
author = {Pingali, Keshav and Nguyen, Donald and Kulkarni, Milind and Burtscher, Martin and Hassaan, M. Amber and Kaleem, Rashid and Lee, Tsung-Hsien and Lenharth, Andrew and Manevich, Roman and M\'{e}ndez-Lojo, Mario and Prountzos, Dimitrios and Sui, Xin},
title = {The Tao of Parallelism in Algorithms},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993501},
doi = {10.1145/1993498.1993501},
abstract = {For more than thirty years, the parallel programming community has used the dependence graph as the main abstraction for reasoning about and exploiting parallelism in "regular" algorithms that use dense arrays, such as finite-differences and FFTs. In this paper, we argue that the dependence graph is not a suitable abstraction for algorithms in new application areas like machine learning and network analysis in which the key data structures are "irregular" data structures like graphs, trees, and sets.To address the need for better abstractions, we introduce a data-centric formulation of algorithms called the operator formulation in which an algorithm is expressed in terms of its action on data structures. This formulation is the basis for a structural analysis of algorithms that we call tao-analysis. Tao-analysis can be viewed as an abstraction of algorithms that distills out algorithmic properties important for parallelization. It reveals that a generalized form of data-parallelism called amorphous data-parallelism is ubiquitous in algorithms, and that, depending on the tao-structure of the algorithm, this parallelism may be exploited by compile-time, inspector-executor or optimistic parallelization, thereby unifying these seemingly unrelated parallelization techniques. Regular algorithms emerge as a special case of irregular algorithms, and many application-specific optimization techniques can be generalized to a broader context.These results suggest that the operator formulation and tao-analysis of algorithms can be the foundation of a systematic approach to parallel programming.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {12–25},
numpages = {14},
keywords = {tao-analysis, irregular programs, operator formulation, galois system, amorphous data-parallelism},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993501,
author = {Pingali, Keshav and Nguyen, Donald and Kulkarni, Milind and Burtscher, Martin and Hassaan, M. Amber and Kaleem, Rashid and Lee, Tsung-Hsien and Lenharth, Andrew and Manevich, Roman and M\'{e}ndez-Lojo, Mario and Prountzos, Dimitrios and Sui, Xin},
title = {The Tao of Parallelism in Algorithms},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993501},
doi = {10.1145/1993316.1993501},
abstract = {For more than thirty years, the parallel programming community has used the dependence graph as the main abstraction for reasoning about and exploiting parallelism in "regular" algorithms that use dense arrays, such as finite-differences and FFTs. In this paper, we argue that the dependence graph is not a suitable abstraction for algorithms in new application areas like machine learning and network analysis in which the key data structures are "irregular" data structures like graphs, trees, and sets.To address the need for better abstractions, we introduce a data-centric formulation of algorithms called the operator formulation in which an algorithm is expressed in terms of its action on data structures. This formulation is the basis for a structural analysis of algorithms that we call tao-analysis. Tao-analysis can be viewed as an abstraction of algorithms that distills out algorithmic properties important for parallelization. It reveals that a generalized form of data-parallelism called amorphous data-parallelism is ubiquitous in algorithms, and that, depending on the tao-structure of the algorithm, this parallelism may be exploited by compile-time, inspector-executor or optimistic parallelization, thereby unifying these seemingly unrelated parallelization techniques. Regular algorithms emerge as a special case of irregular algorithms, and many application-specific optimization techniques can be generalized to a broader context.These results suggest that the operator formulation and tao-analysis of algorithms can be the foundation of a systematic approach to parallel programming.},
journal = {SIGPLAN Not.},
month = jun,
pages = {12–25},
numpages = {14},
keywords = {amorphous data-parallelism, tao-analysis, operator formulation, irregular programs, galois system}
}

@inproceedings{10.1145/2000417.2000418,
author = {Teodoro, George and Sussman, Alan},
title = {AARTS: Low Overhead Online Adaptive Auto-Tuning},
year = {2011},
isbn = {9781450307086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000417.2000418},
doi = {10.1145/2000417.2000418},
abstract = {We present an online lightweight auto-tuning system for shared-memory parallel programs. We employ an online adaptive tuning algorithm that is based on performance measurements, to adapt to performance variability that arises during program execution. We address the impact of synchronous vs. asynchronous interactions between the application and the tuning system, and describe an adaptive approach that benefits from the improvements provided by both options. We presented a performance study of the online tuning system, and compared it to synchronous tuning systems. Finally, AARTS is evaluated under different scenarios, showing the potential benefits of using online tuning and the ability of AARTS to exploit those benefits.},
booktitle = {Proceedings of the 1st International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era},
pages = {1–11},
numpages = {11},
keywords = {online tuning, auto-tuning},
location = {San Jose, California, USA},
series = {EXADAPT '11}
}

@inproceedings{10.1145/2024724.2024735,
author = {Wang, Yanzhi and Xie, Qing and Ammari, Ahmed and Pedram, Massoud},
title = {Deriving a Near-Optimal Power Management Policy Using Model-Free Reinforcement Learning and Bayesian Classification},
year = {2011},
isbn = {9781450306362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024724.2024735},
doi = {10.1145/2024724.2024735},
abstract = {To cope with the variations and uncertainties that emanate from hardware and application characteristics, dynamic power management (DPM) frameworks must be able to learn about the system inputs and environment and adjust the power management policy on the fly. In this paper we present an online adaptive DPM technique based on model-free reinforcement learning (RL), which is commonly used to control stochastic dynamical systems. In particular, we employ temporal difference learning for semi-Markov decision process (SMDP) for the model-free RL. In addition a novel workload predictor based on an online Bayes classifier is presented to provide effective estimates of the workload states for the RL algorithm. In this DPM framework, power and latency tradeoffs can be precisely controlled based on a user-defined parameter. Experiments show that amount of average power saving (without any increase in the latency) is up to 16.7% compared to a reference expert-based approach. Alternatively, the per-request latency reduction without any power consumption increase is up to 28.6% compared to the expert-based approach.},
booktitle = {Proceedings of the 48th Design Automation Conference},
pages = {41–46},
numpages = {6},
keywords = {reinforcement learning, dynamic power management, Bayes classification},
location = {San Diego, California},
series = {DAC '11}
}

@inproceedings{10.1145/2018358.2018383,
author = {Boer, Alexander and van Engers, Tom},
title = {An Agent-Based Legal Knowledge Acquisition Methodology for Agile Public Administration},
year = {2011},
isbn = {9781450307550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018358.2018383},
doi = {10.1145/2018358.2018383},
abstract = {This paper proposes a knowledge elicitation method based on serious gaming for theory construction about the effects of the law on the behaviours of agents. These games provide input to simulations of business process and product design alternatives. For knowledge representation, we have combined agent role descriptions with a generic task framework. An important thesis of this paper is that, in the interest of quick and simple domain analysis, agent roles, not intelligent agents, should be the focal object of simulation of complex social organizations. At least if getting a grip on social complexity is the purpose of modeling.},
booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Law},
pages = {171–180},
numpages = {10},
keywords = {knowledge acquisition, public administration, legal knowledge engineering},
location = {Pittsburgh, Pennsylvania},
series = {ICAIL '11}
}

@inproceedings{10.5555/1996039.1996043,
author = {Zhao, Yao and Cao, Yinzhi and Chen, Yan and Zhang, Ming and Goyal, Anup},
title = {Rake: Semantics Assisted Network-Based Tracing Framework},
year = {2011},
publisher = {IEEE Press},
abstract = {The ability to trace request execution paths is critical for diagnosing performance faults in large-scale distributed systems. Previous black-box and white-box approaches are either inaccurate or invasive. We present a novel semantics-assisted gray-box tracing approach, called Rake, which can accurately trace individual request by observing network traffic. Rake infers the causality between messages by identifying polymorphic IDs in messages according to application semantics. To make Rake universally applicable, we design a Rake language so that users can easily describe necessary semantics of their applications while reusing the core Rake component. We evaluate Rake using a few popular distributed applications, including web search, distributed computing cluster, content provider network, and online chatting. Our results demonstrate Rake is much more accurate than the black-box approaches while requiring no modification to OS/applications. In the CoralCDN (a content distributed network) experiments, Rake links messages with much higher accuracy than WAP5, a state-of-the-art blackbox approach. In the Hadoop (a distributed computing cluster platform) experiments, Rake helps reveal several previously unknown issues that may lead to performance degradation, including a RPC (Remote Procedure Call) abusing problem.},
booktitle = {Proceedings of the Nineteenth International Workshop on Quality of Service},
articleno = {3},
numpages = {9},
location = {San Jose, California},
series = {IWQoS '11}
}

@inproceedings{10.1145/1995966.1996000,
author = {Squicciarini, Anna Cinzia and Sundareswaran, Smitha and Lin, Dan and Wede, Josh},
title = {A3P: Adaptive Policy Prediction for Shared Images over Popular Content Sharing Sites},
year = {2011},
isbn = {9781450302562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1995966.1996000},
doi = {10.1145/1995966.1996000},
abstract = {More and more people go online today and share their personal images using popular web services like Picasa. While enjoying the convenience brought by advanced technology, people also become aware of the privacy issues of data being shared. Recent studies have highlighted that people expect more tools to allow them to regain control over their privacy. In this work, we propose an Adaptive Privacy Policy Prediction (A3P) system to help users compose privacy settings for their images. In particular, we examine the role of image content and metadata as possible indicators of users' privacy preferences. We propose a two-level image classification framework to obtain image categories which may be associated with similar policies. Then, we develop a policy prediction algorithm to automatically generate a policy for each newly uploaded image. Most importantly, the generated policy will follow the trend of the user's privacy concerns evolved with time. We have conducted an extensive user study and the results demonstrate effectiveness of our system with the prediction accuracy around 90%.},
booktitle = {Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia},
pages = {261–270},
numpages = {10},
keywords = {privacy, images},
location = {Eindhoven, The Netherlands},
series = {HT '11}
}

@inproceedings{10.1145/1993636.1993733,
author = {Woodruff, David P.},
title = {Near-Optimal Private Approximation Protocols via a Black Box Transformation},
year = {2011},
isbn = {9781450306911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993636.1993733},
doi = {10.1145/1993636.1993733},
abstract = {We show the following transformation: any two-party protocol for outputting a (1+ε)-approximation to f(x,y) = ∑j=1n g(xj, yj) with probability at least 2/3, for any non-negative efficienty computable function g, can be transformed into a two-party private approximation protocol with only a polylogarithmic factor loss in communication, computation, and round complexity. In general it is insufficient to use secure function evaluation or fully homomorphic encryption on a standard, non-private protocol for approximating f. This is because the approximation may reveal information about x and y that does not follow from f(x,y). Applying our transformation and variations of it, we obtain near-optimal private approximation protocols for a wide range of problems in the data stream literature for which previously nothing was known. We give near-optimal private approximation protocols for the lp-distance for every p ≥ 0, for the heavy hitters and importance sampling problems with respect to any lp-norm, for the max-dominance and other dominant lp-norms, for the distinct summation problem, for entropy, for cascaded frequency moments, for subspace approximation and block sampling, and for measuring independence of datasets. Using a result for data streams, we obtain private approximation protocols with polylogarithmic communication for every non-decreasing and symmetric function g(xj,yj) = h(xj-yj) with at most quadratic growth. If the original (non-private) protocol is a simultaneous protocol, e.g., a sketching algorithm, then our only cryptographic assumption is efficient symmetric computationally-private information retrieval; otherwise it is fully homomorphic encryption. For all but one of these problems, the original protocol is a sketching algorithm. Our protocols generalize straightforwardly to more than two parties.},
booktitle = {Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing},
pages = {735–744},
numpages = {10},
keywords = {cryptography, data stream algorithms, communication complexity, approximation algorithms},
location = {San Jose, California, USA},
series = {STOC '11}
}

@inproceedings{10.1145/1996092.1996094,
author = {Ren, Kai and L\'{o}pez, Julio and Gibson, Garth},
title = {Otus: Resource Attribution in Data-Intensive Clusters},
year = {2011},
isbn = {9781450307000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996092.1996094},
doi = {10.1145/1996092.1996094},
abstract = {Frameworks for large scale data-intensive applications, such as Hadoop and Dryad, have gained tremendous popularity.Understanding the resource requirements of these frameworks and the performance characteristics of distributed applications is inherently difficult. We present an approach, based on resource attribution, that aims at facilitating performance analyses of distributed data-intensive applications.This approach is embodied in Otus, a monitoring tool to attribute resource usage to jobs and services in Hadoop clusters.Otus collects and correlates performance metrics from distributed components and provides views that display time-series of these metrics filtered and aggregated using multiple criteria.Our evaluation shows that this approach can be deployed without incurring major overheads.Our experience with Otus in a production cluster suggests its effectiveness at helping users and cluster administrators with application performance analysis and troubleshooting.},
booktitle = {Proceedings of the Second International Workshop on MapReduce and Its Applications},
pages = {1–8},
numpages = {8},
keywords = {resource attribution, monitoring, metrics correlation, data-intensive systems},
location = {San Jose, California, USA},
series = {MapReduce '11}
}

@inproceedings{10.1145/1996130.1996143,
author = {Zhou, Bowen and Kulkarni, Milind and Bagchi, Saurabh},
title = {Vrisha: Using Scaling Properties of Parallel Programs for Bug Detection and Localization},
year = {2011},
isbn = {9781450305525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996130.1996143},
doi = {10.1145/1996130.1996143},
abstract = {Detecting and isolating bugs that arise in parallel programs is a tedious and a challenging task. An especially subtle class of bugs are those that are scale-dependent: while small-scale test cases may not exhibit the bug, the bug arises in large-scale production runs, and can change the result or performance of an application. A popular approach to finding bugs is statistical bug detection, where abnormal behavior is detected through comparison with bug-free behavior. Unfortunately, for scale-dependent bugs, there may not be bug-free runs at large scales and therefore traditional statistical techniques are not viable. In this paper, we propose Vrisha, a statistical approach to detecting and localizing scale-dependent bugs. Vrisha detects bugs in large-scale programs by building models of behavior based on bug-free behavior at small scales. These models are constructed using kernel canonical correlation analysis (KCCA) and exploit scale-determined properties, whose values are predictably dependent on application scale. We use Vrisha to detect and diagnose two bugs caused by errors in popular MPI libraries and show that our techniques can be implemented with low overhead and low false-positive rates.},
booktitle = {Proceedings of the 20th International Symposium on High Performance Distributed Computing},
pages = {85–96},
numpages = {12},
keywords = {bug detection, large-scale bugs, KCCA},
location = {San Jose, California, USA},
series = {HPDC '11}
}

@inproceedings{10.1145/1996109.1996119,
author = {Ramakrishnan, Lavanya and Zbiegel, Piotr T. and Campbell, Scott and Bradshaw, Rick and Canon, Richard Shane and Coghlan, Susan and Sakrejda, Iwona and Desai, Narayan and Declerck, Tina and Liu, Anping},
title = {Magellan: Experiences from a Science Cloud},
year = {2011},
isbn = {9781450306997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996109.1996119},
doi = {10.1145/1996109.1996119},
abstract = {Cloud resources promise to be an avenue to address new categories of scientific applications including data-intensive science applications, on-demand/surge computing, and applications that require customized software environments. However, there is a limited understanding on how to operate and use clouds for scientific applications. Magellan, a project funded through the Department of Energy's (DOE) Advanced Scientific Computing Research (ASCR) program, is investigating the use of cloud computing for science at the Argonne Leadership Computing Facility (ALCF) and the National Energy Research Scientific Computing Facility (NERSC). In this paper, we detail the experiences to date at both sites and identify the gaps and open challenges from both a resource provider as well as application perspective.},
booktitle = {Proceedings of the 2nd International Workshop on Scientific Cloud Computing},
pages = {49–58},
numpages = {10},
keywords = {virtual machines, science, cloud computing, programming model, mapreduce},
location = {San Jose, California, USA},
series = {ScienceCloud '11}
}

@inproceedings{10.1145/1996014.1996019,
author = {Weissman, Jon B. and Sundarrajan, Pradeep and Gupta, Abhishek and Ryden, Matthew and Nair, Rohit and Chandra, Abhishek},
title = {Early Experience with the Distributed Nebula Cloud},
year = {2011},
isbn = {9781450307048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996014.1996019},
doi = {10.1145/1996014.1996019},
abstract = {Current cloud infrastructures are important for their ease of use and performance. However, they suffer from several shortcomings. The main problem is inefficient data mobility due to the centralization of cloud resources. We believe such clouds are highly unsuited for dispersed-data-intensive applications, where the data may be spread at multiple geographical locations (e.g., distributed user blogs). Instead, we propose a new cloud model called Nebula: a dispersed, context-aware, and cost-effective cloud. We provide experimental evidence for the need for Nebulas using a distributed blog analysis application followed by the system architecture and components of our system.},
booktitle = {Proceedings of the Fourth International Workshop on Data-Intensive Distributed Computing},
pages = {17–26},
numpages = {10},
keywords = {dsitributed computing, clouds},
location = {San Jose, California, USA},
series = {DIDC '11}
}

