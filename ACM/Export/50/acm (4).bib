@article{10.1145/1805974.1805983,
author = {Vaidya, Jaideep and Atluri, Vijayalakshmi and Guo, Qi},
title = {The Role Mining Problem: A Formal Perspective},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1094-9224},
url = {https://doi.org/10.1145/1805974.1805983},
doi = {10.1145/1805974.1805983},
abstract = {Devising a complete and correct set of roles has been recognized as one of the most important and challenging tasks in implementing role-based access control. A key problem related to this is the notion of goodness/interestingness—when is a role good/interesting? In this article, we define the Role Mining Problem (RMP) as the problem of discovering an optimal set of roles from existing user permissions. The main contribution of this article is to formally define RMP and analyze its theoretical bounds. In addition to the above basic RMP, we introduce two different variations of the RMP, called the δ-Approx RMP and the minimal-noise RMP that have pragmatic implications. We reduce the known “Set Basis Problem” to RMP to show that RMP is an NP-complete problem. An important contribution of this article is also to show the relation of the RMP to several problems already identified in the data mining and data analysis literature. By showing that the RMP is in essence reducible to these known problems, we can directly borrow the existing implementation solutions and guide further research in this direction. We also develop a heuristic solution based on the previously proposed FastMiner algorithm, which is very accurate and efficient.},
journal = {ACM Trans. Inf. Syst. Secur.},
month = jul,
articleno = {27},
numpages = {31},
keywords = {role engineering, role mining, RBAC}
}

@inproceedings{10.1145/1838574.1838598,
author = {Zhao, Lan and Lee, Wonjun and Song, Carol X. and Huber, Matthew and Goldner, Aaron},
title = {Bringing High Performance Climate Modeling into the Classroom},
year = {2010},
isbn = {9781605588186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1838574.1838598},
doi = {10.1145/1838574.1838598},
abstract = {Climate science educators face great challenges on combining theory with hands-on practices in teaching climate modeling. Typical model runs require large computation and storage resources that may not be available on a campus. Additionally, the training and support required to bring novices up to speed would consume significant class time. The same challenges also exist across many other science and engineering disciplines. The TeraGrid science gateway program is leading the way of a new paradigm in addressing such challenges. As part of the TeraGrid science gateway initiative, The Purdue CCSM portal aims at assisting both research and education users to run Community Climate System Model (CCSM) simulations using the TeraGrid high performance computing resources. It provides a one-stop shop for creating, configuring, running CCSM simulations as well as managing jobs and processing output data. The CCSM portal was used in a Purdue graduate class for students to get hands-on experience with running world class climate simulations and use the results to study climate change impact on political policies. The CCSM portal is based on a service-oriented architecture with multiple interfaces to facilitate training. This paper describes the design of the CCSM portal with the goal of supporting classroom users, the challenges of utilizing the portal in a classroom setting, and the solutions implemented. We present two student projects from the fall 2009 class that successfully used the CCSM portal.},
booktitle = {Proceedings of the 2010 TeraGrid Conference},
articleno = {24},
numpages = {7},
keywords = {community climate system model (CCSM), user interfaces, science gateway, education users, TeraGrid},
location = {Pittsburgh, Pennsylvania},
series = {TG '10}
}

@inproceedings{10.1145/1839594.1839607,
author = {Meerbaum-Salant, Orni and Armoni, Michal and Ben-Ari, Mordechai (Moti)},
title = {Learning Computer Science Concepts with Scratch},
year = {2010},
isbn = {9781450302579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1839594.1839607},
doi = {10.1145/1839594.1839607},
abstract = {Scratch is a visual programming environment that is widely used by young people. We investigated if Scratch can be used to teach concepts of computer science. We developed new learning materials for middle-school students that were designed according to the constructionist philosophy of Scratch and evaluated them in two schools. The classes were normal classes, not extracurricular activities whose participants are self-selected. Questionnaires and a test were constructed based upon a novel combination of the Revised Bloom Taxonomy and the SOLO taxonomy. These quantitative instruments were augmented with a qualitative analysis of observations within the classes. The results showed that in general students could successfully learn important concepts of computer science, although there were some problems with initialization, variables and concurrency; these problems can be overcome by modifications to the teaching process.},
booktitle = {Proceedings of the Sixth International Workshop on Computing Education Research},
pages = {69–76},
numpages = {8},
keywords = {scratch, middle schools, solo taxonomy, concurrency, bloom's taxonomy},
location = {Aarhus, Denmark},
series = {ICER '10}
}

@inproceedings{10.1145/1839594.1839609,
author = {Kinnunen, Paivi and Simon, Beth},
title = {Experiencing Programming Assignments in CS1: The Emotional Toll},
year = {2010},
isbn = {9781450302579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1839594.1839609},
doi = {10.1145/1839594.1839609},
abstract = {The constant comparative method was used to explicate the experience computing majors have with programming assignments in a CS1 course. Findings from a series of four interviews conducted with a purposeful sample of nine students revealed that the primary reflective experience with programming assignments was emotional. That is, the way students remembered and discussed their experiences with programming assignments was dominated by emotional experiences and reactions. We identified six stages whose dimensions capture the variation in students' emotional experiences with programming assignments. This paper reports the beginnings of analysis geared at developing a theory of students' programming assignment experience, rich in detail and grounded in student experience. When completed such a theory may lead to curricular supports, interventions, or tools, to help steer student experience away from the most harmful of emotional tolls.},
booktitle = {Proceedings of the Sixth International Workshop on Computing Education Research},
pages = {77–86},
numpages = {10},
keywords = {retention, emotion, cs1, novice programmers},
location = {Aarhus, Denmark},
series = {ICER '10}
}

@inproceedings{10.1145/1858171.1858218,
author = {Chen, Yunan},
title = {Take It Personally: Accounting for Individual Difference in Designing Diabetes Management Systems},
year = {2010},
isbn = {9781450301039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858171.1858218},
doi = {10.1145/1858171.1858218},
abstract = {The goal of this study was to investigate how diabetes patients use health information to support their daily disease management. A qualitative interview study was conducted with type-2 diabetes patients and healthcare providers. The analysis suggests that individual diabetes patients have a unique way of managing their care through the interpretation of personal health experiences. The ways in which patients learn to interact with their diabetes are detailed in this paper in four themes: understanding typical life routine, accommodating atypical activities, disproving &amp; discovering healthy tips and reevaluating personal expectations. The findings of this study call for a diabetes management system that addresses a patient's physiological, social and psychological activities within the process of individual disease management. The finding opens up new opportunities for designing interactive systems to account for individual differences, encouraging positive patient involvement and sustaining long-term health outcomes.},
booktitle = {Proceedings of the 8th ACM Conference on Designing Interactive Systems},
pages = {252–261},
numpages = {10},
keywords = {diabetes management, personal health experience, individual experience},
location = {Aarhus, Denmark},
series = {DIS '10}
}

@inproceedings{10.1145/1858171.1858239,
author = {Burmester, Michael and Mast, Marcus and J\"{a}ger, Kilian and Homans, Hendrik},
title = {Valence Method for Formative Evaluation of User Experience},
year = {2010},
isbn = {9781450301039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858171.1858239},
doi = {10.1145/1858171.1858239},
abstract = {This paper describes a method for formative evaluation of the user experience based on the user experience model of Hassenzahl [11]. It captures positive and negative feelings during the exploration of an interactive product. In a subsequent retrospective interview phase users indicate for each instance of a positive or negative feeling the product design aspects inducing it. This phase further employs the laddering interview technique [24] to reveal the meaning of product design aspects to the user and the underlying fulfilled or frustrated needs. The generated information helps designers to understand and optimize the user experience potential of a product.},
booktitle = {Proceedings of the 8th ACM Conference on Designing Interactive Systems},
pages = {364–367},
numpages = {4},
keywords = {user experience, formative evaluation},
location = {Aarhus, Denmark},
series = {DIS '10}
}

@inproceedings{10.1145/1858171.1858192,
author = {Kim, Tanyoung and Hong, Hwajung and Magerko, Brian},
title = {Design Requirements for Ambient Display That Supports Sustainable Lifestyle},
year = {2010},
isbn = {9781450301039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858171.1858192},
doi = {10.1145/1858171.1858192},
abstract = {People are ready to change themselves to adopt more eco-friendly habits such as conserving electricity when they are aware of the possible problems of their lifestyle. In this sense, ambient display, which users experience occasionally without its interfering with their primary tasks, is well suited to provide the feedback of their personal activities in a more subtle manner than direct information presentation. We present the results of user studies with two ambient displays in different visualization styles. Participants showed diverse usage behaviors of ambient displays according to their motivational level of sustainable lifestyle. In addition, iconic metaphor of eco-visualization can trigger more emotional attachment while indexical representation helps retrospective functions. Finally, we suggest design requirements for ambient displays that support different stages of persuasion from raising awareness to motivating to change behaviors and to maintaining desired habits.},
booktitle = {Proceedings of the 8th ACM Conference on Designing Interactive Systems},
pages = {103–112},
numpages = {10},
keywords = {eco-visualization, persuasive technology, behavior change, sustainable design, ambient display},
location = {Aarhus, Denmark},
series = {DIS '10}
}

@inproceedings{10.1145/1840784.1840802,
author = {Frommholz, Ingo and Larsen, Birger and Piwowarski, Benjamin and Lalmas, Mounia and Ingwersen, Peter and van Rijsbergen, Keith},
title = {Supporting Polyrepresentation in a Quantum-Inspired Geometrical Retrieval Framework},
year = {2010},
isbn = {9781450302470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1840784.1840802},
doi = {10.1145/1840784.1840802},
abstract = {The relevance of a document has many facets, going beyond the usual topical one, which have to be considered to satisfy a user's information need. Multiple representations of documents, like user-given reviews or the actual document content, can give evidence towards certain facets of relevance. In this respect polyrepresentation of documents, where such evidence is combined, is a crucial concept to estimate the relevance of a document. In this paper, we discuss how a geometrical retrieval framework inspired by quantum mechanics can be extended to support polyrepresentation. We show by example how different representations of a document can be modelled in a Hilbert space, similar to physical systems known from quantum mechanics. We further illustrate how these representations are combined by means of the tensor product to support polyrepresentation, and discuss the case that representations of documents are not independent from a user point of view. Besides giving a principled framework for polyrepresentation, the potential of this approach is to capture and formalise the complex interdependent relationships that the different representations can have between each other.},
booktitle = {Proceedings of the Third Symposium on Information Interaction in Context},
pages = {115–124},
numpages = {10},
keywords = {quantum-inspired model, polyrepresentation},
location = {New Brunswick, New Jersey, USA},
series = {IIiX '10}
}

@inproceedings{10.5555/1873781.1873788,
author = {Bergsma, Shane and Cherry, Colin},
title = {Fast and Accurate Arc Filtering for Dependency Parsing},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We propose a series of learned arc filters to speed up graph-based dependency parsing. A cascade of filters identify implausible head-modifier pairs, with time complexity that is first linear, and then quadratic in the length of the sentence. The linear filters reliably predict, in context, words that are roots or leaves of dependency trees, and words that are likely to have heads on their left or right. We use this information to quickly prune arcs from the dependency graph. More than 78% of total arcs are pruned while retaining 99.5% of the true dependencies. These filters improve the speed of two state-of-the-art dependency parsers, with low overhead and negligible loss in accuracy.},
booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics},
pages = {53–61},
numpages = {9},
location = {Beijing, China},
series = {COLING '10}
}

@article{10.1145/1851175.1851178,
author = {Garrison, Gary and Wakefield, Robin L. and Xu, Xiaobo and `Kim, Sang Hyun},
title = {Globally Distributed Teams: The Effect of Diversity on Trust, Cohesion and Individual Performance},
year = {2010},
issue_date = {August 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/1851175.1851178},
doi = {10.1145/1851175.1851178},
abstract = {Globally distributed teams are becoming more common among organizations that seek to maximize knowledge creation and innovation for competitive advantage. Although they are becoming widely used among global organizations, distributed teams are creating an environment replete in cultural and functional diversity. Whereas synergy among members is desired, diversity is likely to hinder team cohesion and individual performance. Our study models and empirically tests the effect of perceptions of diversity on trust, cohesion, and individual performance in actual globally distributed teams. The results indicate that individual productivity is negatively influenced by the extent of diversity within a team; however, this liability may be restrained if an environment of trust is encouraged and team cohesion develops.},
journal = {SIGMIS Database},
month = aug,
pages = {27–48},
numpages = {22},
keywords = {globally distributed teams, diversity, trust, cohesion, performance}
}

@inproceedings{10.5555/1944566.1944570,
author = {Balahur, Alexandra and Boldrini, Ester and Montoyo, Andr\'{e}s and Mart\'{\i}nez-Barco, Patricio},
title = {Going beyond Traditional QA Systems: Challenges and Keys in Opinion Question Answering},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The treatment of factual data has been widely studied in different areas of Natural Language Processing (NLP). However, processing subjective information still poses important challenges. This paper presents research aimed at assessing techniques that have been suggested as appropriate in the context of subjective - Opinion Question Answering (OQA). We evaluate the performance of an OQA with these new components and propose methods to optimally tackle the issues encountered. We assess the impact of including additional resources and processes with the purpose of improving the system performance on two distinct blog datasets. The improvements obtained for the different combination of tools are statistically significant. We thus conclude that the proposed approach is adequate for the OQA task, offering a good strategy to deal with opinionated questions.},
booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics: Posters},
pages = {27–35},
numpages = {9},
location = {Beijing, China},
series = {COLING '10}
}

@inproceedings{10.5555/1944566.1944624,
author = {Ji, Heng},
title = {Challenges from Information Extraction to Information Fusion},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Information Extraction (IE) technology is facing new challenges of dealing with large-scale heterogeneous data sources from different documents, languages and modalities. Information fusion, a new emerging area derived from IE, aims to address these challenges. We specify the requirements and possible solutions to perform information fusion. The issues include redundancy removal, contradiction resolution and uncertainty reduction. We believe this is a critical step to advance IE to a higher level of performance and portability.},
booktitle = {Proceedings of the 23rd International Conference on Computational Linguistics: Posters},
pages = {507–515},
numpages = {9},
location = {Beijing, China},
series = {COLING '10}
}

@article{10.1145/1851175.1851179,
author = {Heart, Tsipi},
title = {Who is out There? Exploring the Effects of Trust and Perceived Risk on Saas Adoption Intentions},
year = {2010},
issue_date = {August 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/1851175.1851179},
doi = {10.1145/1851175.1851179},
abstract = {Software as a Service (SaaS) is a relatively new organizational application sourcing alternative, offering organizations the option to access applications--via the Internet--that are remotely hosted on offsite servers instead of installing equivalent applications in-house, thus presumably saving costs. Although SaaS has been offered since the late 1990s, so far it has not become a dominant sourcing alternative for organizational core applications, in spite of the fact that most leading IT companies now offer remotely-hosted organization-wide applications. This study conceptualized and empirically tested a model of the effects of the perceived risk of SaaS and trust in the SaaS vendor community on the organizational intention to adopt SaaS at this early stage of the SaaS market. Three novel, risk-related constructs were developed: perceived risk of SaaS, perceived risk of systems unavailability, and perceived risk of data insecurity. Likewise, three new trust-related constructs were also conceived: trust in the SaaS vendor community, perceived capabilities and perceived reputation of the SaaS vendor community. An empirical test of the model demonstrated the negative effect of perceived risk and the positive effects of trust in, and the reputation of, the SaaS vendor community, on the intention to adopt SaaS. Trust in the SaaS vendor community was also found to strongly affect all three risk concepts.},
journal = {SIGMIS Database},
month = aug,
pages = {49–68},
numpages = {20},
keywords = {software as a service, trust, perceived capabilities, perceived reputation, perceived risk, intention to adopt}
}

@inproceedings{10.1145/1931344.1931359,
author = {Millonig, Alexandra and Maierbrugger, Gudrun},
title = {Using Semi-Automated Shadowing for Analysing Stress-Inducedspatio-Temporal Behaviour Patterns of Passengers in Public Transport Infrastructures},
year = {2010},
isbn = {9781605589268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1931344.1931359},
doi = {10.1145/1931344.1931359},
abstract = {In the course of the scientific project IANUS, we examined the impacts of stress on the physical and psychological condition of passengers in train stations in order to identify relevant determinants and to develop recommendations for reducing stress-inducing factors in public transport infrastructures. We used a combination of biometric measuring of heart rate, semi-automated observation of spatio-temporal behaviour, psychological interviews, eye-tracking and visual field analyses during laboratory and field tests. In this contribution, we focus on the methodology of observing the spatio-temporal behaviour patterns of 35 individuals participating in field tests conducted in Vienna, Austria. The observation datasets have been analysed concerning velocities, route choice, stopping behaviour and related activities in order to identify noticeable patterns and events that can indicate stress-related behaviour. The outcomes were subsequently consolidated with the results of the complementary methods in order to identify stress-inducing factors in transport infrastructures.},
booktitle = {Proceedings of the 7th International Conference on Methods and Techniques in Behavioral Research},
articleno = {15},
numpages = {5},
keywords = {across-method triangulation, pedestrian spatio-temporal behaviour, shadowing},
location = {Eindhoven, The Netherlands},
series = {MB '10}
}

@inproceedings{10.1145/1931344.1931376,
author = {Voynarovskaya, Natalia and Gorbunov, Roman and Barakova, Emilia and Rauterberg, Matthias},
title = {Automatic Mental Heath Assistant: Monitoring and Measuring Nonverbal Behavior of the Crew during Long-Term Missions},
year = {2010},
isbn = {9781605589268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1931344.1931376},
doi = {10.1145/1931344.1931376},
abstract = {This paper presents a method for monitoring the mental state of small isolated crews during long-term missions (such as space mission, polar expeditions, submarine crews, meteorological stations, and etc.) The research is done as a part of Automatic Mental Health Assistant (AMHA) project which aims to develop set of techniques for automatic measuring of intra- and inter- personal states in working groups. The method is focused on those aspects of psychological and sociological states that are crucial for the performance of the crew. In particular, we focus on measuring of emotional stress, initial signs of conflicts, trust, and ability to collaborate. The present research is performed in collaboration with MARS-500 experiment in which a small group of people is isolated for a long period of time. The MARS-500 experiment, in this way, provides a unique platform for study of human-human interaction. The confinement study will imitate all key peculiarities expected to be present during future missions to Mars (i.e. ultra long duration flight, need for autonomy, complicated communication with a digital communication center due to signal delay, and limited stock of expendables). The developed method is also currently tested by usage of a web-based platform.},
booktitle = {Proceedings of the 7th International Conference on Methods and Techniques in Behavioral Research},
articleno = {32},
numpages = {5},
keywords = {colored trails, emotions, social network analysis, nonverbal communication, long-term missions, evolutionary game theory},
location = {Eindhoven, The Netherlands},
series = {MB '10}
}

@inproceedings{10.1145/1931344.1931350,
author = {van den Broek, Egon L. and Nijholt, Anton and Westerink, Joyce H. D. M.},
title = {Unveiling Affective Signals},
year = {2010},
isbn = {9781605589268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1931344.1931350},
doi = {10.1145/1931344.1931350},
abstract = {The ability to process and, subsequently, understand affective signals is the core of emotional intelligence and empathy. However, more than a decade of research in affective computing has shown that it is hard to develop computational models of this process. We pose that the solution for this problem lays in a better understanding of how to process these affective signals. This article introduces a symposium that brought together various approaches towards unveiling affective signals. As such, it is envisioned to be a springboard for affective computing.},
booktitle = {Proceedings of the 7th International Conference on Methods and Techniques in Behavioral Research},
articleno = {6},
numpages = {4},
keywords = {pattern recognition, emotion, signal processing, affect, methods, affective computing},
location = {Eindhoven, The Netherlands},
series = {MB '10}
}

@inproceedings{10.1145/1962300.1962346,
author = {van der Sar, Manon and Mulder, Ingrid},
title = {Human-Camera Interaction: An Exploratory Study on People's Emotions and Attitude towards Cameras},
year = {2010},
isbn = {9781605589466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1962300.1962346},
doi = {10.1145/1962300.1962346},
abstract = {Motivation -- Cameras are increasingly dominating our life, but do these influence our behaviour as well? What are people's emotions and attitude towards camera surveillance?Research approach -- In an exploratory study (n=23) people's emotional reactions to (visible and hidden) cameras were observed. Next, a survey studied people's attitude towards camera surveillance at different places (n=102).Findings/Design -- Results suggest that people are conditioned by cameras, as they react both consciously and unconsciously to cameras. People like to spy other people, while they do not like to be observed.Research limitations/Implications -- The current study is exploratory, which limited generalisation of our findings.Originality/Value -- The research contributes to the public debate on camera surveillance and how people (un)consciously react to cameras.Take away message -- Cameras evoke emotions.},
booktitle = {Proceedings of the 28th Annual European Conference on Cognitive Ergonomics},
pages = {223–226},
numpages = {4},
keywords = {emotion, camera surveillance, public safety, exploratory study},
location = {Delft, Netherlands},
series = {ECCE '10}
}

@inproceedings{10.1145/1962300.1962342,
author = {Chauvin, Christine and Coppin, Gilles and Ch\'{e}n\'{e}, H\'{e}l\'{e}na},
title = {Analysis of the Dynamics of Common Ground: A Methodological Proposal},
year = {2010},
isbn = {9781605589466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1962300.1962342},
doi = {10.1145/1962300.1962342},
abstract = {The elaboration and maintenance of a common ground between team members are crucial aspects for the efficiency and safety of complex systems.A method has been designed to analyze communication between members of a bridge team of a merchant ship, that was recorded during training sessions on a bridge simulator. This method aims to pointing out the positive or negative dynamics of their common ground. It relies on Hoc coding schemes, as well as on the Bales Interaction Process Analysis.Using a transposition of computer science parsing tools, we try to qualify the different levels and the dynamics of the common ground, expressed in terms of left-open requests or average time of closure.This research is in progress. Its main interest is to combine existing coding schemes (coming from cognitive ergonomics and from social psychology) to formalize functional communications and to use computer science tools to point out and qualify the dynamics of a common ground.},
booktitle = {Proceedings of the 28th Annual European Conference on Cognitive Ergonomics},
pages = {209–212},
numpages = {4},
keywords = {dynamic systems, cooperation, team management, ship handling},
location = {Delft, Netherlands},
series = {ECCE '10}
}

@inproceedings{10.1145/1851182.1851219,
author = {Mahimkar, Ajay Anil and Song, Han Hee and Ge, Zihui and Shaikh, Aman and Wang, Jia and Yates, Jennifer and Zhang, Yin and Emmons, Joanne},
title = {Detecting the Performance Impact of Upgrades in Large Operational Networks},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851219},
doi = {10.1145/1851182.1851219},
abstract = {Networks continue to change to support new applications, improve reliability and performance and reduce the operational cost. The changes are made to the network in the form of upgrades such as software or hardware upgrades, new network or service features and network configuration changes. It is crucial to monitor the network when upgrades are made because they can have a significant impact on network performance and if not monitored may lead to unexpected consequences in operational networks. This can be achieved manually for a small number of devices, but does not scale to large networks with hundreds or thousands of routers and extremely large number of different upgrades made on a regular basis.In this paper, we design and implement a novel infrastructure MERCURY for detecting the impact of network upgrades (or triggers) on performance. MERCURY extracts interesting triggers from a large number of network maintenance activities. It then identifies behavior changes in network performance caused by the triggers. It uses statistical rule mining and network configuration to identify commonality across the behavior changes. We systematically evaluate MERCURY using data collected at a large tier-1 ISP network. By comparing to operational practice, we show that MERCURY is able to capture the interesting triggers and behavior changes induced by the triggers. In some cases, MERCURY also discovers previously unknown network behaviors demonstrating the effectiveness in identifying network conditions flying under the radar.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {303–314},
numpages = {12},
keywords = {change detection, network upgrades, performance impact, statistical data mining},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@article{10.1145/1851275.1851219,
author = {Mahimkar, Ajay Anil and Song, Han Hee and Ge, Zihui and Shaikh, Aman and Wang, Jia and Yates, Jennifer and Zhang, Yin and Emmons, Joanne},
title = {Detecting the Performance Impact of Upgrades in Large Operational Networks},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/1851275.1851219},
doi = {10.1145/1851275.1851219},
abstract = {Networks continue to change to support new applications, improve reliability and performance and reduce the operational cost. The changes are made to the network in the form of upgrades such as software or hardware upgrades, new network or service features and network configuration changes. It is crucial to monitor the network when upgrades are made because they can have a significant impact on network performance and if not monitored may lead to unexpected consequences in operational networks. This can be achieved manually for a small number of devices, but does not scale to large networks with hundreds or thousands of routers and extremely large number of different upgrades made on a regular basis.In this paper, we design and implement a novel infrastructure MERCURY for detecting the impact of network upgrades (or triggers) on performance. MERCURY extracts interesting triggers from a large number of network maintenance activities. It then identifies behavior changes in network performance caused by the triggers. It uses statistical rule mining and network configuration to identify commonality across the behavior changes. We systematically evaluate MERCURY using data collected at a large tier-1 ISP network. By comparing to operational practice, we show that MERCURY is able to capture the interesting triggers and behavior changes induced by the triggers. In some cases, MERCURY also discovers previously unknown network behaviors demonstrating the effectiveness in identifying network conditions flying under the radar.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {303–314},
numpages = {12},
keywords = {network upgrades, statistical data mining, change detection, performance impact}
}

@inproceedings{10.1145/1851290.1851298,
author = {Keshav, Srinivasan and Rosenberg, Catherine},
title = {How Internet Concepts and Technologies Can Help Green and Smarten the Electrical Grid},
year = {2010},
isbn = {9781450301961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851290.1851298},
doi = {10.1145/1851290.1851298},
abstract = {Several powerful forces are gathering to make fundamental and irrevocable changes to the century-old grid. The next-generation grid, often called the 'smart grid,' will feature distributed energy production, vastly more storage, tens of millions of stochastic renewable-energy sources, and the use of communication technologies both to allow precise matching of supply to demand and to incentivize appropriate consumer behaviour. These changes will have the effect of reducing energy waste and reducing the carbon footprint of the grid, making it 'smarter' and 'greener.' In this position paper, we discuss how the concepts and techniques pioneered by the Internet, the fruit of four decades of research in this area, are directly applicable to the design of a smart, green grid. This is because both the Internet and the electrical grid are designed to meet fundamental needs, for information and for energy, respectively, by connecting geographically dispersed suppliers with geographically dispersed consumers. Keeping this and other similarities (and fundamental differences, as well) in mind, we propose several specific areas where Internet concepts and technologies can contribute to the development of a smart, green grid. We also describe some areas where the Internet operations can be improved based on the experience gained in the electrical grid. We hope that our work will initiate a dialogue between the Internet and the smart grid communities.},
booktitle = {Proceedings of the First ACM SIGCOMM Workshop on Green Networking},
pages = {35–40},
numpages = {6},
keywords = {green networking, electrical grid},
location = {New Delhi, India},
series = {Green Networking '10}
}

@inproceedings{10.1145/1851182.1851199,
author = {McSherry, Frank and Mahajan, Ratul},
title = {Differentially-Private Network Trace Analysis},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851199},
doi = {10.1145/1851182.1851199},
abstract = {We consider the potential for network trace analysis while providing the guarantees of "differential privacy." While differential privacy provably obscures the presence or absence of individual records in a dataset, it has two major limitations: analyses must (presently) be expressed in a higher level declarative language; and the analysis results are randomized before returning to the analyst.We report on our experiences conducting a diverse set of analyses in a differentially private manner. We are able to express all of our target analyses, though for some of them an approximate expression is required to keep the error-level low. By running these analyses on real datasets, we find that the error introduced for the sake of privacy is often (but not always) low even at high levels of privacy. We factor our learning into a toolkit that will be likely useful for other analyses. Overall, we conclude that differential privacy shows promise for a broad class of network analyses.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {123–134},
numpages = {12},
keywords = {trace analysis, differential privacy},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@article{10.1145/1851275.1851199,
author = {McSherry, Frank and Mahajan, Ratul},
title = {Differentially-Private Network Trace Analysis},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/1851275.1851199},
doi = {10.1145/1851275.1851199},
abstract = {We consider the potential for network trace analysis while providing the guarantees of "differential privacy." While differential privacy provably obscures the presence or absence of individual records in a dataset, it has two major limitations: analyses must (presently) be expressed in a higher level declarative language; and the analysis results are randomized before returning to the analyst.We report on our experiences conducting a diverse set of analyses in a differentially private manner. We are able to express all of our target analyses, though for some of them an approximate expression is required to keep the error-level low. By running these analyses on real datasets, we find that the error introduced for the sake of privacy is often (but not always) low even at high levels of privacy. We factor our learning into a toolkit that will be likely useful for other analyses. Overall, we conclude that differential privacy shows promise for a broad class of network analyses.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {123–134},
numpages = {12},
keywords = {trace analysis, differential privacy}
}

@inproceedings{10.1145/1851290.1851294,
author = {Krioukov, Andrew and Mohan, Prashanth and Alspaugh, Sara and Keys, Laura and Culler, David and Katz, Randy H.},
title = {NapSAC: Design and Implementation of a Power-Proportional Web Cluster},
year = {2010},
isbn = {9781450301961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851290.1851294},
doi = {10.1145/1851290.1851294},
abstract = {Energy consumption is a major and costly problem in data centers. A large fraction of this energy goes to powering idle machines that are not doing any useful work. We identify two causes of this inefficiency: low server utilization and a lack of power-proportionality. To address this problem we present a design for an power-proportional cluster consisting of a power-aware cluster manager and a set of heterogeneous machines. Our design makes use of currently available energy-efficient hardware, mechanisms for transitioning in and out of low-power sleep states, and dynamic provisioning and scheduling to continually adjust to workload and minimize power consumption. With our design we are able to reduce energy consumption while maintaining acceptable response times for a web service workload based on Wikipedia. With our dynamic provisioning algorithms we demonstrate via simulation a 63% savings in power usage in a typically provisioned datacenter where all machines are left on and awake at all times. Our results show that we are able to achieve close to 90% of the savings a theoretically optimal provisioning scheme would achieve. We have also built a prototype cluster which runs Wikipedia to demonstrate the use of our design in a real environment.},
booktitle = {Proceedings of the First ACM SIGCOMM Workshop on Green Networking},
pages = {15–22},
numpages = {8},
keywords = {heterogenous hardware, power management, cluster, power proportional, web application, data center, web server, energy},
location = {New Delhi, India},
series = {Green Networking '10}
}

@inproceedings{10.1145/1851182.1851227,
author = {Pujol, Josep M. and Erramilli, Vijay and Siganos, Georgos and Yang, Xiaoyuan and Laoutaris, Nikos and Chhabra, Parminder and Rodriguez, Pablo},
title = {The Little Engine(s) That Could: Scaling Online Social Networks},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851227},
doi = {10.1145/1851182.1851227},
abstract = {The difficulty of scaling Online Social Networks (OSNs) has introduced new system design challenges that has often caused costly re-architecting for services like Twitter and Facebook. The complexity of interconnection of users in social networks has introduced new scalability challenges. Conventional vertical scaling by resorting to full replication can be a costly proposition. Horizontal scaling by partitioning and distributing data among multiples servers - e.g. using DHTs - can lead to costly inter-server communication.We design, implement, and evaluate SPAR, a social partitioning and replication middle-ware that transparently leverages the social graph structure to achieve data locality while minimizing replication. SPAR guarantees that for all users in an OSN, their direct neighbor's data is co-located in the same server. The gains from this approach are multi-fold: application developers can assume local semantics, i.e., develop as they would for a single server; scalability is achieved by adding commodity servers with low memory and network I/O requirements; and redundancy is achieved at a fraction of the cost.We detail our system design and an evaluation based on datasets from Twitter, Orkut, and Facebook, with a working implementation. We show that SPAR incurs minimum overhead, and can help a well-known open-source Twitter clone reach Twitter's scale without changing a line of its application logic and achieves higher throughput than Cassandra, Facebook's DHT based key-value store database.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {375–386},
numpages = {12},
keywords = {social networks, scalability, replication, partition},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@article{10.1145/1851275.1851227,
author = {Pujol, Josep M. and Erramilli, Vijay and Siganos, Georgos and Yang, Xiaoyuan and Laoutaris, Nikos and Chhabra, Parminder and Rodriguez, Pablo},
title = {The Little Engine(s) That Could: Scaling Online Social Networks},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/1851275.1851227},
doi = {10.1145/1851275.1851227},
abstract = {The difficulty of scaling Online Social Networks (OSNs) has introduced new system design challenges that has often caused costly re-architecting for services like Twitter and Facebook. The complexity of interconnection of users in social networks has introduced new scalability challenges. Conventional vertical scaling by resorting to full replication can be a costly proposition. Horizontal scaling by partitioning and distributing data among multiples servers - e.g. using DHTs - can lead to costly inter-server communication.We design, implement, and evaluate SPAR, a social partitioning and replication middle-ware that transparently leverages the social graph structure to achieve data locality while minimizing replication. SPAR guarantees that for all users in an OSN, their direct neighbor's data is co-located in the same server. The gains from this approach are multi-fold: application developers can assume local semantics, i.e., develop as they would for a single server; scalability is achieved by adding commodity servers with low memory and network I/O requirements; and redundancy is achieved at a fraction of the cost.We detail our system design and an evaluation based on datasets from Twitter, Orkut, and Facebook, with a working implementation. We show that SPAR incurs minimum overhead, and can help a well-known open-source Twitter clone reach Twitter's scale without changing a line of its application logic and achieves higher throughput than Cassandra, Facebook's DHT based key-value store database.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {375–386},
numpages = {12},
keywords = {partition, social networks, replication, scalability}
}

@article{10.1145/1810891.1810916,
author = {McSherry, Frank},
title = {Privacy Integrated Queries: An Extensible Platform for Privacy-Preserving Data Analysis},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/1810891.1810916},
doi = {10.1145/1810891.1810916},
abstract = {Privacy Integrated Queries (PINQ) is an extensible data analysis platform designed to provide unconditional privacy guarantees for the records of the underlying data sets. PINQ provides analysts with access to records through an SQL-like declarative language (LINQ) amidst otherwise arbitrary C# code. At the same time, the design of PINQ's analysis language and its careful implementation provide formal guarantees of differential privacy for any and all uses of the platform. PINQ's guarantees require no trust placed in the expertise or diligence of the analysts, broadening the scope for design and deployment of privacy-preserving data analyses, especially by privacy nonexperts.},
journal = {Commun. ACM},
month = sep,
pages = {89–97},
numpages = {9}
}

@article{10.1145/1836216.1836223,
author = {Blevis, Eli and Blevis, Shunying},
title = {Hope for the Best and Prepare for the Worst: Interaction Design and the Tipping Point},
year = {2010},
issue_date = {September + October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/1836216.1836223},
doi = {10.1145/1836216.1836223},
journal = {Interactions},
month = sep,
pages = {26–30},
numpages = {5}
}

@article{10.14778/1920841.1920908,
author = {Dittrich, Jens and Quian\'{e}-Ruiz, Jorge-Arnulfo and Jindal, Alekh and Kargin, Yagiz and Setty, Vinay and Schad, J\"{o}rg},
title = {Hadoop++: Making a Yellow Elephant Run like a Cheetah (without It Even Noticing)},
year = {2010},
issue_date = {September 2010},
publisher = {VLDB Endowment},
volume = {3},
number = {1–2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1920841.1920908},
doi = {10.14778/1920841.1920908},
abstract = {MapReduce is a computing paradigm that has gained a lot of attention in recent years from industry and research. Unlike parallel DBMSs, MapReduce allows non-expert users to run complex analytical tasks over very large data sets on very large clusters and clouds. However, this comes at a price: MapReduce processes tasks in a scan-oriented fashion. Hence, the performance of Hadoop --- an open-source implementation of MapReduce --- often does not match the one of a well-configured parallel DBMS. In this paper we propose a new type of system named Hadoop++: it boosts task performance without changing the Hadoop framework at all (Hadoop does not even 'notice it'). To reach this goal, rather than changing a working system (Hadoop), we inject our technology at the right places through UDFs only and affect Hadoop from inside. This has three important consequences: First, Hadoop++ significantly outperforms Hadoop. Second, any future changes of Hadoop may directly be used with Hadoop++ without rewriting any glue code. Third, Hadoop++ does not need to change the Hadoop interface. Our experiments show the superiority of Hadoop++ over both Hadoop and HadoopDB for tasks related to indexing and join processing.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {515–529},
numpages = {15}
}

@article{10.1145/1824777.1824787,
author = {Oudot, Steve Y. and Guibas, Leonidas J. and Gao, Jie and Wang, Yue},
title = {Geodesic Delaunay Triangulations in Bounded Planar Domains},
year = {2010},
issue_date = {August 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1549-6325},
url = {https://doi.org/10.1145/1824777.1824787},
doi = {10.1145/1824777.1824787},
abstract = {We introduce a new feature size for bounded domains in the plane endowed with an intrinsic metric. Given a point x in a domain X, the systolic feature size of X at x measures half the length of the shortest loop through x that is not null-homotopic in X. The resort to an intrinsic metric makes the systolic feature size rather insensitive to the local geometry of the domain, in contrast with its predecessors (local feature size, weak feature size, homology feature size). This reduces the number of samples required to capture the topology of X, provided that a reliable approximation to the intrinsic metric of X is available. Under sufficient sampling conditions involving the systolic feature size, we show that the geodesic Delaunay triangulation Dx(L) of a finite sampling L is homotopy equivalent to X. Under similar conditions, Dx(L) is sandwiched between the geodesic witness complex CWX(L) and a relaxed version CWX,ν(L). In the conference version of the article, we took advantage of this fact and proved that the homology of Dx(L) (and hence the one of X) can be retrieved by computing the persistent homology between CWX(L) and CWX,ν(L). Here, we investigate further and show that the homology of X can also be recovered from the persistent homology associated with inclusions of type CWX,ν(L)↪CWX,ν′(L), under some conditions on the parameters ν≤ν′. Similar results are obtained for Vietoris-Rips complexes in the intrinsic metric. The proofs draw some connections with recent advances on the front of homology inference from point cloud data, but also with several well-known concepts of Riemannian (and even metric) geometry. On the algorithmic front, we propose algorithms for estimating the systolic feature size of a bounded planar domain X, selecting a landmark set of sufficient density, and computing the homology of X using geodesic witness complexes or Rips complexes.},
journal = {ACM Trans. Algorithms},
month = sep,
articleno = {67},
numpages = {47},
keywords = {Delaunay triangulation, intrinsic metric, Alexandrov space, feature size, systole, witness complex}
}

@inproceedings{10.1145/1854229.1854238,
author = {Levett, Chris P. and Jhumka, Arshad and Anand, Sarabjot Singh},
title = {Towards Event Ordering in Digital Forensics},
year = {2010},
isbn = {9781450302869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1854229.1854238},
doi = {10.1145/1854229.1854238},
abstract = {In criminal investigation and criminal justice, investigators are usually faced with several reports, which contain a set of events of interest. Often, it is important to be able to order these events so that relevant queries can be posed on this ordering, such as "was X at location Y when the murder took place?". However, ordering of these events is very difficult, especially if very few events are anchored in time, i.e., few events are associated with an explicit time. Manual extraction of all the events of interest from these reports is tedious. On the other hand, automated extraction is inaccurate at best, in the sense that either several events that may not be important could be included. This ultimately gives a large set of events to consider, and imposing an ordering on this set can yield a large tree structure, where nodes represent an event of interest, and an edge (i, j) indicates that event i occurred before or at the same time as event j, and the root node represents a special "start" event. In this paper, we investigate two techniques for automating extraction of events, and then ordering these. We compare the efficiency of the techniques through the size of the tree structure obtained},
booktitle = {Proceedings of the 12th ACM Workshop on Multimedia and Security},
pages = {35–42},
numpages = {8},
keywords = {machine learning, ordering, timestamps, forensics},
location = {Roma, Italy},
series = {MM&amp;Sec '10}
}

@inproceedings{10.1145/1854273.1854282,
author = {Ganesan, Karthik and Jo, Jungho and Bircher, W. Lloyd and Kaseridis, Dimitris and Yu, Zhibin and John, Lizy K.},
title = {System-Level Max Power (SYMPO): A Systematic Approach for Escalating System-Level Power Consumption Using Synthetic Benchmarks},
year = {2010},
isbn = {9781450301787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1854273.1854282},
doi = {10.1145/1854273.1854282},
abstract = {To effectively design a computer system for the worst case power consumption scenario, system architects often use hand-crafted maximum power consuming benchmarks at the assembly language level. These stressmarks, also called power viruses, are very tedious to generate and require significant domain knowledge. In this paper, we propose SYMPO, an automatic SYstem level Max POwer virus generation framework, which maximizes the power consumption of the CPU and the memory system using genetic algorithm and an abstract workload generation framework. For a set of three ISAs, we show the efficacy of the power viruses generated using SYMPO by comparing the power consumption with that of MPrime torture test, which is widely used by industry to test system stability. Our results show that the usage of SYMPO results in the generation of power viruses that consume 14-41% more power compared to MPrime on SPARC ISA. The genetic algorithm achieved this result in about 70 to 90 generations in 11 to 15 hours when using a full system simulator. We also show that the power viruses generated in the Alpha ISA consume 9-24% more power compared to the previous approach of stressmark generation. We measure and provide the power consumption of these benchmarks on hardware by instrumenting a quad-core AMD Phenom II X4 system. The SYMPO power virus consumes more power compared to various industry grade power viruses on x86 hardware. We also provide a microarchitecture independent characterization of various industry standard power viruses.},
booktitle = {Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
pages = {19–28},
numpages = {10},
keywords = {thermal design point, synthetic benchmark, system-level power virus},
location = {Vienna, Austria},
series = {PACT '10}
}

@inproceedings{10.1145/1854273.1854321,
author = {Tournavitis, Georgios and Franke, Bj\"{o}rn},
title = {Semi-Automatic Extraction and Exploitation of Hierarchical Pipeline Parallelism Using Profiling Information},
year = {2010},
isbn = {9781450301787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1854273.1854321},
doi = {10.1145/1854273.1854321},
abstract = {In recent years multi-core computer systems have left the realm of high-performance computing and virtually all of today's desktop computers and embedded computing systems are equipped with several processing cores. Still, no single parallel programming model has found widespread support and parallel programming remains an art for the majority of application programmers. In addition, there exists a plethora of sequential legacy applications for which automatic parallelization is the only hope to benefit from the increased processing power of modern multi-core systems. In the past automatic parallelization largely focused on data parallelism. In this paper we present a novel approach to extracting and exploiting pipeline parallelism from sequential applications. We use profiling to overcome the limitations of static data and control flow analysis enabling more aggressive parallelization. Our approach is orthogonal to existing automatic parallelization approaches and additional data parallelism may be exploited in the individual pipeline stages. The key contribution of this paper is a whole-program representation that supports profiling, parallelism extraction and exploitation. We demonstrate how this enhances conventional pipeline parallelization by incorporating support for multi-level loops and pipeline stage replication in a uniform and automatic way. We have evaluated our methodology on a set of multimedia and stream processing benchmarks and demonstrate speedups of up to 4.7 on a eight-core Intel Xeon machine.},
booktitle = {Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
pages = {377–388},
numpages = {12},
keywords = {pipeline parallelism, streaming applications, program dependence graph, parallelization},
location = {Vienna, Austria},
series = {PACT '10}
}

@inproceedings{10.1145/1868328.1868338,
author = {Lavazza, Luigi and Robiolo, Gabriela},
title = {The Role of the Measure of Functional Complexity in Effort Estimation},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868338},
doi = {10.1145/1868328.1868338},
abstract = {Background. Currently there are several definitions of measures that should represent the size of software functional requirements. These measures have gained a quite relevant role, since they are one of the few basis upon which effort estimation can be based. However, traditional Functional Size Measures do not take into account the amount and complexity of the elaboration required, concentrating instead on the amount of data accessed or moved. This is a problem, when it comes to effort estimation, since the amount and complexity of the required data elaborations affect the implementation effort, but are not adequately represented by the current measures (including the standardized ones).Objective. The paper evaluates different types of functional size measures as effort estimators. Moreover, the consequences of taking into consideration also the amount and complexity of required elaboration in the effort estimation models are evaluated.Methods. In this paper we take into consideration a representative set of functional size measures (namely, function points, COSMIC function points and use case points) and a recently proposed elaboration complexity measure (Paths) and evaluate how well these measures are correlated with the development effort. To this end, we measured a set of 17 projects and analyzed the resulting data.Results. We found that it is possible to build statistically valid models of the development effort that use the functional size and complexity measures as independent variables. In fact, we discovered that using the measure of elaboration complexity in addition to the functional size substantially improves the precision of the fitting.Conclusions. The analysis reported here suggests that a measure of the amount and complexity of elaboration required from a software system should be used, in conjunction with traditional functional size measures, in the estimation of software development effort. Further investigations, involving a greater number of projects, are however needed to confirm these findings.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {6},
numpages = {10},
keywords = {function points, functional complexity measurement, functional size measurement, use case points, COSMIC function points, effort estimation, use case-based measurement},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/1852786.1852794,
author = {Murgia, Alessandro and Concas, Giulio and Marchesi, Michele and Tonelli, Roberto},
title = {A Machine Learning Approach for Text Categorization of Fixing-Issue Commits on CVS},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852794},
doi = {10.1145/1852786.1852794},
abstract = {We studied data mining from CVS repositories of two large OO projects, Eclipse and Netbeans, focusing on "fixing-issue" commits.We highlight common characteristics of issue reporting, and problems related to the identification of these messages, and compare static traditional approaches, like Knowledge Engineering, to dynamic approaches based on Machine Learning techniques. We compare for the first time performances of Machine Learning (ML) techniques to automatic classify "fixing-issues" among message commits. Our study calculates precision and recall of different Machine Learning Classifiers for the correct classification of issue-reporting commits. Our results show that some ML classifiers can correctly classify up to 99.9% of such commits.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {6},
numpages = {10},
keywords = {classifier, data mining, machine learning},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/1852786.1852804,
author = {Li, Jingyue and Moe, Nils B. and Dyb\r{a}, Tore},
title = {Transition from a Plan-Driven Process to Scrum: A Longitudinal Case Study on Software Quality},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852804},
doi = {10.1145/1852786.1852804},
abstract = {Although Scrum is an important topic in software engineering and information systems, few longitudinal industrial studies have investigated the effects of Scrum on software quality, in terms of defects and defect density, and the quality assurance process. In this paper we report on a longitudinal study in which we have followed a project over a three-year period. We compared software quality assurance processes and software defects of the project between a 17-month phase with a plan-driven process, followed by a 20-month phase with Scrum. The results of the study did not show a significant reduction of defect densities or changes of defect profiles after Scrum was used. However, the iterative nature of Scrum resulted in constant system and acceptance testing and related defect fixing, which made the development process more efficient in terms of fewer surprises and better control of software quality and release date. In addition, software quality and knowledge sharing got more focus when using Scrum. However, Scrum put more stress and time pressure on the developers, and made them reluctant to perform certain tasks for later maintenance, such as refactoring.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {13},
numpages = {10},
keywords = {agile software development, software quality, empirical software engineering},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/1852786.1852817,
author = {Jalali, Samireh and Gencel, Cigdem and \v{S}mite, Darja},
title = {Trust Dynamics in Global Software Engineering},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852817},
doi = {10.1145/1852786.1852817},
abstract = {Trust is one of the key factors that determines success or failure of any software project. However, achieving and maintaining trust in distributed software projects, when team members are geographically, temporally and culturally distant from each other, is a remarkable challenge. This paper explores the dynamics of trust and best practices performed in software organizations to address trust-related issues in global software engineering. Semi-structured interviews were conducted in six different distributed software development organizations and a resulting trust dynamics model is presented. Based on the findings, the paper also provides suggestions for the industry to achieve trust in distributed collaborations.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {23},
numpages = {9},
keywords = {trust building, trust, global software engineering, trust maintenance},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/1852786.1852808,
author = {Qattous, Hazem and Gray, Philip and Welland, Ray},
title = {An Empirical Study of Specification by Example in a Software Engineering Tool},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852808},
doi = {10.1145/1852786.1852808},
abstract = {Meta-CASE tools offer CASE tool specialisation by enabling a designer to specify a tool which is then generated automatically. Constraints are often used in such meta-CASE tools for governing the syntax and semantics of model elements and the values of their attributes. However, the constraint definition process is complex, time-consuming and error-prone. This paper presents an empirical study of the use of Specification by Example (SBE), based on the well-known notion of Programming by Example (PBE), as a user-computer interactive technique for such constraint specification. Two constraint specification techniques have been implemented in a meta-CASE tool a wizard that represents a conventional form-filling technique and an SBE technique that depends on the user providing one or more examples and the system inferring a list of possible intended constraints. The empirical study compared the wizard and SBE with respect to constraint definition correctness, task completion time, and user satisfaction. Two common modelling diagrams have been used, a State Transition Diagram and a Use Case Diagram. Results suggest that SBE is superior to the wizard in terms of measured criteria described above. Observations on the interaction of users with the system and opinions of participants are also presented.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {16},
numpages = {10},
keywords = {specification by example, user study, domain specific language, example-based interface, programming by example, meta-CASE tools},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.1145/1941007.1941009,
author = {Dufresne, Aude and Courtemanche, Fran\c{c}ois and Tep, Sandrine Prom},
title = {Analyse Des Interactions En Utilisant Le Suivi Oculaire, Le Suivi Physiologique et Les Structures d'actions},
year = {2010},
isbn = {9781450304108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1941007.1941009},
doi = {10.1145/1941007.1941009},
abstract = {In order to improve human-computer interaction, eye-tracking measures and physiological reactions are combined to the context of navigation and the structure of actions to analyze emotional state of users. Understanding affective reactions is essential to improve the human-computer interaction and the design of systems. In the domain of e-commerce those techniques are being used informally to observe interactions during the design of systems or for research, more systematic methods are necessary to efficiently highlight important aspects of the interaction and the emotional reactions. More so to help interpret measures it is necessary to complete measures with the recognition of structures of actions, so that reactions could be interpreted in context.},
booktitle = {Proceedings of the 22nd Conference on l'Interaction Homme-Machine},
pages = {1–8},
numpages = {8},
keywords = {physiological measures, UGC, eye-tracking, task models, machine learning, emotions, e-commerce, interaction evaluation},
location = {Luxembourg, Luxembourg},
series = {IHM '10}
}

@inproceedings{10.1145/1863482.1863489,
author = {Lobachev, Oleg and Loogen, Rita},
title = {Estimating Parallel Performance, a Skeleton-Based Approach},
year = {2010},
isbn = {9781450302548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1863482.1863489},
doi = {10.1145/1863482.1863489},
abstract = {In this paper we estimate parallel execution times, based on identifying separate "parts" of the work done by parallel programs. We assume that programs are described using algorithmic skeletons. Therefore our runtime analysis works without any source code inspection. The time of parallel program execution is expressed in terms of the sequential work and the parallel penalty. We measure these values for different problem sizes and numbers of processors and estimate them for unknown values in both dimensions. This allows us to predict parallel execution time for unknown inputs and non-available processor numbers.Another useful application of our formalism is a measure of parallel program quality. We analyse the values for parallel penalty both for growing input size and for increasing numbers of processing elements. From these data, conclusions on parallel performance and scalability are drawn.},
booktitle = {Proceedings of the Fourth International Workshop on High-Level Parallel Programming and Applications},
pages = {25–34},
numpages = {10},
keywords = {algorithmic skeletons, forecasting, parallel runtime, runtime estimation, polynomial regression, serial fraction, scalability measure, amdahl's law},
location = {Baltimore, Maryland, USA},
series = {HLPP '10}
}

@inproceedings{10.1145/1864349.1864394,
author = {Madan, Anmol and Cebrian, Manuel and Lazer, David and Pentland, Alex},
title = {Social Sensing for Epidemiological Behavior Change},
year = {2010},
isbn = {9781605588438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864349.1864394},
doi = {10.1145/1864349.1864394},
abstract = {An important question in behavioral epidemiology and public health is to understand how individual behavior is affected by illness and stress. Although changes in individual behavior are intertwined with contagion, epidemiologists today do not have sensing or modeling tools to quantitatively measure its effects in real-world conditions. In this paper, we propose a novel application of ubiquitous computing. We use mobile phone based co-location and communication sensing to measure characteristic behavior changes in symptomatic individuals, reflected in their total communication, interactions with respect to time of day (e.g., late night, early morning), diversity and entropy of face-to-face interactions and movement. Using these extracted mobile features, it is possible to predict the health status of an individual, without having actual health measurements from the subject. Finally, we estimate the temporal information flux and implied causality between physical symptoms, behavior and mental health.},
booktitle = {Proceedings of the 12th ACM International Conference on Ubiquitous Computing},
pages = {291–300},
numpages = {10},
keywords = {mobile sensing, social computing, spatial epidemiology},
location = {Copenhagen, Denmark},
series = {UbiComp '10}
}

@inproceedings{10.1145/1864349.1864393,
author = {Rachuri, Kiran K. and Musolesi, Mirco and Mascolo, Cecilia and Rentfrow, Peter J. and Longworth, Chris and Aucinas, Andrius},
title = {EmotionSense: A Mobile Phones Based Adaptive Platform for Experimental Social Psychology Research},
year = {2010},
isbn = {9781605588438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864349.1864393},
doi = {10.1145/1864349.1864393},
abstract = {Today's mobile phones represent a rich and powerful computing platform, given their sensing, processing and communication capabilities. Phones are also part of the everyday life of billions of people, and therefore represent an exceptionally suitable tool for conducting social and psychological experiments in an unobtrusive way.de the ability of sensing individual emotions as well as activities, verbal and proximity interactions among members of social groups. Moreover, the system is programmable by means of a declarative language that can be used to express adaptive rules to improve power saving. We evaluate a system prototype on Nokia Symbian phones by means of several small-scale experiments aimed at testing performance in terms of accuracy and power consumption. Finally, we present the results of real deployment where we study participants emotions and interactions. We cross-validate our measurements with the results obtained through questionnaires filled by the users, and the results presented in social psychological studies using traditional methods. In particular, we show how speakers and participants' emotions can be automatically detected by means of classifiers running locally on off-the-shelf mobile phones, and how speaking and interactions can be correlated with activity and location measures.},
booktitle = {Proceedings of the 12th ACM International Conference on Ubiquitous Computing},
pages = {281–290},
numpages = {10},
keywords = {speaker recognition, energy efficiency, mobile phones, social psychology, emotion recognition},
location = {Copenhagen, Denmark},
series = {UbiComp '10}
}

@inproceedings{10.1145/1864349.1864376,
author = {Dillahunt, Tawanna and Mankoff, Jennifer and Paulos, Eric},
title = {Understanding Conflict between Landlords and Tenants: Implications for Energy Sensing and Feedback},
year = {2010},
isbn = {9781605588438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864349.1864376},
doi = {10.1145/1864349.1864376},
abstract = {Energy use in the home is a topic of increasing interest and concern, and one on which technology can have a significant impact. However, existing work typically focuses on moderately affluent homeowners who have relative autonomy with respect to their home, or does not address socio-economic status, class, and other related issues. For the 30% of the U.S. population who rent their homes, many key decisions regarding energy use must be negotiated with a landlord. Because energy use impacts the bottom line of both landlords and tenants, this can be a source of conflict in the landlord/tenant relationship. Ubicomp technologies for reducing energy use in rental units must engage with landlord/tenant conflicts to be successful. Unfortunately, little detailed knowledge is available about the impact of landlord/tenant conflicts on energy use. We present an analysis of a series of qualitative studies with landlords and tenants. We argue that a consideration of multiple stakeholders, and the power imbalances among them, will drive important new research questions and lead to more widely applicable solutions. The main contribution of our work is a set of open research questions and design recommendations for technologies that may affect and be affected by the conflict between stakeholders around energy use.},
booktitle = {Proceedings of the 12th ACM International Conference on Ubiquitous Computing},
pages = {149–158},
numpages = {10},
keywords = {energy, sustainability, inequality, domestic computing},
location = {Copenhagen, Denmark},
series = {UbiComp '10}
}

@inproceedings{10.1145/1864349.1864359,
author = {Tentori, Monica and Hayes, Gillian R.},
title = {Designing for Interaction Immediacy to Enhance Social Skills of Children with Autism},
year = {2010},
isbn = {9781605588438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864349.1864359},
doi = {10.1145/1864349.1864359},
abstract = {Children with Autism Spectrum Disorder often require therapeutic interventions to support engagement in effective social interactions. In this paper, we present the results of a study conducted in three public schools that use an educational and behavioral intervention for the instruction of social skills in changing situational contexts. The results of this study led to the concept of interaction immediacy to help children maintain appropriate spatial boundaries, reply to conversation initiators, disengage appropriately at the end of an interaction, and identify potential communication partners. We describe design principles for Ubicomp technologies to support interaction immediacy and present an example design. The contribution of this work is twofold. First, we present an understanding of social skills in mobile and dynamic contexts. Second, we introduce the concept of interaction immediacy and show its effectiveness as a guiding principle for the design of Ubicomp applications.},
booktitle = {Proceedings of the 12th ACM International Conference on Ubiquitous Computing},
pages = {51–60},
numpages = {10},
keywords = {autism, social skills, interaction immediacy, social compass},
location = {Copenhagen, Denmark},
series = {UbiComp '10}
}

@inproceedings{10.1145/1864349.1864358,
author = {Whittle, Jon and Simm, William and Ferrario, Maria-Angela and Frankova, Katerina and Garton, Laurence and Woodcock, Andr\'{e}e and Nasa, Baseerit and Binner, Jane and Ariyatum, Aom},
title = {VoiceYourView: Collecting Real-Time Feedback on the Design of Public Spaces},
year = {2010},
isbn = {9781605588438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864349.1864358},
doi = {10.1145/1864349.1864358},
abstract = {This paper reports on VoiceYourView, a kind of intelligent kiosk, which uses speech recognition and natural language processing to gather the public's creative input on the public space designs. Over a six week period, VoiceYourView was deployed in a public space and 2000 design critiques were collected from 600 people. The paper shows that people are capable of providing creative input on their environment using unstructured speech or text and that a good proportion of these comments are actionable. The paper also investigates the use of public displays to auto-summarize comments left by the public so far. Although there is anecdotal evidence that this encourages participation, an experiment found that filtering comments (e.g., to display only positive responses) had no effect on what people had to say.},
booktitle = {Proceedings of the 12th ACM International Conference on Ubiquitous Computing},
pages = {41–50},
numpages = {10},
keywords = {civic engagement, intelligent kiosk, public reporting},
location = {Copenhagen, Denmark},
series = {UbiComp '10}
}

@inproceedings{10.1145/1863543.1863576,
author = {Crestani, Marcus and Sperber, Michael},
title = {Experience Report: Growing Programming Languages for Beginning Students},
year = {2010},
isbn = {9781605587943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1863543.1863576},
doi = {10.1145/1863543.1863576},
abstract = {A student learning how to program learns best when the programming language and programming environment cater to her specific needs. These needs are different from the requirements of a professional programmer. Consequently, the design of teaching languages poses challenges different from the design of professional languages. Using a functional language by itself gives advantages over more popular, professional languages, but fully exploiting these advantages requires careful adaptation to the needs of the students' as-is, these languages do not support the students nearly as well as they could. This paper describes our experience adopting the didactic approach of How to Design Programs, focussing on the design process for our own set of teaching languages. We have observed students as they try to program as part of our introductory course, and used these observations to significantly improve the design of these languages. This paper describes the changes we have made, and the journey we took to get there.},
booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
pages = {229–234},
numpages = {6},
keywords = {introductory programming},
location = {Baltimore, Maryland, USA},
series = {ICFP '10}
}

@article{10.1145/1932681.1863576,
author = {Crestani, Marcus and Sperber, Michael},
title = {Experience Report: Growing Programming Languages for Beginning Students},
year = {2010},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/1932681.1863576},
doi = {10.1145/1932681.1863576},
abstract = {A student learning how to program learns best when the programming language and programming environment cater to her specific needs. These needs are different from the requirements of a professional programmer. Consequently, the design of teaching languages poses challenges different from the design of professional languages. Using a functional language by itself gives advantages over more popular, professional languages, but fully exploiting these advantages requires careful adaptation to the needs of the students' as-is, these languages do not support the students nearly as well as they could. This paper describes our experience adopting the didactic approach of How to Design Programs, focussing on the design process for our own set of teaching languages. We have observed students as they try to program as part of our introductory course, and used these observations to significantly improve the design of these languages. This paper describes the changes we have made, and the journey we took to get there.},
journal = {SIGPLAN Not.},
month = sep,
pages = {229–234},
numpages = {6},
keywords = {introductory programming}
}

@inproceedings{10.1145/2377576.2377580,
author = {Donmez, Birsen and Cummings, M. L.},
title = {Metric Selection for Evaluating Human Supervisory Control of Unmanned Vehicles},
year = {2010},
isbn = {9781450302906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377576.2377580},
doi = {10.1145/2377576.2377580},
abstract = {Broad metric classes were proposed in the literature in order to facilitate metric selection for evaluating human-autonomous vehicle interaction. However, there still lacks a systematic method for selecting an efficient set of metrics from the many metrics available. We previously identified a list of evaluation criteria that can help determine the quality of a metric, and generated a list of potential metric costs and benefits. Depending on research objectives and limitations, these costs and benefits can have different weights of importance. Through an experiment with subject matter experts, we investigated which metric characteristics human factors practitioners consider to be important in evaluating human supervisory control of unmanned vehicles. We also tested two different multi-criteria decision making methods to help practitioners assign subjective weights to the cost/benefit criteria. The majority of participants rated the evaluation criteria used in both tools as very useful. However, the majority of participants' metric selections before using the methods were the same as the suggestions provided by the methods. Since determining weights of metric importance is an inherently subjective process, even with objective computational tools, the real value of using such a tool may be reminding human factors practitioners of the important experimental criteria and relationships between these criteria that should be considered when designing an experiment.},
booktitle = {Proceedings of the 10th Performance Metrics for Intelligent Systems Workshop},
pages = {14–21},
numpages = {8},
keywords = {AHP, analytic hierarchy process, human supervisory control, experiments, metrics, metric quality},
location = {Baltimore, Maryland},
series = {PerMIS '10}
}

@inproceedings{10.1145/1869652.1869660,
author = {D\'{\i}ez, Fernando and Chavarriaga, J. Enrique and Campos, Pedro G. and Bellog\'{\i}n, Alejandro},
title = {Movie Recommendations Based in Explicit and Implicit Features Extracted from the <i>Filmtipset</i> Dataset},
year = {2010},
isbn = {9781450302586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869652.1869660},
doi = {10.1145/1869652.1869660},
abstract = {In this paper, we describe the experiments conducted by the Information Retrieval Group at the Universidad Aut\'{o}noma de Madrid (Spain) in order to better recommend movies for the 2010 CAMRa Challenge edition. Experiments were carried out on the dataset corresponding to social Filmtipset track. To obtain the movies recommendations we have used different algorithms based on Random Walks, which are well documented in the literature of collaborative recommendation. We have also included a new proposal in one of the algorithms in order to get better results. The results obtained have been computed by means of the trec_eval standard NIST evaluation procedure.},
booktitle = {Proceedings of the Workshop on Context-Aware Movie Recommendation},
pages = {45–52},
numpages = {8},
keywords = {recommender systems, challenge, movie recommendations, social networks, random walk},
location = {Barcelona, Spain},
series = {CAMRa '10}
}

@inproceedings{10.1145/1863509.1863515,
author = {Fritchie, Scott Lystig},
title = {Chain Replication in Theory and in Practice},
year = {2010},
isbn = {9781450302531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1863509.1863515},
doi = {10.1145/1863509.1863515},
abstract = {When implementing a distributed storage system, using an algorithm with a formal definition and proof is a wise idea. However, translating any algorithm into effective code can be difficult because the implementation must be both correct and fast.This paper is a case study of the implementation of the chain replication protocol in a distributed key-value store called Hibari. In theory, the chain replication algorithm is quite simple and should be straightforward to implement correctly. In practice, however, there were many implementation details that had effects both profound and subtle. The Erlang community, as well as distributed systems implementors in general, can use the lessons learned with Hibari (specifically in areas of performance enhancements and failure detection) to avoid many dangers that lurk at the interface between theory and real-world computing.},
booktitle = {Proceedings of the 9th ACM SIGPLAN Workshop on Erlang},
pages = {33–44},
numpages = {12},
keywords = {erlang, key-value store, chain replication, hibari},
location = {Baltimore, Maryland, USA},
series = {Erlang '10}
}

@inproceedings{10.1145/1940941.1940975,
author = {Lamis, Trevor},
title = {A Forensic Approach to Incident Response},
year = {2010},
isbn = {9781450302029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1940941.1940975},
doi = {10.1145/1940941.1940975},
abstract = {An incident response plan is critical for the detection and removal of information security threats. Incident response involves many aspects other than technical issues. There are management, legal, and social issues that an incident response team needs to consider. An incident response identifies, contains, and eliminates the incident. Then, the compromised system is fully recovered and restored. To hold the intruder accountable, a forensic investigation is needed. Documentation of all activities and evidence gathering is crucial when during the entire response and investigation. The paper proposes and discusses interconnected methodological frameworks for both incident response and network forensics.},
booktitle = {2010 Information Security Curriculum Development Conference},
pages = {177–185},
numpages = {9},
keywords = {network traces, evidence, incident response, network forensics},
location = {Kennesaw, Georgia},
series = {InfoSecCD '10}
}

@inproceedings{10.1145/1866029.1866062,
author = {Mueller, Florian and Vetere, Frank and Gibbs, Martin R. and Edge, Darren and Agamanolis, Stefan and Sheridan, Jennifer G.},
title = {Jogging over a Distance between Europe and Australia},
year = {2010},
isbn = {9781450302715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866029.1866062},
doi = {10.1145/1866029.1866062},
abstract = {Exertion activities, such as jogging, require users to invest intense physical effort and are associated with physical and social health benefits. Despite the benefits, our understanding of exertion activities is limited, especially when it comes to social experiences. In order to begin understanding how to design for technologically augmented social exertion experiences, we present "Jogging over a Distance", a system in which spatialized audio based on heart rate allowed runners as far apart as Europe and Australia to run together. Our analysis revealed how certain aspects of the design facilitated a social experience, and consequently we describe a framework for designing augmented exertion activities. We make recommendations as to how designers could use this framework to aid the development of future social systems that aim to utilize the benefits of exertion.},
booktitle = {Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology},
pages = {189–198},
numpages = {10},
keywords = {whole-body interaction, sports, physiological data, audio, mobile phone, exergaming, exergame, running, heart rate, exertion interface, spatialization},
location = {New York, New York, USA},
series = {UIST '10}
}

@inproceedings{10.1145/1866307.1866346,
author = {Corrigan-Gibbs, Henry and Ford, Bryan},
title = {Dissent: Accountable Anonymous Group Messaging},
year = {2010},
isbn = {9781450302456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866307.1866346},
doi = {10.1145/1866307.1866346},
abstract = {Users often wish to participate in online groups anonymously, but misbehaving users may abuse this anonymity to disrupt the group's communication. Existing messaging protocols such as DC-nets leave groups vulnerable to denial-of-service and Sybil attacks, Mix-nets are difficult to protect against traffic analysis, and accountable voting protocols are unsuited to general anonymous messaging.We present the first general messaging protocol that offers provable anonymity with accountability for moderate-size groups, and efficiently handles unbalanced loads where few members wish to transmit in a given round. The N group members first cooperatively shuffle an N x N matrix of pseudorandom seeds, then use these seeds in N "pre-planned" DC-nets protocol runs. Each DC-nets run transmits the variable-length bulk data comprising one member's message, using the minimum number of bits required for anonymity under our attack model. The protocol preserves message integrity and one-to-one correspondence between members and messages, makes denial-of-service attacks by members traceable to the culprit, and efficiently handles large, unbalanced message loads. A working prototype demonstrates the protocol's practicality for anonymous messaging in groups of 40+ members.},
booktitle = {Proceedings of the 17th ACM Conference on Computer and Communications Security},
pages = {340–350},
numpages = {11},
keywords = {group communication, peer-to-peer networks, anonymity, denial of service, accountability, verifiable anonymous shuffle},
location = {Chicago, Illinois, USA},
series = {CCS '10}
}

@inproceedings{10.1145/1866919.1866923,
author = {Asuncion, Arthur U. and Goodrich, Michael T.},
title = {Turning Privacy Leaks into Floods: Surreptitious Discovery of Social Network Friendships and Other Sensitive Binary Attribute Vectors},
year = {2010},
isbn = {9781450300964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866919.1866923},
doi = {10.1145/1866919.1866923},
abstract = {We study methods for attacking the privacy of social networking sites, collaborative filtering sites, databases of genetic signatures, and other data sets that can be represented as vectors of binary relationships. Our methods are based on reductions to nonadaptive group testing, which implies that our methods can exploit a minimal amount of privacy leakage, such as contained in a single bit that indicates if two people in a social network have a friend in common or not. We analyze our methods for turning such privacy leaks into floods using theoretical characterizations as well as experimental tests. Our empirical analyses are based on experiments involving privacy attacks on the social networking sites Facebook and LiveJournal, a database of mitochondrial DNA, a power grid network, and the movie-rating database released as a part of the Netflix Prize contest. For instance, with respect to Facebook, our analysis shows that it is effectively possible to break the privacy of members who restrict their friends lists to friends-of-friends.},
booktitle = {Proceedings of the 9th Annual ACM Workshop on Privacy in the Electronic Society},
pages = {21–30},
numpages = {10},
keywords = {privacy leaks, genetic signatures, combinatorial group testing, binary attribute vectors, social networks},
location = {Chicago, Illinois, USA},
series = {WPES '10}
}

