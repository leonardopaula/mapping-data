@inproceedings{10.1145/2568225.2568259,
author = {Chen, Tse-Hsun and Shang, Weiyi and Jiang, Zhen Ming and Hassan, Ahmed E. and Nasser, Mohamed and Flora, Parminder},
title = {Detecting Performance Anti-Patterns for Applications Developed Using Object-Relational Mapping},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568259},
doi = {10.1145/2568225.2568259},
abstract = { Object-Relational Mapping (ORM) provides developers a conceptual abstraction for mapping the application code to the underlying databases. ORM is widely used in industry due to its convenience; permitting developers to focus on developing the business logic without worrying too much about the database access details. However, developers often write ORM code without considering the impact of such code on database performance, leading to cause transactions with timeouts or hangs in large-scale systems. Unfortunately, there is little support to help developers automatically detect suboptimal database accesses. In this paper, we propose an automated framework to detect ORM performance anti-patterns. Our framework automatically flags performance anti-patterns in the source code. Furthermore, as there could be hundreds or even thousands of instances of anti-patterns, our framework provides sup- port to prioritize performance bug fixes based on a statistically rigorous performance assessment. We have successfully evaluated our framework on two open source and one large-scale industrial systems. Our case studies show that our framework can detect new and known real-world performance bugs and that fixing the detected performance anti- patterns can improve the system response time by up to 98%. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {1001–1012},
numpages = {12},
keywords = {Static Analysis, Performance, Dynamic Analysis, Performance Evaluation, Performance Anti-pattern},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568240,
author = {Thomas, Keerthi and Bandara, Arosha K. and Price, Blaine A. and Nuseibeh, Bashar},
title = {Distilling Privacy Requirements for Mobile Applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568240},
doi = {10.1145/2568225.2568240},
abstract = { As mobile computing applications have become commonplace, it is increasingly important for them to address end-users’ privacy requirements. Privacy requirements depend on a number of contextual socio-cultural factors to which mobility adds another level of contextual variation. However, traditional requirements elicitation methods do not sufficiently account for contextual factors and therefore cannot be used effectively to represent and analyse the privacy requirements of mobile end users. On the other hand, methods that do investigate contextual factors tend to produce data that does not lend itself to the process of requirements extraction. To address this problem we have developed a Privacy Requirements Distillation approach that employs a problem analysis framework to extract and refine privacy requirements for mobile applications from raw data gathered through empirical studies involving end users. Our approach introduces privacy facets that capture patterns of privacy concerns which are matched against the raw data. We demonstrate and evaluate our approach using qualitative data from an empirical study of a mobile social networking application. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {871–882},
numpages = {12},
keywords = {privacy, mobile, requirements engineering},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568244,
author = {Sheth, Swapneel and Kaiser, Gail and Maalej, Walid},
title = {Us and Them: A Study of Privacy Requirements across North America, Asia, and Europe},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568244},
doi = {10.1145/2568225.2568244},
abstract = { Data privacy when using online systems like Facebook and Amazon has become an increasingly popular topic in the last few years. However, only a little is known about how users and developers perceive privacy and which concrete measures would mitigate their privacy concerns. To investigate privacy requirements, we conducted an online survey with closed and open questions and collected 408 valid responses. Our results show that users often reduce privacy to security, with data sharing and data breaches being their biggest concerns. Users are more concerned about the content of their documents and their personal data such as location than about their interaction data. Unlike users, developers clearly prefer technical measures like data anonymization and think that privacy laws and policies are less effective. We also observed interesting differences between people from different geographies. For example, people from Europe are more concerned about data breaches than people from North America. People from Asia/Pacific and Europe believe that content and metadata are more critical for privacy than people from North America. Our results contribute to developing a user-driven privacy framework that is based on empirical evidence in addition to the legal, technical, and commercial perspectives. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {859–870},
numpages = {12},
keywords = {user developer collaboration, interaction data, Human factors in software engineering, privacy, requirements engineering, empirical studies},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568276,
author = {Gorla, Alessandra and Tavecchia, Ilaria and Gross, Florian and Zeller, Andreas},
title = {Checking App Behavior against App Descriptions},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568276},
doi = {10.1145/2568225.2568276},
abstract = { How do we know a program does what it claims to do? After clustering Android apps by their description topics, we identify outliers in each cluster with respect to their API usage. A "weather" app that sends messages thus becomes an anomaly; likewise, a "messaging" app would typically not be expected to access the current location. Applied on a set of 22,500+ Android applications, our CHABADA prototype identified several anomalies; additionally, it flagged 56% of novel malware as such, without requiring any known malware patterns. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {1025–1035},
numpages = {11},
keywords = {description analysis, malware detection, Android, clustering},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568279,
author = {Wagstrom, Patrick and Datta, Subhajit},
title = {Does Latitude Hurt While Longitude Kills? Geographical and Temporal Separation in a Large Scale Software Development Project},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568279},
doi = {10.1145/2568225.2568279},
abstract = { Distributed software development allows firms to leverage cost advantages and place work near centers of competency. This distribution comes at a cost -- distributed teams face challenges from differing cultures, skill levels, and a lack of shared working hours. In this paper we examine whether and how geographic and temporal separation in a large scale distributed software development influences developer interactions. We mine the work item trackers for a large commercial software project with a globally distributed development team. We examine both the time to respond and the propensity of individuals to respond and find that when taken together, geographic distance has little effect, while temporal separation has a significant negative impact on the time to respond. However, both have little impact on the social network of individuals in the organization. These results suggest that while temporally distributed teams do communicate, it is at a slower rate, and firms may wish to locate partner teams in similar time zones for maximal performance. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {199–210},
numpages = {12},
keywords = {Exponential Random Graph Models, Agile Development, Social Network Analysis, Distributed Software Development, Collaboration},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568226,
author = {Murphy-Hill, Emerson and Zimmermann, Thomas and Nagappan, Nachiappan},
title = {Cowboys, Ankle Sprains, and Keepers of Quality: How is Video Game Development Different from Software Development?},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568226},
doi = {10.1145/2568225.2568226},
abstract = { Video games make up an important part of the software industry, yet the software engineering community rarely studies video games. This imbalance is a problem if video game development differs from general software development, as some game experts suggest. In this paper we describe a study with 14 interviewees and 364 survey respondents. The study elicited substantial differences between video game development and other software development. For example, in game development, “cowboy coders” are necessary to cope with the continuous interplay between creative desires and technical constraints. Consequently, game developers are hesitant to use automated testing because of these tests’ rapid obsolescence in the face of shifting creative desires of game designers. These differences between game and non-game development have implications for research, industry, and practice. For instance, as a starting point for impacting game development, researchers could create testing tools that enable game developers to create tests that assert flexible behavior with little up-front investment. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {1–11},
numpages = {11},
keywords = {practices, Software engineering, games},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568245,
author = {M\"{a}ntyl\"{a}, Mika V. and Petersen, Kai and Lehtinen, Timo O. A. and Lassenius, Casper},
title = {Time Pressure: A Controlled Experiment of Test Case Development and Requirements Review},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568245},
doi = {10.1145/2568225.2568245},
abstract = { Time pressure is prevalent in the software industry in which shorter and shorter deadlines and high customer demands lead to increasingly tight deadlines. However, the effects of time pressure have received little attention in software engineering research. We performed a controlled experiment on time pressure with 97 observations from 54 subjects. Using a two-by-two crossover design, our subjects performed requirements review and test case development tasks. We found statistically significant evidence that time pressure increases efficiency in test case development (high effect size Cohen’s d=1.279) and in requirements review (medium effect size Cohen’s d=0.650). However, we found no statistically significant evidence that time pressure would decrease effectiveness or cause adverse effects on motivation, frustration or perceived performance. We also investigated the role of knowledge but found no evidence of the mediating role of knowledge in time pressure as suggested by prior work, possibly due to our subjects. We conclude that applying moderate time pressure for limited periods could be used to increase efficiency in software engineering tasks that are well structured and straight forward. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {83–94},
numpages = {12},
keywords = {Test case development, Review, Time pressure, Experiment},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@article{10.1145/2627729,
author = {Manitsaris, S. and Glushkova, A. and Bevilacqua, F. and Moutarde, F.},
title = {Capture, Modeling, and Recognition of Expert Technical Gestures in Wheel-Throwing Art of Pottery},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/2627729},
doi = {10.1145/2627729},
abstract = {This research has been conducted in the context of the ArtiMuse project that aims at the modeling and renewal of rare gestural knowledge and skills involved in the traditional craftsmanship and more precisely in the art of wheel-throwing pottery. These knowledge and skills constitute intangible cultural heritage and refer to the fruit of diverse expertise founded and propagated over the centuries thanks to the ingeniousness of the gesture and the creativity of the human spirit. Nowadays, this expertise is very often threatened with disappearance because of the difficulty to resist globalization and the fact that most of those “expertise holders” are not easily accessible due to geographical or other constraints. In this article, a methodological framework for capturing and modeling gestural knowledge and skills in wheel-throwing pottery is proposed. It is based on capturing gestures using wireless inertial sensors and statistical modeling. In particular, we used a system that allows for online alignment of gestures using a modified Hidden Markov Model. This methodology is implemented into a human--computer interface, which permits both the modeling and recognition of expert technical gestures. This system could be used to assist in the learning of these gestures by giving continuous feedback in real time by measuring the difference between expert and learner gestures. The system has been tested and evaluated on different potters with rare expertise, which is strongly related to their local identity.},
journal = {J. Comput. Cult. Herit.},
month = jun,
articleno = {10},
numpages = {15},
keywords = {HCI, Perception, inertial sensors, machine learning}
}

@article{10.1145/2632217,
author = {Gracia, Dar\'{\i}o Su\'{a}rez and Ferrer\'{o}n, Alexandra and Campo, Luis Montesano Del and Arnal, Teresa Monreal and Y\'{u}fera, V\'{\i}ctor Vi\~{n}als},
title = {Revisiting LP-NUCA Energy Consumption: Cache Access Policies and Adaptive Block Dropping},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/2632217},
doi = {10.1145/2632217},
abstract = {Cache working-set adaptation is key as embedded systems move to multiprocessor and Simultaneous Multithreaded Architectures (SMT) because interthread pollution harms system performance and battery life. Light-Power NUCA (LP-NUCA) is a working-set adaptive cache that depends on temporal-locality to save energy. This work identifies the sources of energy waste in LP-NUCAs: parallel access to the tag and data arrays of the tiles and low locality phases with useless block migration. To counteract both issues, we prove that switching to serial access reduces energy without harming performance and propose a machine learning Adaptive Drop Rate (ADR) controller that minimizes the amount of replacement and migration when locality is low.This work demonstrates that these techniques efficiently adapt the cache drop and access policies to save energy. They reduce LP-NUCA consumption 22.7% for 1SMT. With interthread cache contention in 2SMT, the savings rise to 29%. Versus a conventional organization, energy--delay improves 20.8% and 25% for 1- and 2SMT benchmarks, and, in 65% of the 2SMT mixes, gains are larger than 20%.},
journal = {ACM Trans. Archit. Code Optim.},
month = jun,
articleno = {19},
numpages = {26},
keywords = {embedded processors, energy, hill climbing, NUCA, locality of reference}
}

@article{10.1145/2622615,
author = {Castellano, Ginevra and Leite, Iolanda and Pereira, Andr\'{e} and Martinho, Carlos and Paiva, Ana and Mcowan, Peter W.},
title = {Context-Sensitive Affect Recognition for a Robotic Game Companion},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2622615},
doi = {10.1145/2622615},
abstract = {Social perception abilities are among the most important skills necessary for robots to engage humans in natural forms of interaction. Affect-sensitive robots are more likely to be able to establish and maintain believable interactions over extended periods of time. Nevertheless, the integration of affect recognition frameworks in real-time human-robot interaction scenarios is still underexplored. In this article, we propose and evaluate a context-sensitive affect recognition framework for a robotic game companion for children. The robot can automatically detect affective states experienced by children in an interactive chess game scenario. The affect recognition framework is based on the automatic extraction of task features and social interaction-based features. Vision-based indicators of the children’s nonverbal behaviour are merged with contextual features related to the game and the interaction and given as input to support vector machines to create a context-sensitive multimodal system for affect recognition. The affect recognition framework is fully integrated in an architecture for adaptive human-robot interaction. Experimental evaluation showed that children’s affect can be successfully predicted using a combination of behavioural and contextual data related to the game and the interaction with the robot. It was found that contextual data alone can be used to successfully predict a subset of affective dimensions, such as interest toward the robot. Experiments also showed that engagement with the robot can be predicted using information about the user’s valence, interest and anticipatory behaviour. These results provide evidence that social engagement can be modelled as a state consisting of affect and attention components in the context of the interaction.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {10},
numpages = {25},
keywords = {multimodal video corpus, human-robot interaction, robot companions, Affect recognition, contextual information}
}

@article{10.14778/2732951.2732956,
author = {Zhang, Hao and Tudor, Bogdan Marius and Chen, Gang and Ooi, Beng Chin},
title = {Efficient In-Memory Data Management: An Analysis},
year = {2014},
issue_date = {June 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732951.2732956},
doi = {10.14778/2732951.2732956},
abstract = {This paper analyzes the performance of three systems for in-memory data management: Memcached, Redis and the Resilient Distributed Datasets (RDD) implemented by Spark. By performing a thorough performance analysis of both analytics operations and fine-grained object operations such as set/get, we show that neither system handles efficiently both types of workloads. For Memcached and Redis the CPU and I/O performance of the TCP stack are the bottlenecks -- even when serving in-memory objects within a single server node. RDD does not support efficient get operation for random objects, due to a large startup cost of the get job. Our analysis reveals a set of features that a system must support in order to achieve efficient in-memory data management.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {833–836},
numpages = {4}
}

@inproceedings{10.1145/2593069.2596688,
author = {Sarma, Santanu and Venkatasubramanian, Nalini and Dutt, Nikil},
title = {Sense-Making from Distributed and Mobile Sensing Data: A Middleware Perspective},
year = {2014},
isbn = {9781450327305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593069.2596688},
doi = {10.1145/2593069.2596688},
abstract = {This paper presents a scalable and collaborative mobile crowdsensing framework for efficient collective understanding of users, contexts, and their environments. Collaborative mobile crowdsensing enables information to be gathered and shared by users who are directly involved (participatory sensing) or integrated seamlessly as needed (opportunistic sensing) through user mobile platforms. To address the scalability needs of the mobile ecosystem, we additionally employ compressive sensing techniques for approximate gathering and processing of sensor data - this requires new mechanisms for sensor data collection, tunable approximate processing, and mobile networking architecture, to create a compressive collaborative mobile crowdsensing platform called SenseDroid. The proposed framework is build using a multi-tired hierarchical architecture to sense spatial variations of a parameter of interest, perceive spatio-temporal fields, and enable energy efficient local mobile sensing with a small number of measurements. This approximate, yet tunable approach combines different sensing approaches opportunistically while trading scalability (and coverage) for data accuracy (and energy efficiency). In this paper we propose and discuss the framework and the challenges associated with compressive and collaborative mobile sensing for multi-tired hierarchical mobile network architecture for emerging mobile collaborative applications.},
booktitle = {Proceedings of the 51st Annual Design Automation Conference},
pages = {1–6},
numpages = {6},
keywords = {Participatory Sensing, Compressive Sensing, Mobile Phone Sensing, Collaborative Sensing, Sensor Network},
location = {San Francisco, CA, USA},
series = {DAC '14}
}

@inproceedings{10.1145/2593069.2593199,
author = {Das, Anup and Shafik, Rishad A. and Merrett, Geoff V. and Al-Hashimi, Bashir M. and Kumar, Akash and Veeravalli, Bharadwaj},
title = {Reinforcement Learning-Based Inter- and Intra-Application Thermal Optimization for Lifetime Improvement of Multicore Systems},
year = {2014},
isbn = {9781450327305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593069.2593199},
doi = {10.1145/2593069.2593199},
abstract = {The thermal profile of multicore systems vary both within an application's execution (intra) and also when the system switches from one application to another (inter). In this paper, we propose an adaptive thermal management approach to improve the lifetime reliability of multicore systems by considering both inter- and intra-application thermal variations. Fundamental to this approach is a reinforcement learning algorithm, which learns the relationship between the mapping of threads to cores, the frequency of a core and its temperature (sampled from on-board thermal sensors). Action is provided by overriding the operating system's mapping decisions using affinity masks and dynamically changing CPU frequency using in-kernel governors. Lifetime improvement is achieved by controlling not only the peak and average temperatures but also thermal cycling, which is an emerging wear-out concern in modern systems. The proposed approach is validated experimentally using an Intel quad-core platform executing a diverse set of multimedia benchmarks. Results demonstrate that the proposed approach minimizes average temperature, peak temperature and thermal cycling, improving the mean-time-to-failure (MTTF) by an average of 2x for intra-application and 3x for inter-application scenarios when compared to existing thermal management techniques. Furthermore, the dynamic and static energy consumption are also reduced by an average 10% and 11% respectively.},
booktitle = {Proceedings of the 51st Annual Design Automation Conference},
pages = {1–6},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '14}
}

@article{10.1145/2656864.2656866,
author = {Diegues, Nuno and Orazov, Muhammet and Paiva, Jo\~{a}o and Rodrigues, Lu\'{\i}s and Romano, Paolo},
title = {Optimizing Hyperspace Hashing via Analytical Modelling and Adaptation},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2656864.2656866},
doi = {10.1145/2656864.2656866},
abstract = {Hyperspace hashing is a recent multi-dimensional indexing technique for distributed key-value stores that aims at supporting efficient queries using multiple objects' attributes. However, the advantage of supporting complex queries comes at the cost of a complex configuration. In this paper we address the problem of automating the configuration of this innovative distributed indexing mechanism. We first show that a misconfiguration may significantly affect the performance of the system. We then derive a performance model that provides key insights on the behaviour of hyperspace hashing. Based on this model, we derive a technique to automatically and dynamically select the best configuration.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jun,
pages = {23–35},
numpages = {13},
keywords = {autonomic configuration, key-value store, multi-dimensional, analytical modelling, HyperDex}
}

@article{10.1145/2512208,
author = {Jawaheer, Gawesh and Weller, Peter and Kostkova, Patty},
title = {Modeling User Preferences in Recommender Systems: A Classification Framework for Explicit and Implicit User Feedback},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2512208},
doi = {10.1145/2512208},
abstract = {Recommender systems are firmly established as a standard technology for assisting users with their choices; however, little attention has been paid to the application of the user model in recommender systems, particularly the variability and noise that are an intrinsic part of human behavior and activity. To enable recommender systems to suggest items that are useful to a particular user, it can be essential to understand the user and his or her interactions with the system. These interactions typically manifest themselves as explicit and implicit user feedback that provides the key indicators for modeling users’ preferences for items and essential information for personalizing recommendations. In this article, we propose a classification framework for the use of explicit and implicit user feedback in recommender systems based on a set of distinct properties that include Cognitive Effort, User Model, Scale of Measurement, and Domain Relevance. We develop a set of comparison criteria for explicit and implicit user feedback to emphasize the key properties. Using our framework, we provide a classification of recommender systems that have addressed questions about user feedback, and we review state-of-the-art techniques to improve such user feedback and thereby improve the performance of the recommender system. Finally, we formulate challenges for future research on improvement of user feedback.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {8},
numpages = {26},
keywords = {implicit feedback, improvement of feedback, explicit feedback, Feedback, recommender systems}
}

@article{10.1145/2619092,
author = {Yu, Young Jin and Shin, Dong In and Shin, Woong and Song, Nae Young and Choi, Jae Woo and Kim, Hyeong Seog and Eom, Hyeonsang and Yeom, Heon Young},
title = {Optimizing the Block I/O Subsystem for Fast Storage Devices},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/2619092},
doi = {10.1145/2619092},
abstract = {Fast storage devices are an emerging solution to satisfy data-intensive applications. They provide high transaction rates for DBMS, low response times for Web servers, instant on-demand paging for applications with large memory footprints, and many similar advantages for performance-hungry applications. In spite of the benefits promised by fast hardware, modern operating systems are not yet structured to take advantage of the hardware’s full potential. The software overhead caused by an OS, negligible in the past, adversely impacts application performance, lessening the advantage of using such hardware. Our analysis demonstrates that the overheads from the traditional storage-stack design are significant and cannot easily be overcome without modifying the hardware interface and adding new capabilities to the operating system.In this article, we propose six optimizations that enable an OS to fully exploit the performance characteristics of fast storage devices. With the support of new hardware interfaces, our optimizations minimize per-request latency by streamlining the I/O path and amortize per-request latency by maximizing parallelism inside the device. We demonstrate the impact on application performance through well-known storage benchmarks run against a Linux kernel with a customized SSD. We find that eliminating context switches in the I/O path decreases the software overhead of an I/O request from 20 microseconds to 5 microseconds and a new request merge scheme called Temporal Merge enables the OS to achieve 87% to 100% of peak device performance, regardless of request access patterns or types. Although the performance improvement by these optimizations on a standard SATA-based SSD is marginal (because of its limited interface and relatively high response times), our sensitivity analysis suggests that future SSDs with lower response times will benefit from these changes. The effectiveness of our optimizations encourages discussion between the OS community and storage vendors about future device interfaces for fast storage devices.},
journal = {ACM Trans. Comput. Syst.},
month = jun,
articleno = {6},
numpages = {48},
keywords = {extended interface, device polling, I/O subsystem, request batching}
}

@article{10.1145/2617945,
author = {Mentis, Helena M. and Laaksolahti, Jarmo and H\"{o}\"{o}k, Kristina},
title = {My Self and You: Tension in Bodily Sharing of Experience},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/2617945},
doi = {10.1145/2617945},
abstract = {There is a growing interest in designing systems for sharing experience through bodily interaction. To explore this design space, we built a probe system we named the Lega. In our 2-month-long research design process, we noted that the users’ attention was set on their own reflective experience, rather than attending to the person(s) with which they were sharing their experience. To explain these findings, we present an inductive analysis of the data through a phenomenological lens to pinpoint what causes such behavior. Our analysis extends our understanding of how to design for social embodied interaction, pointing to how we need to embrace the tension between self-reflection and shared experience, making inward listening and social expression visible acts, accessible to social construction and understanding. It entails experiencing our embodied self as others experience us in order to build a dialogue.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {20},
numpages = {26},
keywords = {interactional, experience, Phenomenology}
}

@article{10.1145/2617588,
author = {Cherry, Erin and Latulipe, Celine},
title = {Quantifying the Creativity Support of Digital Tools through the Creativity Support Index},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/2617588},
doi = {10.1145/2617588},
abstract = {Creativity support tools help people engage creatively with the world, but measuring how well a tool supports creativity is challenging since creativity is ill-defined. To this end, we developed the Creativity Support Index (CSI), which is a psychometric survey designed for evaluating the ability of a creativity support tool to assist a user engaged in creative work. The CSI measures six dimensions of creativity support: Exploration, Expressiveness, Immersion, Enjoyment, Results Worth Effort, and Collaboration. The CSI allows researchers to understand not just how well a tool supports creative work overall, but what aspects of creativity support may need attention. In this article, we present the CSI, along with scenarios for how it can be deployed in a variety of HCI research settings and how the CSI scores can help target design improvements. We also present the iterative, rigorous development and validation process used to create the CSI.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {21},
numpages = {25},
keywords = {psychometrics, evaluation, Creativity support tools, surveys}
}

@inproceedings{10.1145/2611040.2611090,
author = {Riel, Arthur J. and Popescu, Denisa and Guanlao, Luisita},
title = {Social Data Mining and Knowledge Flows Between Government and Its Citizenry in Crisis and Normal Situations},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611090},
doi = {10.1145/2611040.2611090},
abstract = {As the world's population is rapidly growing in urban areas, leveraging technologies for building efficient and sustainable infrastructure to support its residents has been a core focus of the cities and local governments' strategic plans. This paper will discuss applications relevant to urban areas with a focus on discussing research insight tools that draw on real-time public opinion sentiments in democratic societies. These tools will provide important data on what citizens know and want to know in normal circumstances. It also provides real-time analysis on development and humanitarian challenges within urban areas in times of crisis. The knowledge flows that are defined by citizen participation in government decisions can be exploited for other purposes, including the creation of smarter infrastructure (e.g., an electric grid or traffic management) aimed at improving the efficiency and quality of the services provided by local governments.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {56},
numpages = {5},
keywords = {intelligent infrastructure, Data mining, citizen engagement, information integration},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2611040.2611094,
author = {Vakali, Athena and Anthopoulos, Leonidas and Krco, Srdjan},
title = {Smart Cities Data Streams Integration: Experimenting with Internet of Things and Social Data Flows},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611094},
doi = {10.1145/2611040.2611094},
abstract = {Smart cities are nowadays expanding and flourishing worldwide with Internet of Things (IoT), i.e. smart things like sensors and actuators, and mobile devices applications and installations which change the citizens' and authorities' everyday life. Smart cities produce daily huge streams of sensors data while citizens interact with Web and/or mobile devices utilizing social networks. In such a smart city context, new approaches to integrate big data streams from both sensors and social networks are needed to exploit big data production and circulation towards offering innovative solutions and applications. The SmartSantander infrastructure (EU FP7 project) has offered the ground for the SEN2SOC experiment which has integrated sensor and social data streams. This presentation outlines its research and industrial perspective and potential impact.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {60},
numpages = {5},
keywords = {Data Management, Visualization, Smart People, Smart Government, Data Modeling, Smart City},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2594368.2594379,
author = {Parate, Abhinav and Chiu, Meng-Chieh and Chadowitz, Chaniel and Ganesan, Deepak and Kalogerakis, Evangelos},
title = {RisQ: Recognizing Smoking Gestures with Inertial Sensors on a Wristband},
year = {2014},
isbn = {9781450327930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594368.2594379},
doi = {10.1145/2594368.2594379},
abstract = {Smoking-induced diseases are known to be the leading cause of death in the United States. In this work, we design RisQ, a mobile solution that leverages a wristband containing a 9-axis inertial measurement unit to capture changes in the orientation of a person's arm, and a machine learning pipeline that processes this data to accurately detect smoking gestures and sessions in real-time. Our key innovations are four-fold: a) an arm trajectory-based method that extracts candidate hand-to-mouth gestures, b) a set of trajectory-based features to distinguish smoking gestures from confounding gestures including eating and drinking, c) a probabilistic model that analyzes sequences of hand-to-mouth gestures and infers which gestures are part of individual smoking sessions, and d) a method that leverages multiple IMUs placed on a person's body together with 3D animation of a person's arm to reduce burden of self-reports for labeled data collection. Our experiments show that our gesture recognition algorithm can detect smoking gestures with high accuracy (95.7%), precision (91%) and recall (81%). We also report a user study that demonstrates that we can accurately detect the number of smoking sessions with very few false positives over the period of a day, and that we can reliably extract the beginning and end of smoking session periods.},
booktitle = {Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {149–161},
numpages = {13},
keywords = {smoking detection, wearables, inertial measurement unit, mobile computing},
location = {Bretton Woods, New Hampshire, USA},
series = {MobiSys '14}
}

@inproceedings{10.1145/2611040.2611057,
author = {Kontiza, Kalliopi and Bikakis, Antonis},
title = {Web Search Results Visualization: Evaluation of Two Semantic Search Engines},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611057},
doi = {10.1145/2611040.2611057},
abstract = {Semantic search engines improve the accuracy of search results by understanding the meaning and context of terms as they appear in web documents. But do they also improve the presentation of search results? We discuss this research question and attempt to address it by evaluating two semantic search engines: Sig.ma and Kngine. Our analysis is based on the theory and methods of explorative evaluation studies, the inspection evaluation approach in information visualization, user-oriented evaluation studies, and on quantitative data analysis. Our main conclusion is that visualization used in semantic search engines improves the understanding of search results and the overall search experience by exploiting the semantics of web data.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {39},
numpages = {12},
keywords = {Kngine, Analytical Inspection, User Oriented Evaluation, Information Visualization, Sig.ma, Semantic Search},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2594368.2594386,
author = {Rahman, Tauhidur and Adams, Alexander T. and Zhang, Mi and Cherry, Erin and Zhou, Bobby and Peng, Huaishu and Choudhury, Tanzeem},
title = {BodyBeat: A Mobile System for Sensing Non-Speech Body Sounds},
year = {2014},
isbn = {9781450327930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594368.2594386},
doi = {10.1145/2594368.2594386},
abstract = {In this paper, we propose BodyBeat, a novel mobile sensing system for capturing and recognizing a diverse range of non-speech body sounds in real-life scenarios. Non-speech body sounds, such as sounds of food intake, breath, laughter, and cough contain invaluable information about our dietary behavior, respiratory physiology, and affect. The BodyBeat mobile sensing system consists of a custom-built piezoelectric microphone and a distributed computational framework that utilizes an ARM microcontroller and an Android smartphone. The custom-built microphone is designed to capture subtle body vibrations directly from the body surface without being perturbed by external sounds. The microphone is attached to a 3D printed neckpiece with a suspension mechanism. The ARM embedded system and the Android smartphone process the acoustic signal from the microphone and identify non-speech body sounds. We have extensively evaluated the BodyBeat mobile sensing system. Our results show that BodyBeat outperforms other existing solutions in capturing and recognizing different types of important non-speech body sounds.},
booktitle = {Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {2–13},
numpages = {12},
keywords = {acoustic signal processing, mobile sensing, non-speech body sound, embedded systems},
location = {Bretton Woods, New Hampshire, USA},
series = {MobiSys '14}
}

@inproceedings{10.1145/2611040.2611077,
author = {Aggarwal, Sonal and Van Oostendorp, Herre and Indurkhya, Bipin},
title = {Automating Web-Navigation Support Using a Cognitive Model},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611077},
doi = {10.1145/2611040.2611077},
abstract = {Reducing cognitive overhead is one way to ensure fast and easy web-navigation for users. We present here an automated navigation-support tool for websites. Whenever a user visits a website for searching some information, the tool will suggest relevant links to click, where they can find desired information on that website. The tool is based on a new cognitive model CoLiDeS++Pic, which combines two related models, CoLiDeS+ and CoLiDeS+Pic. The model computes semantic similarity between the user goal and the website information using latent semantic analysis technique. It also takes into account the path adequacy, performs appropriate backtracking if required, and uses semantic information from pictures. In the current implementation, the tool is not fully automated because the semantic features of pictures have to be obtained manually. To evaluate the effectiveness of the tool, we conducted an experiment with tool support and multi-tasking as independent variables. Statistical analysis showed a significant positive impact of the tool support for time needed to perform search tasks, disorientation in navigation and task-accuracy. The navigation performance in time and disorientation of the users and answering questions are improved when provided with the tool support. Multi-tasking had no effect on time needed or answering questions, but we observed an unexpected positive effect on disorientation, suggesting that perhaps the participants were more motivated to perform primary task due to the assigned secondary task (monitoring a video at the same time), which resulted in less disorientation.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {19},
numpages = {6},
keywords = {Cognitive models, Automated Support, Web-Navigation, Semantics, Hyperlink},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2611040.2611067,
author = {Blanvillain, Olivier and Kasioumis, Nikos and Banos, Vangelis},
title = {BlogForever Crawler: Techniques and Algorithms to Harvest Modern Weblogs},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611067},
doi = {10.1145/2611040.2611067},
abstract = {Blogs are a dynamic communication medium which has been widely established on the web. The BlogForever project has developed an innovative system to harvest, preserve, manage and reuse blog content. This paper presents a key component of the BlogForever platform, the web crawler. More precisely, our work concentrates on techniques to automatically extract content such as articles, authors, dates and comments from blog posts. To achieve this goal, we introduce a simple and robust algorithm to generate extraction rules based on string matching using the blog's web feed in conjunction with blog hypertext. This approach leads to a scalable blog data extraction process. Furthermore, we show how we integrate a web browser into the web harvesting process in order to support data extraction from blogs with JavaScript generated content.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {7},
numpages = {8},
keywords = {web data extraction, Blog crawler, wrapper generation},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2594368.2594377,
author = {Ravindranath, Lenin and Nath, Suman and Padhye, Jitendra and Balakrishnan, Hari},
title = {Automatic and Scalable Fault Detection for Mobile Applications},
year = {2014},
isbn = {9781450327930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594368.2594377},
doi = {10.1145/2594368.2594377},
abstract = {This paper describes the design, implementation, and evaluation of VanarSena, an automated fault finder for mobile applications (``apps''). The techniques in VanarSena are driven by a study of 25 million real-world crash reports of Windows Phone apps reported in 2012. Our analysis indicates that a modest number of root causes are responsible for many observed failures, but that they occur in a wide range of places in an app, requiring a wide coverage of possible execution paths. VanarSena adopts a ``greybox'' testing method, instrumenting the app binary to achieve both coverage and speed. VanarSena runs on cloud servers: the developer uploads the app binary; VanarSena then runs several app ``monkeys'' in parallel to emulate user, network, and sensor data behavior, returning a detailed report of crashes and failures. We have tested VanarSena with 3000 apps from the Windows Phone store, finding that 1108 of them had failures; VanarSena uncovered 2969 distinct bugs in existing apps, including 1227 that were not previously reported. Because we anticipate VanarSena being used in regular regression tests, testing speed is important. VanarSena uses two techniques to improve speed. First, it uses a ``hit testing'' method to quickly emulate an app by identifying which user interface controls map to the same execution handlers in the code. Second, it generates a ProcessingCompleted event to accurately determine when to start the next interaction. These features are key benefits of VanarSena's greybox philosophy.},
booktitle = {Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {190–203},
numpages = {14},
keywords = {mobile applications, testing, reliability, software engineering},
location = {Bretton Woods, New Hampshire, USA},
series = {MobiSys '14}
}

@inproceedings{10.1145/2590296.2590337,
author = {Miettinen, Markus and Heuser, Stephan and Kronz, Wiebke and Sadeghi, Ahmad-Reza and Asokan, N.},
title = {ConXsense: Automated Context Classification for Context-Aware Access Control},
year = {2014},
isbn = {9781450328005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590296.2590337},
doi = {10.1145/2590296.2590337},
abstract = {We present ConXsense, the first framework for context-aware access control on mobile devices based on context classification. Previous context-aware access control systems often require users to laboriously specify detailed policies or they rely on pre-defined policies not adequately reflecting the true preferences of users. We present the design and implementation of a context-aware framework that uses a probabilistic approach to overcome these deficiencies. The framework utilizes context sensing and machine learning to automatically classify contexts according to their security and privacy-related properties. We apply the framework to two important smartphone-related use cases: protection against device misuse using a dynamic device lock and protection against sensory malware. We ground our analysis on a sociological survey examining the perceptions and concerns of users related to contextual smartphone security and analyze the effectiveness of our approach with real-world context data. We also demonstrate the integration of our framework with the FlaskDroid architecture for fine-grained access control enforcement on the Android platform.},
booktitle = {Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security},
pages = {293–304},
numpages = {12},
keywords = {mobile security, context-awareness, privacy policies, context sensing},
location = {Kyoto, Japan},
series = {ASIA CCS '14}
}

@inproceedings{10.1145/2590296.2590297,
author = {Xie, Guowu and Hang, Huy and Faloutsos, Michalis},
title = {Scanner Hunter: Understanding HTTP Scanning Traffic},
year = {2014},
isbn = {9781450328005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590296.2590297},
doi = {10.1145/2590296.2590297},
abstract = {This paper focuses on detecting and studying HTTP scanners, which are malicious entities that explore a website selectively for "opportunities" that can potentially be used for subsequent intrusion attempts. Interestingly, there is practically no prior work on the detection of these entities, which are different from web crawlers or machines performing network-level reconnaissance activities such as port scanning. Detecting HTTP scanners is challenging as they are stealthy and often only probe a few key places on a website, so finding them is a needle-in-the-haystack problem. At the same time, they pose serious risk because they perform the first, exploratory step to provide the seed information that may allow hackers to compromise a website. Our work makes two main contributions. First, we propose Scanner Hunter, arguably the first method to detect HTTP scanners efficiently. The novelty and success of the method lies in the use of community structure, in an appropriately constructed bipartite graph, in order to expose groups of HTTP scanners. The rationale is that the aggregated behavior makes identifying groups of scanners easier than attempting to profile and label IP addresses individually. Scanner Hunter achieves an impressive 96.5% detection precision, which is roughly twice as high as the precision of the Machine Learning-based methods that we use as reference. Second, we provide an extensive study of HTTP scanners in an effort to understand: (a) their spatial and temporal properties, (b) the techniques and tools used by the scanners, and (c) the types of resources they are looking for, which can provides hints as to what the subsequent penetration attempt may target. We use six months worth of web traffic logs collected in 2012 from a University campus, the websites hosted by which received over 1.9 billion requests from 12.8 million IPs. We found that the number of HTTP scanners is non-trivial with roughly 4,000 IPs engaging in this type of activity per week. Our work will hopefully raise the awareness of the community regarding this problem while at the same time provide a promising detection technique that can provide the basis for mitigating the risk posed by HTTP scanners.},
booktitle = {Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security},
pages = {27–38},
numpages = {12},
keywords = {detection, scanning traffic, vulnerability, web security, HTTP scanning, HTTP scanner},
location = {Kyoto, Japan},
series = {ASIA CCS '14}
}

@inproceedings{10.1145/2590296.2590300,
author = {Zhang, Su and Zhang, Xinwen and Ou, Xinming},
title = {After We Knew It: Empirical Study and Modeling of Cost-Effectiveness of Exploiting Prevalent Known Vulnerabilities across IaaS Cloud},
year = {2014},
isbn = {9781450328005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590296.2590300},
doi = {10.1145/2590296.2590300},
abstract = {Infrastructure as a Service (IaaS) cloud has been attracting more and more customers as it provides the highest level of flexibility by offering configurable virtual machines (VMs) and computing infrastructures. Public VM images are usually available for customers to customize and launch. However, the 1 to N mapping between VM images and running instances in IaaS makes vulnerabilities propagate rapidly across the entire public cloud. Besides, IaaS cloud naturally comes with a larger and more stable attack surface and more concentrated target resources than traditional surroundings. In this paper, we first identify the threat of exploiting prevalent vulnerabilities over public IaaS cloud with an empirical study in Amazon EC2. We find that attackers can compromise a considerable number of VMs with trivial cost. We then do a qualitative cost-effectiveness analysis of this threat. Our main result is a two-fold observation: in IaaS cloud, exploiting prevalent vulnerabilities is much more cost-effective than traditional in-house computing environment, therefore attackers have stronger incentive; Fortunately, on the other hand, cloud defenders (cloud providers and customers) also have much lower cost-loss ratio than in traditional environment, therefore they can be more effective for defending attacks. We then build a game-theoretic model and conduct a risk-gain analysis to compare exploiting and patching strategies under cloud and traditional computing environments. Our modeling indicates that under cloud environment, both attack and defense become less cost-effective as time goes by, and the earlier actioner can be more rewarding. We propose countermeasures against such threat in order to bridge the gap between current security situation and defending mechanisms. To our best knowledge, we are the first to analyze and model the threat with prevalent known-vulnerabilities in public cloud.},
booktitle = {Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security},
pages = {317–328},
numpages = {12},
keywords = {cloud computing, patching management, vulnerability management, game theory, virtual machine images},
location = {Kyoto, Japan},
series = {ASIA CCS '14}
}

@inproceedings{10.1145/2594291.2594294,
author = {Sampson, Adrian and Panchekha, Pavel and Mytkowicz, Todd and McKinley, Kathryn S. and Grossman, Dan and Ceze, Luis},
title = {Expressing and Verifying Probabilistic Assertions},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594294},
doi = {10.1145/2594291.2594294},
abstract = {Traditional assertions express correctness properties that must hold on every program execution. However, many applications have probabilistic outcomes and consequently their correctness properties are also probabilistic (e.g., they identify faces in images, consume sensor data, or run on unreliable hardware). Traditional assertions do not capture these correctness properties. This paper proposes that programmers express probabilistic correctness properties with probabilistic assertions and describes a new probabilistic evaluation approach to efficiently verify these assertions. Probabilistic assertions are Boolean expressions that express the probability that a property will be true in a given execution rather than asserting that the property must always be true. Given either specific inputs or distributions on the input space, probabilistic evaluation verifies probabilistic assertions by first performing distribution extraction to represent the program as a Bayesian network. Probabilistic evaluation then uses statistical properties to simplify this representation to efficiently compute assertion probabilities directly or with sampling. Our approach is a mix of both static and dynamic analysis: distribution extraction statically builds and optimizes the Bayesian network representation and sampling dynamically interprets this representation. We implement our approach in a tool called Mayhap for C and C++ programs. We evaluate expressiveness, correctness, and performance of Mayhap on programs that use sensors, perform approximate computation, and obfuscate data for privacy. Our case studies demonstrate that probabilistic assertions describe useful correctness properties and that Mayhap efficiently verifies them.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {112–122},
numpages = {11},
keywords = {sensors, differential privacy, data obfuscation, approximate computing, symbolic execution, probabilistic programming},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@article{10.1145/2666356.2594294,
author = {Sampson, Adrian and Panchekha, Pavel and Mytkowicz, Todd and McKinley, Kathryn S. and Grossman, Dan and Ceze, Luis},
title = {Expressing and Verifying Probabilistic Assertions},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666356.2594294},
doi = {10.1145/2666356.2594294},
abstract = {Traditional assertions express correctness properties that must hold on every program execution. However, many applications have probabilistic outcomes and consequently their correctness properties are also probabilistic (e.g., they identify faces in images, consume sensor data, or run on unreliable hardware). Traditional assertions do not capture these correctness properties. This paper proposes that programmers express probabilistic correctness properties with probabilistic assertions and describes a new probabilistic evaluation approach to efficiently verify these assertions. Probabilistic assertions are Boolean expressions that express the probability that a property will be true in a given execution rather than asserting that the property must always be true. Given either specific inputs or distributions on the input space, probabilistic evaluation verifies probabilistic assertions by first performing distribution extraction to represent the program as a Bayesian network. Probabilistic evaluation then uses statistical properties to simplify this representation to efficiently compute assertion probabilities directly or with sampling. Our approach is a mix of both static and dynamic analysis: distribution extraction statically builds and optimizes the Bayesian network representation and sampling dynamically interprets this representation. We implement our approach in a tool called Mayhap for C and C++ programs. We evaluate expressiveness, correctness, and performance of Mayhap on programs that use sensors, perform approximate computation, and obfuscate data for privacy. Our case studies demonstrate that probabilistic assertions describe useful correctness properties and that Mayhap efficiently verifies them.},
journal = {SIGPLAN Not.},
month = jun,
pages = {112–122},
numpages = {11},
keywords = {symbolic execution, probabilistic programming, sensors, approximate computing, data obfuscation, differential privacy}
}

@inproceedings{10.1145/2618137.2618139,
author = {Poss, Raphael and Altmeyer, Sebastian and Thompson, Mark and Jelier, Rob},
title = {Academia 2.0: Removing the Publisher Middle-Man While Retaining Impact},
year = {2014},
isbn = {9781450329514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2618137.2618139},
doi = {10.1145/2618137.2618139},
abstract = {Recent work on academic publishing has focused on transparency, to eliminate skews in favor of results channeled through already established publishers. This movement, called "open peer review", will require infrastructure. So far, proposed realizations of open peer review have relied on centralized coordinating platforms; this is unsatisfactory as this architectural choice stays vulnerable to long-term predatory commercial capture and data loss. Instead, we propose "Academia 2.0", a combination of both true peer-to-peer, distributed scientific dissemination channels, and their accompanying workflows for open peer review. It features safe decoupling of storage, indexing and search sites and supports research metrics. Our proposal relies on the existence of semantic web sites for researchers and powerful Internet search engines, an assumption which did not hold 10 years ago. We also introduce post-hoc citations, a key mechanism for quality control, impact measurement and post-hoc credit attribution for previous work. Due to the technology involved, computer engineering is likely the scientific field with the most potential to try out and evaluate our model.},
booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Reproducible Research Methodologies and New Publication Models in Computer Engineering},
articleno = {3},
numpages = {6},
keywords = {academic publishing, post-hoc citation, semantic web, emergent impact, dissemination of science},
location = {Edinburgh, United Kingdom},
series = {TRUST '14}
}

@inproceedings{10.1145/2594291.2594306,
author = {Gupta, Gagan and Sridharan, Srinath and Sohi, Gurindar S.},
title = {Globally Precise-Restartable Execution of Parallel Programs},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594306},
doi = {10.1145/2594291.2594306},
abstract = {Emerging trends in computer design and use are likely to make exceptions, once rare, the norm, especially as the system size grows. Due to exceptions, arising from hardware faults, approximate computing, dynamic resource management, etc., successful and error-free execution of programs may no longer be assured. Yet, designers will want to tolerate the exceptions so that the programs execute completely, efficiently and without external intervention.Modern computers easily handle exceptions in sequential programs, using precise interrupts. But they are ill-equipped to handle exceptions in parallel programs, which are growing in prevalence. In this work we introduce the notion of globally precise-restartable execution of parallel programs, analogous to precise-interruptible execution of sequential programs. We present a software runtime recovery system based on the approach to handle exceptions in suitably-written parallel programs. Qualitative and quantitative analyses show that the proposed system scales with the system size, especially when exceptions are frequent, unlike the conventional checkpoint-and-recovery method.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {181–192},
numpages = {12},
keywords = {deterministic multithreading, precise exceptions},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@article{10.1145/2666356.2594306,
author = {Gupta, Gagan and Sridharan, Srinath and Sohi, Gurindar S.},
title = {Globally Precise-Restartable Execution of Parallel Programs},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666356.2594306},
doi = {10.1145/2666356.2594306},
abstract = {Emerging trends in computer design and use are likely to make exceptions, once rare, the norm, especially as the system size grows. Due to exceptions, arising from hardware faults, approximate computing, dynamic resource management, etc., successful and error-free execution of programs may no longer be assured. Yet, designers will want to tolerate the exceptions so that the programs execute completely, efficiently and without external intervention.Modern computers easily handle exceptions in sequential programs, using precise interrupts. But they are ill-equipped to handle exceptions in parallel programs, which are growing in prevalence. In this work we introduce the notion of globally precise-restartable execution of parallel programs, analogous to precise-interruptible execution of sequential programs. We present a software runtime recovery system based on the approach to handle exceptions in suitably-written parallel programs. Qualitative and quantitative analyses show that the proposed system scales with the system size, especially when exceptions are frequent, unlike the conventional checkpoint-and-recovery method.},
journal = {SIGPLAN Not.},
month = jun,
pages = {181–192},
numpages = {12},
keywords = {deterministic multithreading, precise exceptions}
}

@inproceedings{10.1145/2594291.2594316,
author = {Rompf, Tiark and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Chafi, Hassan and Olukotun, Kunle},
title = {Surgical Precision JIT Compilers},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594316},
doi = {10.1145/2594291.2594316},
abstract = {Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable.In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domain-specific compiler optimizations or safety checks.We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction.In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {41–52},
numpages = {12},
keywords = {staging, program generation, JIT compilation},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@article{10.1145/2666356.2594316,
author = {Rompf, Tiark and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Chafi, Hassan and Olukotun, Kunle},
title = {Surgical Precision JIT Compilers},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666356.2594316},
doi = {10.1145/2666356.2594316},
abstract = {Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable.In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domain-specific compiler optimizations or safety checks.We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction.In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.},
journal = {SIGPLAN Not.},
month = jun,
pages = {41–52},
numpages = {12},
keywords = {program generation, staging, JIT compilation}
}

@inproceedings{10.1145/2597652.2597654,
author = {Michelogiannakis, George and Williams, Alexander and Williams, Samuel and Shalf, John},
title = {Collective Memory Transfers for Multi-Core Chips},
year = {2014},
isbn = {9781450326421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597652.2597654},
doi = {10.1145/2597652.2597654},
abstract = {Future performance improvements for microprocessors have shifted from clock frequency scaling towards increases in on-chip parallelism. Performance improvements for a wide variety of parallel applications require domain decomposition of data arrays from a contiguous arrangement in memory to a tiled layout for on-chip L1 caches and scratchpads. However, DRAM performance suffers under the non-streaming access patterns generated by many independent cores. In this paper, we propose collective memory scheduling (CMS) that uses simple software and inexpensive hardware to identify collective transfers and guarantee that loads and stores arrive in memory address order to the memory controller. CMS actively takes charge of collective transfers and pushes or pulls data to or from the on-chip processors according to memory address order. CMS reduces application execution time by up to 55% (20% average) compared to a state-of-the-art architecture where each processor reads and writes its data independently. CMS also reduces DRAM read power by 2.2\texttimes{} and write power by 50%.},
booktitle = {Proceedings of the 28th ACM International Conference on Supercomputing},
pages = {343–352},
numpages = {10},
keywords = {domain decomposition, data-parallel applications, row buffer locality, collective transfers, over fetch, in-order accesses},
location = {Munich, Germany},
series = {ICS '14}
}

@inproceedings{10.1145/2600918.2600919,
author = {Jelasity, M\'{a}rk and Birman, Kenneth P.},
title = {Distributional Differential Privacy for Large-Scale Smart Metering},
year = {2014},
isbn = {9781450326476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600918.2600919},
doi = {10.1145/2600918.2600919},
abstract = {In smart power grids it is possible to match supply and demand by applying control mechanisms that are based on fine-grained load prediction. A crucial component of every control mechanism is monitoring, that is, executing queries over the network of smart meters. However, smart meters can learn so much about our lives that if we are to use such methods, it becomes imperative to protect privacy. Recent proposals recommend restricting the provider to differentially private queries, however the practicality of such approaches has not been settled. Here, we tackle an important problem with such approaches: even if queries at different points in time over statistically independent data are implemented in a differentially private way, the parameters of the distribution of the query might still reveal sensitive personal information. Protecting these parameters is hard if we allow for continuous monitoring, a natural requirement in the smart grid. We propose novel differentially private mechanisms that solve this problem for sum queries. We evaluate our methods and assumptions using a theoretical analysis as well as publicly available measurement data and show that the extra noise needed to protect distribution parameters is small.},
booktitle = {Proceedings of the 2nd ACM Workshop on Information Hiding and Multimedia Security},
pages = {141–146},
numpages = {6},
keywords = {smart grids, continual observation, distributional differential privacy},
location = {Salzburg, Austria},
series = {IH&amp;MMSec '14}
}

@inproceedings{10.1145/2611264.2611267,
author = {Lee, Youngki and Balan, Rajesh Krishina},
title = {The Case for Human-Centric Personal Analytics},
year = {2014},
isbn = {9781450328258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611264.2611267},
doi = {10.1145/2611264.2611267},
abstract = {The rich context provided by smartphones has enabled many new context-aware applications. However, these applications still need to provide their own mechanisms to interpret low-level sensing data and generate high-level user states. In this paper, we propose the idea of building a personal analytics (PA) layer that will use inputs from multiple lower layer sources, such as sensor data (accelerometers, gyroscopes, etc.), phone data (call logs, application activity, etc.), and online sources (Twitter, Facebook posts, etc.) to generate high-level user contextual states (such as emotions, preferences, and engagements). Developers can then use the PA layer to easily build a new set of interesting and compelling applications. We describe several scenarios enabled by this new layer and present a proposed software architecture. We end with a description of some of the key research challenges that need to be solved to achieve this goal.},
booktitle = {Proceedings of the 2014 Workshop on Physical Analytics},
pages = {25–29},
numpages = {5},
keywords = {personal analytics, human centric contexts},
location = {Bretton Woods, New Hampshire, USA},
series = {WPA '14}
}

@inproceedings{10.1145/2602044.2602050,
author = {Carpenter, Tommy and Golab, Lukasz and Syed, Sohail Javaad},
title = {Is the Grass Greener? Mining Electric Vehicle Opinions},
year = {2014},
isbn = {9781450328197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602044.2602050},
doi = {10.1145/2602044.2602050},
abstract = {Electric Vehicles (EVs) are envisioned to play a large role in the transition from fossil fuel to renewables based transportation. However, their sales thus far are nominal compared to traditional car sales. It has been difficult for manufacturers to measure owners' initial perceptions in order to build improved vehicles more drivers are likely to adopt. Sentiments towards EVs have mostly been determined using either field trials or large surveys of drivers, both of which are problematic. We build a system that mines EV owners' sentiments from online forums. Our system has three main uses. First, it graphs the percentage of positive and negative opinions for each vehicle feature of interest, e.g., battery capacity, giving the user a high level product overview. There is currently no easily-consumable review system for EVs. Second, it allows the user to read opinions about the specific features they are most interested in without searching though irrelevant text. In our case study, we find only 3% of the comments on EV ownership forums express opinions on the features. The system therefore reduces the space of text the user must read by 97%, even assuming they wish to read all opinions about all features. Finally, in addition to mining the same perceptions found during expensive field trials, our system finds perceptions that were only realized after the owners possessed their EVs for an extended period of time, i.e., perceptions not available during shorter trials. The system extracts and classifies opinions with a precision and recall of 60%, which is on par or better than previous opinion mining systems.},
booktitle = {Proceedings of the 5th International Conference on Future Energy Systems},
pages = {241–252},
numpages = {12},
keywords = {opinion mining, electric vehicles, sentiment analysis},
location = {Cambridge, United Kingdom},
series = {e-Energy '14}
}

@inproceedings{10.5555/2665671.2665720,
author = {Liu, Ming and Li, Tao},
title = {Optimizing Virtual Machine Consolidation Performance on NUMA Server Architecture for Cloud Workloads},
year = {2014},
isbn = {9781479943944},
publisher = {IEEE Press},
abstract = {Server virtualization and workload consolidation enable multiple workloads to share a single physical server, resulting in significant energy savings and utilization improvements. The shift of physical server architectures to NUMA and the increasing popularity of scale-out cloud applications undermine workload consolidation efficiency and result in overall system degradation. In this work, we characterize the consolidation of cloud workloads on NUMA virtualized systems, estimate four different sources of architecture overhead, and explore optimization opportunities beyond the default NUMA-aware hypervisor memory managementMotivated by the observed architectural impact on cloud workload consolidation performance, we propose three optimization techniques incorporating NUMA access overhead into the hypervisor's virtual machine memory allocation and page fault handling routines. Among these, estimation of the memory zone access overhead serves as a foundation for the other two techniques: a NUMA overhead aware buddy allocator and a P2M swap FIFO. Cache hit rate, cycle loss due to cache miss, and IPC serve as indicators to estimate the access cost of each memory node. Our optimized buddy allocator dynamically selects low-overhead memory zones and "proportionally" distributes memory pages across target nodes. The P2M swap FIFO records recently unused PFN, MFN lists for mapping exchanges to rebalance memory access pressure within one domain. Our real system based evaluations show a 41.1% performance improvement when consolidating 16-VMs on a 4- socket server (the proposed allocator contributes 22.8% of the performance gain and the P2M swap FIFO accounts for the rest). Furthermore, our techniques can cooperate well with other methods (i.e. vCPU migration) and scale well when varying VM memory size and the number of sockets in a physical host},
booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
pages = {325–336},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}

@article{10.1145/2678373.2665720,
author = {Liu, Ming and Li, Tao},
title = {Optimizing Virtual Machine Consolidation Performance on NUMA Server Architecture for Cloud Workloads},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2678373.2665720},
doi = {10.1145/2678373.2665720},
abstract = {Server virtualization and workload consolidation enable multiple workloads to share a single physical server, resulting in significant energy savings and utilization improvements. The shift of physical server architectures to NUMA and the increasing popularity of scale-out cloud applications undermine workload consolidation efficiency and result in overall system degradation. In this work, we characterize the consolidation of cloud workloads on NUMA virtualized systems, estimate four different sources of architecture overhead, and explore optimization opportunities beyond the default NUMA-aware hypervisor memory managementMotivated by the observed architectural impact on cloud workload consolidation performance, we propose three optimization techniques incorporating NUMA access overhead into the hypervisor's virtual machine memory allocation and page fault handling routines. Among these, estimation of the memory zone access overhead serves as a foundation for the other two techniques: a NUMA overhead aware buddy allocator and a P2M swap FIFO. Cache hit rate, cycle loss due to cache miss, and IPC serve as indicators to estimate the access cost of each memory node. Our optimized buddy allocator dynamically selects low-overhead memory zones and "proportionally" distributes memory pages across target nodes. The P2M swap FIFO records recently unused PFN, MFN lists for mapping exchanges to rebalance memory access pressure within one domain. Our real system based evaluations show a 41.1% performance improvement when consolidating 16-VMs on a 4- socket server (the proposed allocator contributes 22.8% of the performance gain and the P2M swap FIFO accounts for the rest). Furthermore, our techniques can cooperate well with other methods (i.e. vCPU migration) and scale well when varying VM memory size and the number of sockets in a physical host},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {325–336},
numpages = {12}
}

@inproceedings{10.1145/2591971.2591986,
author = {Gorlatova, Maria and Sarik, John and Grebla, Guy and Cong, Mina and Kymissis, Ioannis and Zussman, Gil},
title = {Movers and Shakers: Kinetic Energy Harvesting for the Internet of Things},
year = {2014},
isbn = {9781450327893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591971.2591986},
doi = {10.1145/2591971.2591986},
abstract = {Numerous energy harvesting wireless devices that will serve as building blocks for the Internet of Things (IoT) are currently under development. However, there is still only limited understanding of the properties of various energy sources and their impact on energy harvesting adaptive algorithms. Hence, we focus on characterizing the kinetic (motion) energy that can be harvested by a wireless node with an IoT form factor and on developing energy allocation algorithms for such nodes. In this paper, we describe methods for estimating harvested energy from acceleration traces. To characterize the energy availability associated with specific human activities (e.g., relaxing, walking, cycling), we analyze a motion dataset with over 40 participants. Based on acceleration measurements that we collected for over 200 hours, we study energy generation processes associated with day-long human routines. We also briefly summarize our experiments with moving objects. We develop energy allocation algorithms that take into account practical IoT node design considerations, and evaluate the algorithms using the collected measurements. Our observations provide insights into the design of motion energy harvesters, IoT nodes, and energy harvesting adaptive algorithms.},
booktitle = {The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
pages = {407–419},
numpages = {13},
keywords = {motion energy, algorithms, internet of things, energy harvesting, measurements, low-power networking},
location = {Austin, Texas, USA},
series = {SIGMETRICS '14}
}

@article{10.1145/2637364.2591986,
author = {Gorlatova, Maria and Sarik, John and Grebla, Guy and Cong, Mina and Kymissis, Ioannis and Zussman, Gil},
title = {Movers and Shakers: Kinetic Energy Harvesting for the Internet of Things},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2637364.2591986},
doi = {10.1145/2637364.2591986},
abstract = {Numerous energy harvesting wireless devices that will serve as building blocks for the Internet of Things (IoT) are currently under development. However, there is still only limited understanding of the properties of various energy sources and their impact on energy harvesting adaptive algorithms. Hence, we focus on characterizing the kinetic (motion) energy that can be harvested by a wireless node with an IoT form factor and on developing energy allocation algorithms for such nodes. In this paper, we describe methods for estimating harvested energy from acceleration traces. To characterize the energy availability associated with specific human activities (e.g., relaxing, walking, cycling), we analyze a motion dataset with over 40 participants. Based on acceleration measurements that we collected for over 200 hours, we study energy generation processes associated with day-long human routines. We also briefly summarize our experiments with moving objects. We develop energy allocation algorithms that take into account practical IoT node design considerations, and evaluate the algorithms using the collected measurements. Our observations provide insights into the design of motion energy harvesters, IoT nodes, and energy harvesting adaptive algorithms.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {407–419},
numpages = {13},
keywords = {motion energy, measurements, low-power networking, energy harvesting, algorithms, internet of things}
}

@inproceedings{10.1145/2591971.2592009,
author = {Suneja, Sahil and Isci, Canturk and Bala, Vasanth and de Lara, Eyal and Mummert, Todd},
title = {Non-Intrusive, out-of-Band and out-of-the-Box Systems Monitoring in the Cloud},
year = {2014},
isbn = {9781450327893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591971.2592009},
doi = {10.1145/2591971.2592009},
abstract = {The dramatic proliferation of virtual machines (VMs) in datacenters and the highly-dynamic and transient nature of VM provisioning has revolutionized datacenter operations. However, the management of these environments is still carried out using re-purposed versions of traditional agents, originally developed for managing physical systems, or most recently via newer virtualization-aware alternatives that require guest cooperation and accessibility. We show that these existing approaches are a poor match for monitoring and managing (virtual) systems in the cloud due to their dependence on guest cooperation and operational health, and their growing lifecycle management overheads in the cloud.In this work, we first present Near Field Monitoring (NFM), our non-intrusive, out-of-band cloud monitoring and analytics approach that is designed based on cloud operation principles and to address the limitations of existing techniques. NFM decouples system execution from monitoring and analytics functions by pushing monitoring out of the targets systems' scope. By leveraging and extending VM introspection techniques, our framework provides simple, standard interfaces to monitor running systems in the cloud that require no guest cooperation or modification, and have minimal effect on guest execution. By decoupling monitoring and analytics from target system context, NFM provides ``always-on'' monitoring, even when the target system is unresponsive. NFM also works ``out-of-the-box'' for any cloud instance as it eliminates any need for installing and maintaining agents or hooks in the monitored systems. We describe the end-to-end implementation of our framework with two real-system prototypes based on two virtualization platforms. We discuss the new cloud analytics opportunities enabled by our decoupled execution, monitoring and analytics architecture. We present four applications that are built on top of our framework and show their use for across-time and across-system analytics.},
booktitle = {The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
pages = {249–261},
numpages = {13},
keywords = {virtualization, mon- itoring, vmi, virtual machine, agentless, cloud, analytics, data center},
location = {Austin, Texas, USA},
series = {SIGMETRICS '14}
}

@article{10.1145/2637364.2592009,
author = {Suneja, Sahil and Isci, Canturk and Bala, Vasanth and de Lara, Eyal and Mummert, Todd},
title = {Non-Intrusive, out-of-Band and out-of-the-Box Systems Monitoring in the Cloud},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2637364.2592009},
doi = {10.1145/2637364.2592009},
abstract = {The dramatic proliferation of virtual machines (VMs) in datacenters and the highly-dynamic and transient nature of VM provisioning has revolutionized datacenter operations. However, the management of these environments is still carried out using re-purposed versions of traditional agents, originally developed for managing physical systems, or most recently via newer virtualization-aware alternatives that require guest cooperation and accessibility. We show that these existing approaches are a poor match for monitoring and managing (virtual) systems in the cloud due to their dependence on guest cooperation and operational health, and their growing lifecycle management overheads in the cloud.In this work, we first present Near Field Monitoring (NFM), our non-intrusive, out-of-band cloud monitoring and analytics approach that is designed based on cloud operation principles and to address the limitations of existing techniques. NFM decouples system execution from monitoring and analytics functions by pushing monitoring out of the targets systems' scope. By leveraging and extending VM introspection techniques, our framework provides simple, standard interfaces to monitor running systems in the cloud that require no guest cooperation or modification, and have minimal effect on guest execution. By decoupling monitoring and analytics from target system context, NFM provides ``always-on'' monitoring, even when the target system is unresponsive. NFM also works ``out-of-the-box'' for any cloud instance as it eliminates any need for installing and maintaining agents or hooks in the monitored systems. We describe the end-to-end implementation of our framework with two real-system prototypes based on two virtualization platforms. We discuss the new cloud analytics opportunities enabled by our decoupled execution, monitoring and analytics architecture. We present four applications that are built on top of our framework and show their use for across-time and across-system analytics.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {249–261},
numpages = {13},
keywords = {mon- itoring, agentless, vmi, cloud, virtual machine, analytics, virtualization, data center}
}

@article{10.1145/2617574,
author = {Hayes, Gillian R. and Cheng, Karen G. and Hirano, Sen H. and Tang, Karen P. and Nagel, Marni S. and Baker, Dianne E.},
title = {Estrellita: A Mobile Capture and Access Tool for the Support of Preterm Infants and Their Caregivers},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/2617574},
doi = {10.1145/2617574},
abstract = {In this article, we describe the design process and principles used in the development of Estrellita, a tool to support parents of preterm infants to track health data. We tested Estrellita in the homes of seven families for 4 months while following seven additional families without Estrellita. The feedback from this trial, including in-depth interviews, surveys, and log analyses, sheds light on how parents can use a mobile data collection tool to enhance their problem-solving processes about their own health and that of their infants, as well as to share with others who support them in this care. In addition to presenting the design of a recording technology for preterm infants and its use in a real-life setting, the results of this research provide a deep understanding of how technology can and should be used to support home care of at-risk patients, in which data capture may be essential.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {19},
numpages = {28},
keywords = {health, wellness, preterm infants, Capture and access}
}

@inproceedings{10.1145/2591971.2592002,
author = {Wang, Kaibo and Ding, Xiaoning and Lee, Rubao and Kato, Shinpei and Zhang, Xiaodong},
title = {GDM: Device Memory Management for Gpgpu Computing},
year = {2014},
isbn = {9781450327893},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591971.2592002},
doi = {10.1145/2591971.2592002},
abstract = {GPGPUs are evolving from dedicated accelerators towards mainstream commodity computing resources. During the transition, the lack of system management of device memory space on GPGPUs has become a major hurdle. In existing GPGPU systems, device memory space is still managed explicitly by individual applications, which not only increases the burden of programmers but can also cause application crashes, hangs, or low performance.In this paper, we present the design and implementation of GDM, a fully functional GPGPU device memory manager to address the above problems and unleash the computing power of GPGPUs in general-purpose environments. To effectively coordinate the device memory usage of different applications, GDM takes control over device memory allocations and data transfers to and from device memory, leveraging a buffer allocated in each application's virtual memory. GDM utilizes the unique features of GPGPU systems and relies on several effective optimization techniques to guarantee the efficient usage of device memory space and to achieve high performance.We have evaluated GDM and compared it against state-of-the-art GPGPU system software on a range of workloads. The results show that GDM can prevent applications from crashes, including those induced by device memory leaks, and improve system performance by up to 43%.},
booktitle = {The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
pages = {533–545},
numpages = {13},
keywords = {operating system, gpu, memory management},
location = {Austin, Texas, USA},
series = {SIGMETRICS '14}
}

@article{10.1145/2637364.2592002,
author = {Wang, Kaibo and Ding, Xiaoning and Lee, Rubao and Kato, Shinpei and Zhang, Xiaodong},
title = {GDM: Device Memory Management for Gpgpu Computing},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2637364.2592002},
doi = {10.1145/2637364.2592002},
abstract = {GPGPUs are evolving from dedicated accelerators towards mainstream commodity computing resources. During the transition, the lack of system management of device memory space on GPGPUs has become a major hurdle. In existing GPGPU systems, device memory space is still managed explicitly by individual applications, which not only increases the burden of programmers but can also cause application crashes, hangs, or low performance.In this paper, we present the design and implementation of GDM, a fully functional GPGPU device memory manager to address the above problems and unleash the computing power of GPGPUs in general-purpose environments. To effectively coordinate the device memory usage of different applications, GDM takes control over device memory allocations and data transfers to and from device memory, leveraging a buffer allocated in each application's virtual memory. GDM utilizes the unique features of GPGPU systems and relies on several effective optimization techniques to guarantee the efficient usage of device memory space and to achieve high performance.We have evaluated GDM and compared it against state-of-the-art GPGPU system software on a range of workloads. The results show that GDM can prevent applications from crashes, including those induced by device memory leaks, and improve system performance by up to 43%.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {533–545},
numpages = {13},
keywords = {operating system, memory management, gpu}
}

@inproceedings{10.1145/2607023.2607029,
author = {Wetzel, Stefanie and Spiel, Katta and Bertel, Sven},
title = {Dynamically Adapting an AI Game Engine Based on Players' Eye Movements and Strategies},
year = {2014},
isbn = {9781450327251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2607023.2607029},
doi = {10.1145/2607023.2607029},
abstract = {Artificial intelligence (AI) game engines have frequently been used to drive computational antagonists when playing games against humans. Limited work exists, however, on using human players' psychophysical measures to directly parametrise AI game engines. Instead, parameters to optimise AI performance are usually derived from general play-related data or user models. This paper presents novel research on using eye movement data in addition to data on users' strategies to adapt the live play of a computational antagonist in the visuo-spatial strategy game, Hex. It offers a set of suitable parameters for both types of data. A systematic evaluation of the approach showed, among other things, that using eye movement data led to significantly better gameplay experience for human players, as they experienced less frustration with sufficient challenge. Findings are discussed not only with regard to designing gameplay experience, but also their more general ramifications on using live psychophysical data for intelligent interactive systems.},
booktitle = {Proceedings of the 2014 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {3–12},
numpages = {10},
keywords = {intelligent systems, interaction technologies, adaptive gameplay, eye movement tracking, user experience design, user studies},
location = {Rome, Italy},
series = {EICS '14}
}

@inproceedings{10.1145/2607023.2607026,
author = {Bowen, Judy and Hinze, Annika and Reid, Selina},
title = {Model-Driven Tools for Medical Device Selection},
year = {2014},
isbn = {9781450327251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2607023.2607026},
doi = {10.1145/2607023.2607026},
abstract = {Safety-critical medical devices are used in hospitals and medical facilities throughout the world, and are relied upon to function correctly and be usable so as not to endanger patients. While such devices are often designed for specific use-cases in specific locations, in reality they may be used in a much wider range of contexts. In addition, the proliferation of these devices within a single environment means that selecting the most appropriate device for a specific task is not always straightforward. In this paper, we consider ways of modelling the context of use of medical devices and how such models may be used to support tools which provide medical personnel with assistance in making decisions about which devices to use in which circumstances.},
booktitle = {Proceedings of the 2014 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {129–138},
numpages = {10},
keywords = {ontology, medical devices, formal concept analysis},
location = {Rome, Italy},
series = {EICS '14}
}

@inproceedings{10.1145/2588555.2610498,
author = {Zoumpatianos, Kostas and Idreos, Stratos and Palpanas, Themis},
title = {Indexing for Interactive Exploration of Big Data Series},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610498},
doi = {10.1145/2588555.2610498},
abstract = {Numerous applications continuously produce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available, which is not currently possible with the state-of-the-art indexing methods and for very large data series collections. In this paper, we present the first adaptive indexing mechanism, specifically tailored to solve the problem of indexing and querying very large data series collections. The main idea is that instead of building the complete index over the complete data set up-front and querying only later, we interactively and adaptively build parts of the index, only for the parts of the data on which the users pose queries. The net effect is that instead of waiting for extended periods of time for the index creation, users can immediately start exploring the data series. We present a detailed design and evaluation of adaptive data series indexing over both synthetic data and real-world workloads. The results show that our approach can gracefully handle large data series collections, while drastically reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), adaptive data series indexing has already answered $3*10^5$ queries.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1555–1566},
numpages = {12},
keywords = {adaptive data-series index, nearest neighbor, adaptive indexing, data-series, similarity search},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2588555.2593680,
author = {Cai, Zhuhua and Gao, Zekai J. and Luo, Shangyu and Perez, Luis L. and Vagena, Zografoula and Jermaine, Christopher},
title = {A Comparison of Platforms for Implementing and Running Very Large Scale Machine Learning Algorithms},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2593680},
doi = {10.1145/2588555.2593680},
abstract = {We describe an extensive benchmark of platforms available to a user who wants to run a machine learning (ML) inference algorithm over a very large data set, but cannot find an existing implementation and thus must "roll her own" ML code. We have carefully chosen a set of five ML implementation tasks that involve learning relatively complex, hierarchical models. We completed those tasks on four different computational platforms, and using 70,000 hours of Amazon EC2 compute time, we carefully compared running times, tuning requirements, and ease-of-programming of each.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1371–1382},
numpages = {12},
keywords = {distributed machine learning},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2588555.2595637,
author = {Soliman, Mohamed A. and Antova, Lyublena and Raghavan, Venkatesh and El-Helw, Amr and Gu, Zhongxian and Shen, Entong and Caragea, George C. and Garcia-Alvarado, Carlos and Rahman, Foyzur and Petropoulos, Michalis and Waas, Florian and Narayanan, Sivaramakrishnan and Krikellas, Konstantinos and Baldwin, Rhonda},
title = {Orca: A Modular Query Optimizer Architecture for Big Data},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2595637},
doi = {10.1145/2588555.2595637},
abstract = {The performance of analytical query processing in data management systems depends primarily on the capabilities of the system's query optimizer. Increased data volumes and heightened interest in processing complex analytical queries have prompted Pivotal to build a new query optimizer.In this paper we present the architecture of Orca, the new query optimizer for all Pivotal data management products, including Pivotal Greenplum Database and Pivotal HAWQ. Orca is a comprehensive development uniting state-of-the-art query optimization technology with own original research resulting in a modular and portable optimizer architecture.In addition to describing the overall architecture, we highlight several unique features and present performance comparisons against other systems.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {337–348},
numpages = {12},
keywords = {mpp, query optimization, parallel processing, cost model},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2612733.2612763,
author = {Steinfeld, Nili and Lev-On, Azi},
title = {"Well-Done, Mr. Mayor!": Linguistic Analysis of Municipal Facebook Pages},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2612763},
doi = {10.1145/2612733.2612763},
abstract = {The increasing use of social networks has given rise to a new kind of relations between residents and authorities at the municipal level, where residents can speak directly to administrators and representatives, can take part in open discussions, and may have more direct involvement and influence on local affairs. The more direct democracy facilitated by social media outlets fascinates communication and political science researchers. But while most of their attention is drawn to national politics, the municipal arena can be even more affected by these new means of direct communication. This paper focuses on municipal administration on Facebook, and analyzes the discourse that has developed between citizens and local administrators on municipal Facebook pages, using automatic digital tools.The formal Facebook pages of all of the cities in Israel were extracted using digital tools, and all posts and comments published on these pages in a period of six months were analyzed using automatic linguistic analysis tools that provided information regarding the use and frequencies of words and terms in the texts.The paper presents the prominent topics, use of language, and basic features of citizens--municipalities interactions in formal Facebook pages. The study discusses the findings, their implications, and the advantages and limitations of using digital tools to analyze texts in a digital research field.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {273–279},
numpages = {7},
keywords = {discourse analysis, social media, digital government, digital analysis tools, big data, municipal government, Facebook, linguistic analysis},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@inproceedings{10.1145/2588555.2612184,
author = {Sricharan, Kumar and Das, Kamalika},
title = {Localizing Anomalous Changes in Time-Evolving Graphs},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2612184},
doi = {10.1145/2588555.2612184},
abstract = {Given a time-evolving sequence of undirected, weighted graphs, we address the problem of localizing anomalous changes in graph structure over time. In this paper, we use the term `localization' to refer to the problem of identifying abnormal changes in node relationships (edges) that cause anomalous changes in graph structure. While there already exist several methods that can detect whether a graph transition is anomalous or not, these methods are not well suited for localizing the edges which are responsible for a transition being marked as an anomaly. This is a limitation in applications such as insider threat detection, where identifying the anomalous graph transitions is not sufficient, but rather, identifying the anomalous node relationships and associated nodes is key. To this end, we propose a novel, fast method based on commute time distance called CAD (Commute-time based Anomaly detection in Dynamic graphs) that detects node relationships responsible for abnormal changes in graph structure. In particular, CAD localizes anomalous edges by tracking a measure that combines information regarding changes in graph structure (in terms of commute time distance) as well as changes in edge weights. For large, sparse graphs, CAD returns a list of these anomalous edges and associated nodes in O(nlog n) time per graph instance in the sequence, where $n$ is the number of nodes. We analyze the performance of CAD on several synthetic and real-world data sets such as the Enron email network, the DBLP co-authorship network and a worldwide precipitation network data. Based on experiments conducted, we conclude that CAD consistently and efficiently identifies anomalous changes in relationships between nodes over time.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1347–1358},
numpages = {12},
keywords = {anomaly detection, temporal graphs, anomaly localization, random walks, commute time distance, dynamic graph analysis},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2588555.2610518,
author = {Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Seo, Jiwon and Park, Jongsoo and Hassaan, M. Amber and Sengupta, Shubho and Yin, Zhaoming and Dubey, Pradeep},
title = {Navigating the Maze of Graph Analytics Frameworks Using Massive Graph Datasets},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610518},
doi = {10.1145/2588555.2610518},
abstract = {Graph algorithms are becoming increasingly important for analyzing large datasets in many fields. Real-world graph data follows a pattern of sparsity, that is not uniform but highly skewed towards a few items. Implementing graph traversal, statistics and machine learning algorithms on such data in a scalable manner is quite challenging. As a result, several graph analytics frameworks (GraphLab, CombBLAS, Giraph, SociaLite and Galois among others) have been developed, each offering a solution with different programming models and targeted at different users. Unfortunately, the "Ninja performance gap" between optimized code and most of these frameworks is very large (2-30X for most frameworks and up to 560X for Giraph) for common graph algorithms, and moreover varies widely with algorithms. This makes the end-users' choice of graph framework dependent not only on ease of use but also on performance. In this work, we offer a quantitative roadmap for improving the performance of all these frameworks and bridging the "ninja gap". We first present hand-optimized baselines that get performance close to hardware limits and higher than any published performance figure for these graph algorithms. We characterize the performance of both this native implementation as well as popular graph frameworks on a variety of algorithms. This study helps end-users delineate bottlenecks arising from the algorithms themselves vs. programming model abstractions vs. the framework implementations. Further, by analyzing the system-level behavior of these frameworks, we obtain bottlenecks that are agnostic to specific algorithms. We recommend changes to alleviate these bottlenecks (and implement some of them) and reduce the performance gap with respect to native code. These changes will enable end-users to choose frameworks based mostly on ease of use.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {979–990},
numpages = {12},
keywords = {graph analytics, analysis, graphlab, cluster, frameworks, giraph, graphs, Galois, performance, combblas, socialite},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2588555.2610507,
author = {Leis, Viktor and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
title = {Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610507},
doi = {10.1145/2588555.2610507},
abstract = {With modern computer architecture evolving, two problems conspire against the state-of-the-art approaches in parallel query execution: (i) to take advantage of many-cores, all query work must be distributed evenly among (soon) hundreds of threads in order to achieve good speedup, yet (ii) dividing the work evenly is difficult even with accurate data statistics due to the complexity of modern out-of-order cores. As a result, the existing approaches for plan-driven parallelism run into load balancing and context-switching bottlenecks, and therefore no longer scale. A third problem faced by many-core architectures is the decentralization of memory controllers, which leads to Non-Uniform Memory Access (NUMA). In response, we present the morsel-driven query execution framework, where scheduling becomes a fine-grained run-time task that is NUMA-aware. Morsel-driven query processing takes small fragments of input data (morsels) and schedules these to worker threads that run entire operator pipelines until the next pipeline breaker. The degree of parallelism is not baked into the plan but can elastically change during query execution, so the dispatcher can react to execution speed of different morsels but also adjust resources dynamically in response to newly arriving queries in the workload. Further, the dispatcher is aware of data locality of the NUMA-local morsels and operator state, such that the great majority of executions takes place on NUMA-local memory. Our evaluation on the TPC-H and SSB benchmarks shows extremely high absolute performance and an average speedup of over 30 with 32 cores.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {743–754},
numpages = {12},
keywords = {morsel-driven parallelism, NUMA-awareness},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2588555.2610502,
author = {Alagiannis, Ioannis and Idreos, Stratos and Ailamaki, Anastasia},
title = {H2O: A Hands-Free Adaptive Store},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610502},
doi = {10.1145/2588555.2610502},
abstract = {Modern state-of-the-art database systems are designed around a single data storage layout. This is a fixed decision that drives the whole architectural design of a database system, i.e., row-stores, column-stores. However, none of those choices is a universally good solution; different workloads require different storage layouts and data access methods in order to achieve good performance.In this paper, we present the H2O system which introduces two novel concepts. First, it is flexible to support multiple storage layouts and data access patterns in a single engine. Second, and most importantly, it decides on-the-fly, i.e., during query processing, which design is best for classes of queries and the respective data parts. At any given point in time, parts of the data might be materialized in various patterns purely depending on the query workload; as the workload changes and with every single query, the storage and access patterns continuously adapt. In this way, H2O makes no a priori and fixed decisions on how data should be stored, allowing each single query to enjoy a storage and access pattern which is tailored to its specific properties.We present a detailed analysis of H2O using both synthetic benchmarks and realistic scientific workloads. We demonstrate that while existing systems cannot achieve maximum performance across all workloads, H2O can always match the best case performance without requiring any tuning or workload knowledge.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1103–1114},
numpages = {12},
keywords = {dynamic operators, adaptive hybrids, adaptive storage},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2588555.2610499,
author = {Okcan, Alper and Riedewald, Mirek},
title = {Anti-Combining for MapReduce},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610499},
doi = {10.1145/2588555.2610499},
abstract = {We propose Anti-Combining, a novel optimization for MapReduce programs to decrease the amount of data transferred from mappers to reducers. In contrast to Combiners, which decrease data transfer by performing reduce work on the mappers, Anti-Combining shifts mapper work to the reducers. It is also conceptually different from traditional compression techniques. While the latter are applied outside the MapReduce framework by compressing map output and then decompressing it before the data is fed into the reducer, Anti-Combining is integrated into mapping and reducing functionality itself. This enables lightweight algorithms and data reduction even for cases where the Map output data shows no redundancy that could be exploited by traditional compression techniques. Anti-Combining can be enabled automatically for any given MapReduce program through purely syntactic transformations. In some cases, in particular for certain non-deterministic Map and Partition functions, only a weaker version can be applied. At runtime the Anti-Combining enabled MapReduce program will dynamically and adaptively decrease data transfer by making fine-grained local decisions. Our experiments show that Anti-Combining can achieve data transfer reduction similar to or better than traditional compression techniques, while also reducing CPU and local I/O cost. It can even be applied in combination with them to greater effect.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {839–850},
numpages = {12},
keywords = {anti-combining, throughput optimization, MapReduce},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2612733.2612764,
author = {Gharawi, Mohammed and Estevez, Elsa and Janowski, Tomasz},
title = {Identifying Government Chief Information Officer Education and Training Needs: The Case of Saudi Arabia},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2612764},
doi = {10.1145/2612733.2612764},
abstract = {This paper identifies education and training needs of Government Chief Information Officers (GCIO) in the Kingdom of Saudi Arabia (KSA). It aims to provide foundation that would assist the KSA national e-Government program (YESSER) in identifying and prioritizing initiatives oriented towards building the capacity of GCIOs. Based on the results of a survey conducted among GCIOs and highest IT officials of 30 government agencies and the results of four semi-structured interviews, the paper identifies the knowledge areas and skills that should be developed by GCIO educational programs, the stages of public sector ICT in which GCIOs are most involved, the preferred delivery modes for the training, the preferred institutions for hosting GCIOs education and training programs, and the prerequisites for those who should participate in GCIO educational programs. In addition to the policy recommendations for the KSA Government, the main contribution of the paper is the validation of a methodology that can be applied by any government for designing capacity-building programs for their IT leaders.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {280–289},
numpages = {10},
keywords = {GCIO training, information technology (IT) leadership, chief information officer (CIO), GCIO education, government chief information officer (GCIO)},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@inproceedings{10.1145/2588555.2595634,
author = {Elmeleegy, Khaled and Olston, Christopher and Reed, Benjamin},
title = {SpongeFiles: Mitigating Data Skew in Mapreduce Using Distributed Memory},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2595634},
doi = {10.1145/2588555.2595634},
abstract = {Data skew is a major problem for data processing platforms like MapReduce. Skew causes worker tasks to spill to disk what they cannot fit in memory, which slows down the task and the overall job. Moreover, performance of other jobs sharing same disk degrades. In many cases, this situation occurs even as the cluster has plenty of spare memory it is just not used evenly. We introduce SpongeFiles, a novel distributed-memory abstraction tailored to data processing environments like MapReduce. A SpongeFile is a logical byte array, comprised of large chunks that can be stored in a variety of locations in the cluster. Spilled data goes to SpongeFiles, which route it to the nearest location with sufficient capacity (local memory, remote memory, local disk, or remote disk as a last resort). By enabling memory-sapped nodes to tap into the spare capacity of their neighbors, SpongeFiles minimize expensive disk spilling, thereby improving performance. In our experiments with Hadoop and Pig, SpongeFiles reduce overall job runtimes by up to 55% and by up to 85% under disk contention.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {551–562},
numpages = {12},
keywords = {handling data skew},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2612733.2612754,
author = {Douglas, Sara and Maruyama, Misa and Semaan, Bryan and Robertson, Scott P.},
title = {Politics and Young Adults: The Effects of Facebook on Candidate Evaluation},
year = {2014},
isbn = {9781450329019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2612733.2612754},
doi = {10.1145/2612733.2612754},
abstract = {An increasing number of people are turning to social media to find political information and discuss politics, including the technologically savvy Millennial generation. Our study looks at how young voters use social media to evaluate political candidates. Subjects were shown the Facebook walls of two U.S. politicians running for the seat of governor in the 2011 Mississippi election. Exposure was followed by semi-structured interviews to discover what knowledge they found salient. Content analysis found evidence that the knowledge they gained from Facebook influenced their evaluation of the candidates. Further, we contrast this to a control group that was exposed to related news articles without a social media component. We found that social media produced the additional voting criterion of community, which extends beyond the traditional criteria in political science literature of issues and character. Community interaction influences the vote decision.},
booktitle = {Proceedings of the 15th Annual International Conference on Digital Government Research},
pages = {196–204},
numpages = {9},
keywords = {social networking, e-participation, social media, digital democracy, e-citizenship},
location = {Aguascalientes, Mexico},
series = {dg.o '14}
}

@article{10.1145/2532644,
author = {Kazmi, Aqeel H. and O'grady, Michael J. and Delaney, Declan T. and Ruzzelli, Antonio G. and O'hare, Gregory M. P.},
title = {A Review of Wireless-Sensor-Network-Enabled Building Energy Management Systems},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/2532644},
doi = {10.1145/2532644},
abstract = {Reducing energy consumption within buildings has been an active area of research in the past decade; more recently, there has been an increased influx of activity, motivated by a variety of issues including legislative, tax-related, as well as an increased awareness of energy-related issues. Energy usage both in commercial and residential buildings represents a significant portion of overall energy consumption; however, much of this may be categorized as waste, that is, energy usage that does not fulfil a definite purpose. In the past decade, the viability of Wireless Sensor Network (WSN) technologies has been demonstrated, leading to increased possibilities for novel services for building energy management. This development has resulted in numerous approaches being proposed for harnessing WSNs for energy management and conservation. This article surveys the state-of-the-art in building energy management systems. A generic architecture is proposed after which a detailed taxonomy of existing documented systems is presented. Gaps in the literature are highlighted and directions for future research identified.},
journal = {ACM Trans. Sen. Netw.},
month = jun,
articleno = {66},
numpages = {43},
keywords = {sensor fusion and distributed inference, energy management and control, simulation tools and environments, energy usage feedback and control, Applications of sensor and actuator networks, modelling of systems and physical environments, building energy management systems}
}

@inproceedings{10.1145/2598510.2598572,
author = {Peyton, Tamara and Poole, Erika and Reddy, Madhu and Kraschnewski, Jennifer and Chuang, Cynthia},
title = {"Every Pregnancy is Different": Designing MHealth for the Pregnancy Ecology},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2598572},
doi = {10.1145/2598510.2598572},
abstract = {This paper presents the results of an ongoing study into the potential role of mobile or wireless health applications for targeting the prevention of excessive gestational weight gain in pregnant lower-income American women. Informed by a qualitative study of pregnant women's experiences, we develop a set of design requirements for designing mobile health (mHealth) interventions related to healthy pregnancies. We identify a disconnection between physical activity and food tracking application design paradigms, and the reality of pregnant women's lives and capacities. We introduce the concept of an individualized pregnancy ecology, which provides an alternative paradigm for design of health and wellness management tools for lower-income pregnant women.},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {577–586},
numpages = {10},
keywords = {requirements, pregnancy, qualitative research, pregnancy ecology, mhealth},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{10.1145/2598510.2600887,
author = {Hillman, Serena and Procyk, Jason and Neustaedter, Carman},
title = { 'Alksjdf;Lksfd': Tumblr and the Fandom User Experience},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2600887},
doi = {10.1145/2598510.2600887},
abstract = {A growing trend is the participation in online fandom communities through the support of the blogging platform Tumblr. While past research has investigated backchannels-chatter related to live entertainment on micro-blogging sites such as Twitter-there is a lack of research on the behaviours and motivations of Tumblr users. In our study, we investigate why fandom users chose Tumblr over other social networking sites, their motivations behind participating in fandoms, and how they interact within the Tumblr community. Our findings show that users face many user interface challenges when participating in Tumblr fandoms, especially initially; yet, despite this, Tumblr fandom communities thrive with a common sense of social purpose and exclusivity where users feel they can present a more authentic reflection of themselves to those sharing similar experiences and interests. We describe how this suggests design directions for social networking and blogging sites in order to promote communities of users.},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {775–784},
numpages = {10},
keywords = {backchannels, Tumblr, fandoms, entertainment, micro-blogging, social networking, television, fanfiction},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{10.1145/2598510.2598531,
author = {Hauser, Sabrina and Wakkary, Ron and Neustaedter, Carman},
title = {Understanding Guide Dog Team Interactions: Design Opportunities to Support Work and Play},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2598531},
doi = {10.1145/2598510.2598531},
abstract = {The visually impaired have been a longstanding and well-recognized user group addressed in the field of Human-Computer Interaction (HCI). Recently, the study of sighted dog owners and their pets has gained interest in HCI. Despite this, there is a noticeable gap in the field with regards to research on visually impaired owners and their dogs (guide dog teams). This paper presents a study that explores the interactions of guide dog teams revealing a rich, holistic understanding of their everyday lives and needs, across both work and leisure activities. Our findings inform and inspire future research and practices suggesting three opportunity areas: supporting working guide dog teams, enhancing play-interaction through accessible dog toys utilizing sensor technologies, and speculative and exploratory opportunities. This work contributes to the growing research on designing for human-canine teams and motivates future research with guide dog teams.},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {295–304},
numpages = {10},
keywords = {interactive systems design, team interaction, guide dogs, visually impaired, human-canine interaction, animal-computer interaction, human-animal-interaction},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{10.1145/2713609.2713612,
author = {Clarke, Dave and Clear, Tony and Fisler, Kathi and Hauswirth, Matthias and Krishnamurthi, Shriram and Politz, Joe Gibbs and Tirronen, Ville and Wrigstad, Tobias},
title = {In-Flow Peer Review},
year = {2014},
isbn = {9781450334068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713609.2713612},
doi = {10.1145/2713609.2713612},
abstract = {Peer-review is a valuable tool that helps both the reviewee, who receives feedback about his work, and the reviewer, who sees different potential solutions and improves her ability to critique work. In-flow peer-review (IFPR) is peer-review done while an assignment is in progress. Peer-review done during this time is likely to result in greater motivation for both reviewer and reviewee. This workinggroup report summarizes IFPR and discusses numerous dimensions of the process, each of which alleviates some problems while raising associated concerns.},
booktitle = {Proceedings of the Working Group Reports of the 2014 on Innovation &amp; Technology in Computer Science Education Conference},
pages = {59–79},
numpages = {21},
location = {Uppsala, Sweden},
series = {ITiCSE-WGR '14}
}

@inproceedings{10.1145/2598510.2598554,
author = {Maleki, Maryam M. and Woodbury, Robert F. and Neustaedter, Carman},
title = {Liveness, Localization and Lookahead: Interaction Elements for Parametric Design},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2598554},
doi = {10.1145/2598510.2598554},
abstract = {Scripting has become an integral part of design work in Computer-Aided Design (CAD), especially with parametric systems. Designers who script face a very steep learning and use curve due to the new (to them) script notation and the loss of direct manipulation of the model. Programming In the Model (PIM) is a prototype parametric CAD system with a live interface with side-by-side model and script windows; real-time updating of the script and the model; on-demand dependency, object and script representations in the model; and operation preview (lookahead). These features aim to break the steep learning and use curve of scripting into small steps and to bring programming and modeling tasks `closer together.' A qualitative user study with domain experts shows the importance of multi-directional live scripting and script localization within the model. Other PIM features show promise but require additional design work to create a better user experience.},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {805–814},
numpages = {10},
keywords = {creativity support tools, computer-aided design, computational design, user studies, qualitative methods, parametric design, end-user programming},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{10.1145/2598510.2598594,
author = {He, Helen Ai and Huang, Elaine M.},
title = {A Qualitative Study of Workplace Intercultural Communication Tensions in Dyadic Face-to-Face and Computer-Mediated Interactions},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2598594},
doi = {10.1145/2598510.2598594},
abstract = {We present findings from a qualitative study with 28 participants of the dyadic intercultural communication tensions professionals experience in Face-to-Face (FTF) and Computer-Mediated Communication (CMC) workplace interactions. We identify four categories of intercultural communication tensions that emerged most frequently in our dataset including range of emotional expression, level of formality, "fixed" versus flexible appointments and task versus social-orientation. We discuss how these tensions manifested in FTF and CMC media and unravel the ways media supports or hinders intercultural communication. We present the adaptations participants made to mitigate such tensions and offer implications for design. Our findings demonstrate that the most frequently occurring intercultural communication tensions manifested in both FTF and CMC, regardless of the medium used. This indicates that cultural communication challenges will persist no matter the medium, highlighting the opportunity for technologies to better support workplace intercultural communication.},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {415–424},
numpages = {10},
keywords = {computer-mediated communication, face-to-face, intercultural communication, workplace},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{10.1145/2598510.2598556,
author = {Zhang, Xiao and Wakkary, Ron},
title = {Understanding the Role of Designers' Personal Experiences in Interaction Design Practice},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2598556},
doi = {10.1145/2598510.2598556},
abstract = {Using designers' personal experiences in interaction design practice is often questioned in a predominantly rationalist practice like HCI and professional interaction design. Perhaps for this reason, little work has been conducted to investigate how designers' personal experiences can contribute to technology design. Yet it's undeniable designers have applied their personal experiences to their design practice and also benefited from such experiences. This paper reports on a multiple case study that looks at how interaction designers worked with their personal experiences in three industrial interaction design projects, thus calling for the need to explicitly recognize the legitimacy of using and better support of the use of designers' personal experiences in interaction design practice. In this study, a designer's personal experiences refer to the collections of his/her individual experiences derived from his/her direct observation or past real-life events and activities, as well as his/her interaction with design artifacts and systems whether digital or not.},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {895–904},
numpages = {10},
keywords = {designers' personal experiences, interaction design practice},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{10.1145/2598510.2600881,
author = {Ghellal, Sabiha and Morrison, Ann and Hassenzahl, Marc and Schaufler, Benjamin},
title = {The Remediation of Nosferatu: Exploring Transmedia Experiences},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2600881},
doi = {10.1145/2598510.2600881},
abstract = {In this paper we present The Remediation of Nosferatu, a location based augmented reality horror adventure. Using the theory of fictional universe elements, we work with diverse material from Nosferatu's horror genre and vampire themes as a case study. In this interdisciplinary research we intertwine traditional storytelling and scriptwriting skills with interaction design methods. For the game setting, we create hybrid spaces merging the fictional universe and the physical environment into one pervasive experience, centering around a variety of augmented reality activities played out at sunset. Focusing on the phenomenological world of 21 participants, we analyse triangulated data by distinguishing between a range of more "open" and "closed" styles of interactions. Our study illustrates how Speculative Play may enable non-linear storytelling elements within a transmedia fictional universe. We believe our approach can be more generally useful for designing future rich, enjoyable and meaningful transmedia experiences.},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {617–626},
numpages = {10},
keywords = {transmedia, fictional universe, user experience design, open &amp; closed qualities of experiences},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{10.1145/2615569.2615678,
author = {Kershaw, Daniel and Rowe, Matthew and Stacey, Patrick},
title = {Towards Tracking and Analysing Regional Alcohol Consumption Patterns in the UK through the Use of Social Media},
year = {2014},
isbn = {9781450326223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2615569.2615678},
doi = {10.1145/2615569.2615678},
abstract = {Monitoring rates of alcohol consumption across the UK is a timely problem due to ever-increasing drinking levels. This has led to calls from public services (e.g. police and health services) to assess the effect it is having on people and society. Current research methods that are utilised to assess consumption patterns are costly, time consuming, and do not supply sufficiently detailed results. This is because they look at snapshots of individuals' drinking patterns, which rely on generalised usage patterns, and post consumption recall. In this paper we look into the use of social media such as Twitter (a popular micro blogging site) to monitor the rate of alcohol consumption in regions across the UK by introducing the Social Media Alcohol Index (SMAI). By looking at the variation in term usage, and treating the social network as a spatio-temporal self-reporting sense-network, we aim to discover variation in drinking patterns on both local and national levels within the UK. This study used 31.6 million tweets collected over a 6 week period, and used the Health &amp; Social Care Information Centre (HSCIC) weekly alcohol consumption pattern as a ground truth. High correlations between the ground truth and the computed SMAI (Social Media Alcohol Index) were found on a national and local level, along with the ability to detect variation in consumption on National holidays and celebrations at both local and national levels.},
booktitle = {Proceedings of the 2014 ACM Conference on Web Science},
pages = {220–228},
numpages = {9},
keywords = {alcohol, keyword analysis, sns, trend detection, twitter},
location = {Bloomington, Indiana, USA},
series = {WebSci '14}
}

@inproceedings{10.1145/2713609.2713611,
author = {Brusilovsky, Peter and Edwards, Stephen and Kumar, Amruth and Malmi, Lauri and Benotti, Luciana and Buck, Duane and Ihantola, Petri and Prince, Rikki and Sirki\"{a}, Teemu and Sosnovsky, Sergey and Urquiza, Jaime and Vihavainen, Arto and Wollowski, Michael},
title = {Increasing Adoption of Smart Learning Content for Computer Science Education},
year = {2014},
isbn = {9781450334068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2713609.2713611},
doi = {10.1145/2713609.2713611},
abstract = {Computer science educators are increasingly using interactive learning content to enrich and enhance the pedagogy of their courses. A plethora of such learning content, specifically designed for computer science education, such as visualization, simulation, and web-based environments for learning programming, are now available for various courses. We call such content smart learning content. However, such learning content is seldom used outside its host site despite the benefits it could offer to learners everywhere. In this paper, we investigate the factors that impede dissemination of such content among the wider computer science education community. To accomplish this we surveyed educators, existing tools and recent research literature to identify the current state of the art and analyzed the characteristics of a large number of smart learning content examples along canonical dimensions. In our analysis we focused on examining the technical issues that must be resolved to support finding, integrating and customizing smart learning content in computer science courses. Finally, we propose a new architecture for hosting, integrating and disseminating smart learning content and discuss how it could be implemented based on existing protocols and standards.},
booktitle = {Proceedings of the Working Group Reports of the 2014 on Innovation &amp; Technology in Computer Science Education Conference},
pages = {31–57},
numpages = {27},
keywords = {teaching with technology, technology adoption, smart learning content, classroom management, intelligent tutoring systems, dissemination, educational research, technology integration, educational tools, computer science education},
location = {Uppsala, Sweden},
series = {ITiCSE-WGR '14}
}

@inproceedings{10.1145/2608029.2608032,
author = {Riteau, Pierre and Hwang, Myunghwa and Padmanabhan, Anand and Gao, Yizhao and Liu, Yan and Keahey, Kate and Wang, Shaowen},
title = {A Cloud Computing Approach to On-Demand and Scalable Cybergis Analytics},
year = {2014},
isbn = {9781450329118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2608029.2608032},
doi = {10.1145/2608029.2608032},
abstract = {Spatial data analysis has become ubiquitous as geographic information systems (GIS) are widely used to support scientific investigations and decision making in many fields of science, engineering, and humanities (e.g., ecology, emergency management, environmental engineering and sciences, geosciences, and social sciences). Tremendous data and computational capabilities are needed to handle and analyze massive quantities of spatial data that are collected across multiple spatiotemporal scales and used for diverse purposes. CyberGIS has emerged as a new-generation GIS based on advanced cyberinfrastructure to seamlessly integrate such capabilities into scalable geospatial analytics and modeling tools. One of the key challenges and opportunities of CyberGIS research is to build an on-demand service framework that can manage underlying cyberinfrastructure resources dynamically, in order to provide responsive support for interactive online CyberGIS analytics for which users can generate massive service requests in a short amount of time. This paper presents a cloud computing approach to implementing CyberGIS analytics using cloud computing services in the CyberGIS Gateway, a multiuser and collaborative online problem-solving environment. The primary purpose of this research is to address the question of how to achieve on-demand and scalable CyberGIS analytics that provide a stable response time to the user. We do that through integration with the Nimbus Phantom cloud platform. We then investigate how the cloud platform is able to adaptively handle fluctuating requests for analytics while providing a stable response time.},
booktitle = {Proceedings of the 5th ACM Workshop on Scientific Cloud Computing},
pages = {17–24},
numpages = {8},
keywords = {geographic information systems (gis), cloud computing, cybergis, auto-scaling},
location = {Vancouver, BC, Canada},
series = {ScienceCloud '14}
}

@inproceedings{10.1145/2600212.2600229,
author = {Li, Min and Zeng, Liangzhao and Meng, Shicong and Tan, Jian and Zhang, Li and Butt, Ali R. and Fuller, Nicholas},
title = {MRONLINE: MapReduce Online Performance Tuning},
year = {2014},
isbn = {9781450327497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600212.2600229},
doi = {10.1145/2600212.2600229},
abstract = {MapReduce job parameter tuning is a daunting and time consuming task. The parameter configuration space is huge; there are more than 70 parameters that impact job performance. It is also difficult for users to determine suitable values for the parameters without first having a good understanding of the MapReduce application characteristics. Thus, it is a challenge to systematically explore the parameter space and select a near-optimal configuration. Extant offline tuning approaches are slow and inefficient as they entail multiple test runs and significant human effort.To this end, we propose an online performance tuning system, MRONLINE, that monitors a job's execution, tunes associated performance-tuning parameters based on collected statistics, and provides fine-grained control over parameter configuration. MRONLINE allows each task to have a different configuration, instead of having to use the same configuration for all tasks. Moreover, we design a gray-box based smart hill climbing algorithm that can efficiently converge to a near-optimal configuration with high probability. To improve the search quality and increase convergence speed, we also incorporate a set of MapReduce-specific tuning rules in MRONLINE. Our results using a real implementation on a representative 19-node cluster show that dynamic performance tuning can effectively improve MapReduce application performance by up to 30% compared to the default configuration used in YARN.},
booktitle = {Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {165–176},
numpages = {12},
keywords = {online parameter tuning, yarn, mapreduce, hill climbing, performance tuning, dynamic parameter configuration, cloud computing},
location = {Vancouver, BC, Canada},
series = {HPDC '14}
}

@inproceedings{10.1145/2608020.2608027,
author = {Filgueira, Rosa and Klampanos, Iraklis and Tanimura, Yusuke and Atkinson, Malcolm},
title = {FAST: Flexible Automated Synchronization Transfer},
year = {2014},
isbn = {9781450329132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2608020.2608027},
doi = {10.1145/2608020.2608027},
abstract = {This paper presents a new data synchronizing transfer tool called FAST (Flexible Automated Synchronization Transfer) which allows facilities for reliably transferring multi-channel data and metadata periodically from rock physics experiments to a repository and a database located in a remote machine. FAST is compatible with all operating systems, and allows for different types of transfer opera- tions by selecting a different transfer protocol. FAST is very easy to set up and requires little oversight during operation. It copes automatically with interruptions to local operations and communications.},
booktitle = {Proceedings of the Sixth International Workshop on Data Intensive Distributed Computing},
pages = {47–52},
numpages = {6},
keywords = {data streaming, data intensive computing, distributed system, data transfer, geoscience},
location = {Vancouver, BC, Canada},
series = {DIDC '14}
}

@inproceedings{10.1145/2602299.2602307,
author = {Valuch, Christian and Ansorge, Ulrich and Buchinger, Shelley and Patrone, Aniello Raffaele and Scherzer, Otmar},
title = {The Effect of Cinematic Cuts on Human Attention},
year = {2014},
isbn = {9781450328388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602299.2602307},
doi = {10.1145/2602299.2602307},
abstract = {Understanding the factors that determine human attention in videos is important for many applications, such as user interface design in interactive television (iTV), continuity editing, or data compression techniques. In this article, we identify the demands that cinematic cuts impose on human attention. We hypothesize, test, and confirm that after cuts the viewers' attention is quickly attracted by repeated visual content. We conclude with a recommendation for future models of visual attention in videos and make suggestions how the present results could inspire designers of second screen iTV applications to optimise their interfaces with regard to a maximally smooth viewing experience.},
booktitle = {Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video},
pages = {119–122},
numpages = {4},
keywords = {saccades, attention, evaluation, editing, eye tracking, second screen applications, continuity},
location = {Newcastle Upon Tyne, United Kingdom},
series = {TVX '14}
}

@inproceedings{10.1145/2602299.2602313,
author = {Abreu, Jorge and Almeida, Pedro and Teles, Bruno},
title = {TV Discovery &amp; Enjoy: A New Approach to Help Users Finding the Right TV Program to Watch},
year = {2014},
isbn = {9781450328388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602299.2602313},
doi = {10.1145/2602299.2602313},
abstract = {This paper presents the development and evaluation cycle of an interactive television (iTV) prototype that aims to improve the way users discover and select their TV content, bearing in mind the cognitive model that the viewer typically uses in mindless zapping situations. The development of the iTV application was supported by a study of the habits and behaviours of TV viewers (namely the ones related to the referred cognitive process), followed by the specification of its conceptual model and features, interface mock-ups and its integration in the filtering engine of the iTV application. Additionally, an indexing and cataloging system interconnected with the filtering engine was designed. The developed prototype was evaluated by a group of users, with the results revealing to be very positive, both in in what relates with the interest on the application and its usability. In parallel to the development of the iTV application, a tablet version was conceptualized and evaluated with the aim of studying the suitability of the extension of the same conceptual model to a secondary screen approach.},
booktitle = {Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video},
pages = {63–70},
numpages = {8},
keywords = {usability tests, interaction, prototype, epg, iptv, interactive television},
location = {Newcastle Upon Tyne, United Kingdom},
series = {TVX '14}
}

@inproceedings{10.1145/2602299.2602312,
author = {Geerts, David and Leenheer, Rinze and De Grooff, Dirk and Negenman, Joost and Heijstraten, Susanne},
title = {In Front of and behind the Second Screen: Viewer and Producer Perspectives on a Companion App},
year = {2014},
isbn = {9781450328388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602299.2602312},
doi = {10.1145/2602299.2602312},
abstract = {The growing success of tablets and smartphones has shifted the focus of the interactive TV industry to the introduction of second screen applications. One example is second screen companion apps that offer extra information about a television program, often synchronized with what happens on screen. In this paper, we investigate a second screen companion app, from the perspective of the viewers and producers of such apps. Based on observations and interviews with viewers and producers, and actual usage data of a companion app from Google Analytics, we present several insights and recommendations for how to design companion apps related to ease of use, timing, social interaction, attention and added value.},
booktitle = {Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video},
pages = {95–102},
numpages = {8},
keywords = {second screen, interactive tv, user experience, producers},
location = {Newcastle Upon Tyne, United Kingdom},
series = {TVX '14}
}

@inproceedings{10.1145/2618243.2618252,
author = {Galpin, Ixent and Stokes, Alan B. and Valkanas, George and Gray, Alasdair J. G. and Paton, Norman W. and Fernandes, Alvaro A. A. and Sattler, Kai-Uwe and Gunopulos, Dimitrios},
title = {SensorBench: Benchmarking Approaches to Processing Wireless Sensor Network Data},
year = {2014},
isbn = {9781450327220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2618243.2618252},
doi = {10.1145/2618243.2618252},
abstract = {Wireless sensor networks enable cost-effective data collection for tasks such as precision agriculture and environment monitoring. However, the resource-constrained nature of sensor nodes, which often have both limited computational capabilities and battery lifetimes, means that applications that use them must make judicious use of these resources. Research that seeks to support data intensive sensor applications has explored a range of approaches and developed many different techniques, including bespoke algorithms for specific analyses and generic sensor network query processors. However, all such proposals sit within a multi-dimensional design space, where it can be difficult to understand the implications of specific decisions and to identify optimal solutions. This paper presents a benchmark that seeks to support the systematic analysis and comparison of different techniques and platforms, enabling both development and user communities to make well informed choices. The contributions of the paper include: (i) the identification of key variables and performance metrics; (ii) the specification of experiments that explore how different types of task perform under different metrics for the controlled variables; and (iii) an application of the benchmark to investigate the behavior of several representative platforms and techniques.},
booktitle = {Proceedings of the 26th International Conference on Scientific and Statistical Database Management},
articleno = {21},
numpages = {12},
keywords = {benchmarks, query processing, wireless sensor networks},
location = {Aalborg, Denmark},
series = {SSDBM '14}
}

@inproceedings{10.1145/2618243.2618269,
author = {Assam, Roland and Hassani, Marwan and Brysch, Michael and Seidl, Thomas},
title = {(K, d)-Core Anonymity: Structural Anonymization of Massive Networks},
year = {2014},
isbn = {9781450327220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2618243.2618269},
doi = {10.1145/2618243.2618269},
abstract = {Networks entail vulnerable and sensitive information that pose serious privacy threats. In this paper, we introduce, k-core attack, a new attack model which stems from the k-core decomposition principle. K-core attack undermines the privacy of some state-of-the-art techniques. We propose a novel structural anonymization technique called (k, δ)-Core Anonymity, which harnesses the k-core attack and structurally anonymizes small and large networks. In addition, although real-world social networks are massive in nature, most existing works focus on the anonymization of networks with less than one hundred thousand nodes. (k, δ)-Core Anonymity is tailored for massive networks. To the best of our knowledge, this is the first technique that provides empirical studies on structural network anonymization for massive networks. Using three real and two synthetic datasets, we demonstrate the effectiveness of our technique on small and large networks with up to 1.7 million nodes and 17.8 million edges. Our experiments reveal that our approach outperforms a state-of-the-art work in several aspects.},
booktitle = {Proceedings of the 26th International Conference on Scientific and Statistical Database Management},
articleno = {17},
numpages = {12},
keywords = {social network privacy, privacy, k-core decomposition},
location = {Aalborg, Denmark},
series = {SSDBM '14}
}

@article{10.1145/2611567,
author = {Jagadish, H. V. and Gehrke, Johannes and Labrinidis, Alexandros and Papakonstantinou, Yannis and Patel, Jignesh M. and Ramakrishnan, Raghu and Shahabi, Cyrus},
title = {Big Data and Its Technical Challenges},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2611567},
doi = {10.1145/2611567},
abstract = {Exploring the inherent technical challenges in realizing the potential of Big Data.},
journal = {Commun. ACM},
month = jul,
pages = {86–94},
numpages = {9}
}

@article{10.1145/2597892,
author = {Kostkova, Patty and Szomszor, Martin and St. Louis, Connie},
title = {#swineflu: The Use of Twitter as an Early Warning and Risk Communication Tool in the 2009 Swine Flu Pandemic},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2597892},
doi = {10.1145/2597892},
abstract = {The need to improve population monitoring and enhance surveillance of infectious diseases has never been more pressing. Factors such as air travel act as a catalyst in the spread of new and existing viruses. The unprecedented user-generated activity on social networks over the last few years has created real-time streams of personal data that provide an invaluable tool for monitoring and sampling large populations. Epidemic intelligence relies on constant monitoring of online media sources for early warning, detection, and rapid response; however, the real-time information available in social networks provides a new paradigm for the early warning function.The communication of risk in any public health emergency is a complex task for governments and healthcare agencies. This task is made more challenging in the current situation when the public has access to a wide range of online resources, ranging from traditional news channels to information posted on blogs and social networks. Twitter’s strength is its two-way communication nature --- both as an information source but also as a central hub for publishing, disseminating and discovering online media.This study addresses these two challenges by investigating the role of Twitter during the 2009 swine flu pandemic by analysing data collected from the SN, and by Twitter using the opposite way for dissemination information through the network. First, we demonstrate the role of the social network for early warning by detecting an upcoming spike in an epidemic before the official surveillance systems by up to two weeks in the U.K. and up to two to three weeks in the U.S. Second, we illustrate how online resources are propagated through Twitter at the time of the WHO’s declaration of the swine flu “pandemic”. Our findings indicate that Twitter does favour reputable t bogus information can still leak into the network.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {8},
numpages = {25},
keywords = {swine flu 2009 analysis, data mining, monitoring spread of disease, Twitter, global health and well-being, social media, Epidemic intelligence, real-time data management and public health response}
}

@article{10.1145/2656870.2656876,
author = {Cohen, R. and Lam, D. Y. and Agarwal, N. and Cormier, M. and Jagdev, J. and Jin, T. and Kukreti, M. and Liu, J. and Rahim, K. and Rawat, R. and Sun, W. and Wang, D. and Wexler, M.},
title = {Using Computer Technology to Address the Problem of Cyberbullying},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0095-2737},
url = {https://doi.org/10.1145/2656870.2656876},
doi = {10.1145/2656870.2656876},
abstract = {The issue of cyberbullying is a social concern that has arisen due to the prevalent use of computer technology today. In this paper, we present a multi-faceted solution to mitigate the effects of cyberbullying, one that uses computer technology in order to combat the problem. We propose to provide assistance for various groups affected by cyberbullying (the bullied and the bully, both). Our solution was developed through a series of group projects and includes i) technology to detect the occurrence of cyberbullying ii) technology to enable reporting of cyberbullying iii) proposals to integrate third-party assistance when cyberbullying is detected iv) facilities for those with authority to manage online social networks or to take actions against detected bullies. In all, we demonstrate how this important social problem which arises due to computer technology can also leverage computer technology in order to take steps to better cope with the undesirable effects that have arisen.},
journal = {SIGCAS Comput. Soc.},
month = jul,
pages = {52–61},
numpages = {10},
keywords = {social network, cyberbullying, education, online safety}
}

@article{10.1145/2656870.2656875,
author = {Heron, Michael James and Belford, Pauline},
title = {Ethics in Context: A Scandal in Academia},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0095-2737},
url = {https://doi.org/10.1145/2656870.2656875},
doi = {10.1145/2656870.2656875},
abstract = {The delivery of ethical instruction within formal educational contexts is a task that is fraught with difficulties. Real world situations and examples of misconduct abound, but sourcing sufficient material within the constraints associated with developing course materials can be time-consuming. The availability of resources to illustrate relevant aspects may not be available, or may not fully emphasize the issues that educators wish to incorporate into their discussion of the material. At best, such an approach can only highlight in isolation - larger, overarching connections are rarely available. The provision of ethical instruction is now a core aspect of many undergraduate and postgraduate courses across a variety of disciplines, and accreditation bodies often include meeting the need for instruction in ethical and professional issues as a pre-requisite. In this paper we present a wide-ranging ethical case study called 'A Scandal in Academia'. It is a spiritual successor to the Case of the Killer Robot first published in the very early 1990s. The Scandal in Academia study has been trialed with students at all levels of the undergraduate curriculum and has been very effective in offering a coherent jumping-off point for a discussion of the implications of ethical and unethical activity. It is hoped that the provision of this study will be a useful tool for educators and others looking to investigate and present issues of professional responsibility within formal and informal contexts.},
journal = {SIGCAS Comput. Soc.},
month = jul,
pages = {20–51},
numpages = {32},
keywords = {human factors, morality, ethics, academia, killer robot, case study, professional issues}
}

@article{10.1145/2626290,
author = {Kalyvianaki, Evangelia and Charalambous, Themistoklis and Hand, Steven},
title = {Adaptive Resource Provisioning for Virtualized Servers Using Kalman Filters},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/2626290},
doi = {10.1145/2626290},
abstract = {Resource management of virtualized servers in data centers has become a critical task, since it enables cost-effective consolidation of server applications. Resource management is an important and challenging task, especially for multitier applications with unpredictable time-varying workloads. Work in resource management using control theory has shown clear benefits of dynamically adjusting resource allocations to match fluctuating workloads. However, little work has been done toward adaptive controllers for unknown workload types. This work presents a new resource management scheme that incorporates the Kalman filter into feedback controllers to dynamically allocate CPU resources to virtual machines hosting server applications. We present a set of controllers that continuously detect and self-adapt to unforeseen workload changes. Furthermore, our most advanced controller also self-configures itself without any a priori information and with a small 4.8% performance penalty in the case of high-intensity workload changes. In addition, our controllers are enhanced to deal with multitier server applications: by using the pair-wise resource coupling between tiers, they improve server response to large workload increases as compared to controllers with no such resource-coupling mechanism. Our approaches are evaluated and their performance is illustrated on a 3-tier Rubis benchmark website deployed on a prototype Xen-virtualized cluster.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jul,
articleno = {10},
numpages = {35},
keywords = {Kalman filter, feedback control, virtual machines, multitier server applications, resource management}
}

@article{10.14778/2732967.2732975,
author = {Altowim, Yasser and Kalashnikov, Dmitri V. and Mehrotra, Sharad},
title = {Progressive Approach to Relational Entity Resolution},
year = {2014},
issue_date = {July 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732967.2732975},
doi = {10.14778/2732967.2732975},
abstract = {This paper proposes a progressive approach to entity resolution (ER) that allows users to explore a trade-off between the resolution cost and the achieved quality of the resolved data. In particular, our approach aims to produce the highest quality result given a constraint on the resolution budget, specified by the user. Our proposed method monitors and dynamically reassesses the resolution progress to determine which parts of the data should be resolved next and how they should be resolved. The comprehensive empirical evaluation of the proposed approach demonstrates its significant advantage in terms of efficiency over the traditional ER techniques for the given problem settings.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {999–1010},
numpages = {12}
}

@inproceedings{10.1145/2600428.2609613,
author = {Mishra, Nina and White, Ryen W. and Ieong, Samuel and Horvitz, Eric},
title = {Time-Critical Search},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609613},
doi = {10.1145/2600428.2609613},
abstract = {We study time-critical search, where users have urgent information needs in the context of an acute problem. As examples, users may need to know how to stem a severe bleed, help a baby who is choking on a foreign object, or respond to an epileptic seizure. While time-critical situations and actions have been studied in the realm of decision-support systems, little has been done with time-critical search and retrieval, and little direct support is offered by search systems. Critical challenges with time-critical search include accurately inferring when users have urgent needs and providing relevant information that can be understood and acted upon quickly. We leverage surveys and search log data from a large mobile search provider to (a) characterize the use of search engines for time-critical situations, and (b) develop predictive models to accurately predict urgent information needs, given a query and a diverse set of features spanning topical, temporal, behavioral, and geospatial attributes. The methods and findings highlight opportunities for extending search and retrieval to consider the urgency of queries.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {747–756},
numpages = {10},
keywords = {search in medical emergencies},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/2600428.2609618,
author = {Li, Cheng and Wang, Yue and Resnick, Paul and Mei, Qiaozhu},
title = {ReQ-ReC: High Recall Retrieval with Query Pooling and Interactive Classification},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609618},
doi = {10.1145/2600428.2609618},
abstract = {We consider a scenario where a searcher requires both high precision and high recall from an interactive retrieval process. Such scenarios are very common in real life, exemplified by medical search, legal search, market research, and literature review. When access to the entire data set is available, an active learning loop could be used to ask for additional relevance feedback labels in order to refine a classifier. When data is accessed via search services, however, only limited subsets of the corpus can be considered, subsets defined by queries. In that setting, relevance feedback has been used in a query enhancement loop that updates a query.We describe and demonstrate the effectiveness of ReQ-ReC (ReQuery-ReClassify), a double-loop retrieval system that combines iterative expansion of a query set with iterative refinements of a classifier. This permits a separation of concerns, where the query selector's job is to enhance recall while the classifier's job is to maximize precision on the items that have been retrieved by any of the queries so far. The overall process alternates between the query enhancement loop, to increase recall, and the classifier refinement loop, to increase precision. The separation allows the query enhancement process to explore larger parts of the query space. Our experiments show that this distribution of work significantly outperforms previous relevance feedback methods that rely on a single ranking function to balance precision and recall.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {163–172},
numpages = {10},
keywords = {active learning, query expansion, relevance feedback},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@article{10.1145/2611563,
author = {Panerati, Jacopo and Maggio, Martina and Carminati, Matteo and Sironi, Filippo and Triverio, Marco and Santambrogio, Marco D.},
title = {Coordination of Independent Loops in Self-Adaptive Systems},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/2611563},
doi = {10.1145/2611563},
abstract = {Nowadays, the same piece of code should run on different architectures, providing performance guarantees in a variety of environments and situations. To this end, designers often integrate existing systems with ad-hoc adaptive strategies able to tune specific parameters that impact performance or energy—for example, frequency scaling. However, these strategies interfere with one another and unpredictable performance degradation may occur due to the interaction between different entities. In this article, we propose a software approach to reconfiguration when different strategies, called loops, are encapsulated in the system and are available to be activated. Our solution to loop coordination is based on machine learning and it selects a policy for the activation of loops inside of a system without prior knowledge. We implemented our solution on top of GNU/Linux and evaluated it with a significant subset of the PARSEC benchmark suite.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = jul,
articleno = {12},
numpages = {16},
keywords = {autonomic manager, performance management, Autonomic computing}
}

@inproceedings{10.5555/2685617.2685642,
author = {Valenzuela, Michael L. and Rozenblit, Jerzy W. and Hamilton, Allan J.},
title = {A Predictive Analytics Toolbox for Medical Applications},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Ever more frequently business enterprises are benefiting from the collection and analysis of large data. This paper reports on a work in progress of adapting intelligence, predictive analytics tools for analysis of medical data. While large data has been used in medical research, only recently has this become a trend for hospital management. A suite of tools originally developed for military intelligence analysts are repurposed for hospital management. The original design concepts is reviewed, its medical applications and challenges are described along with an illustrative example.},
booktitle = {Proceedings of the 2014 Summer Simulation Multiconference},
articleno = {25},
numpages = {8},
keywords = {decision support, healthcare, management, visualization},
location = {Monterey, California},
series = {SummerSim '14}
}

@inproceedings{10.5555/2685617.2685643,
author = {Parvaneh, Saman and Bhatnagar, Sugam and Poston, Robert and Najafi, Bijan},
title = {Using Cadaver Simulation to Improve Communication and Economy of Movement as Evidence of Progress with the Trans-Catheter Aortic Valve Implantation (TAVI) Learning Curve},
year = {2014},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Trans-catheter Aortic Valve Implantation (TAVI) is an endovascular treatment for critical aortic stenosis. An early "learning curve" is important drawback of TAVI that can dramatically alter patient safety and hospital costs. There is paucity of data regarding the quantification of learning experience associated with the technique. The aim of this study was to assess learning curve in clinical staff involved in the implementation of TAVI. We hypothesize that training sessions will improve economy of movement (EoM) as characterized by measuring jerkiness of trunk movement in medial-lateral direction. Using wearable technology based on tri-axial accelerometer, the EoM of a scrub technician, who was naive to TAVI procedure, was assessed during 6 consecutive TAVI procedures, including 4 cadaver TAVI simulations and 2 clinical cases. In addition, communication errors between the surgery team were monitored. During the cadaver simulation training, the communication error was reduced by 46.2%. Similarly, the EoM was enhanced by 17.3% (p=0.04) with a non-significant reduction rate of 4.99±4.26% per practice. Interestingly, both communication errors and EoM were reduced during real cases compared to the first simulated case, suggesting that cadaver simulation translated into real cases. This proof of concept study suggests that EoM could be used as an alternative method to objectively evaluate the learning curve during surgical procedures. Results should be confirmed in a larger sample size in both physicians and support staff assisting with TAVI procedure.},
booktitle = {Proceedings of the 2014 Summer Simulation Multiconference},
articleno = {26},
numpages = {7},
keywords = {robotic surgery, wearable technology, TAVI, learning curve, economy of movement, trans-catheter aortic valve implantation},
location = {Monterey, California},
series = {SummerSim '14}
}

@inproceedings{10.1145/2628194.2628249,
author = {Liu, Xiufeng and Thomsen, Christian and Pedersen, Torben Bach},
title = {CloudETL: Scalable Dimensional ETL for Hive},
year = {2014},
isbn = {9781450326278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628194.2628249},
doi = {10.1145/2628194.2628249},
abstract = {Extract-Transform-Load (ETL) programs process data into data warehouses (DWs). Rapidly growing data volumes demand systems that scale out. Recently, much attention has been given to MapReduce for parallel handling of massive data sets in cloud environments. Hive is the most widely used RDBMS-like system for DWs on MapReduce and provides scalable analytics. It is, however, challenging to do proper dimensional ETL processing with Hive; e.g., the concept of slowly changing dimensions (SCDs) is not supported (and due to lacking support for UPDATEs, SCDs are complex to handle manually). Also the powerful Pig platform for data processing on MapReduce does not support such dimensional ETL processing. To remedy this, we present the ETL framework CloudETL which uses Hadoop to parallelize ETL execution and to process data into Hive. The user defines the ETL process by means of high-level constructs and transformations and does not have to worry about technical MapReduce details. CloudETL supports different dimensional concepts such as star schemas and SCDs. We present how CloudETL works and uses different performance optimizations including a purpose-specific data placement policy to co-locate data. Further, we present a performance study and compare with other cloud-enabled systems. The results show that CloudETL scales very well and outperforms the dimensional ETL capabilities of Hive both with respect to performance and programmer productivity. For example, Hive uses 3.9 times as long to load an SCD in an experiment and needs 112 statements while CloudETL only needs 4.},
booktitle = {Proceedings of the 18th International Database Engineering &amp; Applications Symposium},
pages = {195–206},
numpages = {12},
keywords = {hive, ETL, MapReduce},
location = {Porto, Portugal},
series = {IDEAS '14}
}

@inproceedings{10.1145/2616498.2616531,
author = {Merritt, Alexander and Farooqui, Naila and Slawinska, Magdalena and Gavrilovska, Ada and Schwan, Karsten and Gupta, Vishakha},
title = {Slices: Provisioning Heterogeneous HPC Systems},
year = {2014},
isbn = {9781450328937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2616498.2616531},
doi = {10.1145/2616498.2616531},
abstract = {High-end computing systems are becoming increasingly heterogeneous, with nodes comprised of multiple CPUs and accelerators, like GPGPUs, and with potential additional heterogeneity in memory configurations and network connectivities. Further, as we move to exascale systems, the view of their future use is one in which simulations co-run with online analytics or visualization methods, or where a high fidelity simulation may co-run with lower order methods and/or with programs performing uncertainty quantification. To explore and understand the challenges when multiple applications are mapped to heterogeneous machine resources, our research has developed methods that make it easy to construct 'virtual hardware platforms' comprised of sets of CPUs and GPGPUs custom-configured for applications when and as required. Specifically, the 'slicing' runtime presented in this paper manages for each application a set of resources, and at any one time, multiple such slices operate on shared underlying hardware. This paper describes the slicing abstraction and its ability to configure cluster hardware resources. It experiments with application scale-out, focusing on their computationally intensive GPGPU-based computations, and it evaluates cluster-level resource sharing across multiple slices on the Keeneland machine, an XSEDE resource.},
booktitle = {Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
articleno = {46},
numpages = {8},
keywords = {resource slice, assembly, GPGPU virtualization, vgpu},
location = {Atlanta, GA, USA},
series = {XSEDE '14}
}

@inproceedings{10.1145/2616498.2616554,
author = {Samuel, Tabitha K. and Memon, Shahbaz and Scheuller, Bernd and Smallen, Shava},
title = {UNICORE in XSEDE: Through Development, Integration, Deployment and Beyond},
year = {2014},
isbn = {9781450328937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2616498.2616554},
doi = {10.1145/2616498.2616554},
abstract = {In this paper we discuss the implementation of UNICORE in XSEDE. UNICORE is a Grid middleware tool that was identified by XSEDE to further the areas of remote job submission, campus bridging and workflows. We talk about the overall architecture of UNICORE, a typical HPC environment at XSEDE and why UNICORE is a good fit for this environment. We also discuss the initial efforts made by the UNICORE development team as well as XSEDE's Software development team to integrate UNICORE into the XSEDE landscape. We detail how UNICORE went through the XSEDE engineering process and highlight deployment details at XSEDE. We touch upon how UNICORE is beneficial to the HPC user community. In our final section we talk about future efforts to better integrate UNICORE within XSEDE.},
booktitle = {Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
articleno = {64},
numpages = {8},
keywords = {Globus, XSEDE, Remote Job Submission, Grid middleware, Software Engineering, UNICORE, Data Management},
location = {Atlanta, GA, USA},
series = {XSEDE '14}
}

@article{10.1145/2601097.2601148,
author = {Templin, Krzysztof and Didyk, Piotr and Myszkowski, Karol and Hefeeda, Mohamed M. and Seidel, Hans-Peter and Matusik, Wojciech},
title = {Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2601097.2601148},
doi = {10.1145/2601097.2601148},
abstract = {Sudden temporal depth changes, such as cuts that are introduced by video edits, can significantly degrade the quality of stereoscopic content. Since usually not encountered in the real world, they are very challenging for the audience. This is because the eye vergence has to constantly adapt to new disparities in spite of conflicting accommodation requirements. Such rapid disparity changes may lead to confusion, reduced understanding of the scene, and overall attractiveness of the content. In most cases the problem cannot be solved by simply matching the depth around the transition, as this would require flattening the scene completely. To better understand this limitation of the human visual system, we conducted a series of eye-tracking experiments. The data obtained allowed us to derive and evaluate a model describing adaptation of vergence to disparity changes on a stereoscopic display. Besides computing user-specific models, we also estimated parameters of an average observer model. This enables a range of strategies for minimizing the adaptation time in the audience.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {145},
numpages = {8},
keywords = {eye-tracking, binocular, S3D}
}

@inproceedings{10.1145/2614028.2615416,
author = {McNamara, Ann and Mania, Katerina and Koulieris, George and Itti, Laurent},
title = {Attention-Aware Rendering, Mobile Graphics and Games},
year = {2014},
isbn = {9781450329620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2614028.2615416},
doi = {10.1145/2614028.2615416},
booktitle = {ACM SIGGRAPH 2014 Courses},
articleno = {6},
numpages = {119},
location = {Vancouver, Canada},
series = {SIGGRAPH '14}
}

@inproceedings{10.1145/2614028.2615401,
author = {Mitra, Niloy J. and Wand, Michael and Zhang, Hao and Cohen-Or, Daniel and Kim, Vladimir and Huang, Qi-Xing},
title = {Structure-Aware Shape Processing},
year = {2014},
isbn = {9781450329620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2614028.2615401},
doi = {10.1145/2614028.2615401},
booktitle = {ACM SIGGRAPH 2014 Courses},
articleno = {13},
numpages = {21},
location = {Vancouver, Canada},
series = {SIGGRAPH '14}
}

@article{10.1145/2656877.2656889,
author = {Anderson, Tom and Birman, Ken and Broberg, Robert and Caesar, Matthew and Comer, Douglas and Cotton, Chase and Freedman, Michael J. and Haeberlen, Andreas and Ives, Zachary G. and Krishnamurthy, Arvind and Lehr, William and Loo, Boon Thau and Mazi\`{e}res, David and Nicolosi, Antonio and Smith, Jonathan M. and Stoica, Ion and van Renesse, Robbert and Walfish, Michael and Weatherspoon, Hakim and Yoo, Christopher S.},
title = {A Brief Overview of the NEBULA Future Internet Architecture},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2656877.2656889},
doi = {10.1145/2656877.2656889},
abstract = {Nebula is a proposal for a Future Internet Architecture. It is based on the assumptions that: (1) cloud computing will comprise an increasing fraction of the application workload offered to an Internet, and (2) that access to cloud computing resources will demand new architectural features from a network. Features that we have identified include dependability, security, flexibility and extensibility, the entirety of which constitute resilience. Nebula provides resilient networking services using ultrareliable routers, an extensible control plane and use of multiple paths upon which arbitrary policies may be enforced. We report on a prototype system, Zodiac, that incorporates these latter two features.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = jul,
pages = {81–86},
numpages = {6},
keywords = {internet, extensibility, security, routing, network architecture}
}

@article{10.1145/2629330,
author = {Anagnostopoulos, Christos and Hadjiefthymiades, Stathes},
title = {Advanced Principal Component-Based Compression Schemes for Wireless Sensor Networks},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/2629330},
doi = {10.1145/2629330},
abstract = {This article proposes two models that improve the Principal Component-based Context Compression (PC3) model for contextual information forwarding among sensor nodes in a Wireless Sensor Network (WSN). The proposed models (referred to as iPC3 and oPC3) address issues associated with the control of multivariate contextual information transmission in a stationary WSN. Because WSN nodes are typically battery equipped, the primary design goal of the models is to optimize the amount of energy used for data transmission while retaining data accuracy at high levels. The proposed energy conservation techniques and algorithms are based on incremental principal component analysis and optimal stopping theory. iPC3 and oPC3 models are presented and compared with PC3 and other models found in the literature through simulations. The proposed models manage to extend the lifetime of a WSN application by improving energy efficiency within WSN.},
journal = {ACM Trans. Sen. Netw.},
month = jul,
articleno = {7},
numpages = {34},
keywords = {context compression, Energy efficiency, optimal stopping theory, wireless sensor networks, incremental principal component analysis}
}

@inproceedings{10.1145/2632320.2632347,
author = {Politz, Joe Gibbs and Krishnamurthi, Shriram and Fisler, Kathi},
title = {In-Flow Peer-Review of Tests in Test-First Programming},
year = {2014},
isbn = {9781450327558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2632320.2632347},
doi = {10.1145/2632320.2632347},
abstract = {Test-first development and peer review have been studied independently in computing courses, but their combination has not. We report on an experiment in which students in two courses conducted peer review of test suites while assignments were in progress. We find strong correlation between review ratings and staff-assessed work quality, as well as evidence that test suites improved during the review process. Student feedback suggests that reviewing had some causal impact on these improvements. We describe several lessons learned about administering and assessing peer-review within test-first development.},
booktitle = {Proceedings of the Tenth Annual Conference on International Computing Education Research},
pages = {11–18},
numpages = {8},
keywords = {test-first development, peer-review},
location = {Glasgow, Scotland, United Kingdom},
series = {ICER '14}
}

@article{10.14778/2733004.2733075,
author = {Markl, Volker},
title = {Breaking the Chains: On Declarative Data Analysis and Data Independence in the Big Data Era},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733075},
doi = {10.14778/2733004.2733075},
abstract = {Data management research, systems, and technologies have drastically improved the availability of data analysis capabilities, particularly for non-experts, due in part to low-entry barriers and reduced ownership costs (e.g., for data management infrastructures and applications). Major reasons for the widespread success of database systems and today's multi-billion dollar data management market include data independence, separating physical representation and storage from the actual information, and declarative languages, separating the program specification from its intended execution environment. In contrast, today's big data solutions do not offer data independence and declarative specification. As a result, big data technologies are mostly employed in newly-established companies with IT-savvy employees or in large well-established companies with big IT departments. We argue that current big data solutions will continue to fall short of widespread adoption, due to usability problems, despite the fact that in-situ data analytics technologies achieve a good degree of schema independence. In particular, we consider the lack of a declarative specification to be a major road-block, contributing to the scarcity in available data scientists available and limiting the application of big data to the IT-savvy industries. In particular, data scientists currently have to spend a lot of time on tuning their data analysis programs for specific data characteristics and a specific execution environment. We believe that the research community needs to bring the powerful concepts of declarative specification to current data analysis systems, in order to achieve the broad big data technology adoption and effectively deliver the promise that novel big data technologies offer.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1730–1733},
numpages = {4}
}

@article{10.14778/2733004.2733057,
author = {Jindal, Alekh and Rawlani, Praynaa and Wu, Eugene and Madden, Samuel and Deshpande, Amol and Stonebraker, Mike},
title = {Vertexica: Your Relational Friend for Graph Analytics!},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733057},
doi = {10.14778/2733004.2733057},
abstract = {In this paper, we present Vertexica, a graph analytics tools on top of a relational database, which is user friendly and yet highly efficient. Instead of constraining programmers to SQL, Vertexica offers a popular vertex-centric query interface, which is more natural for analysts to express many graph queries. The programmers simply provide their vertex-compute functions and Vertexica takes care of efficiently executing them in the standard SQL engine. The advantage of using Vertexica is its ability to leverage the relational features and enable much more sophisticated graph analysis. These include expressing graph algorithms which are difficult in vertex-centric but straightforward in SQL and the ability to compose end-to-end data processing pipelines, including pre- and post- processing of graphs as well as combining multiple algorithms for deeper insights. Vertexica has a graphical user interface and we outline several demonstration scenarios including, interactive graph analysis, complex graph analysis, and continuous and time series analysis.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1669–1672},
numpages = {4}
}

@article{10.14778/2732977.2732980,
author = {Han, Minyang and Daudjee, Khuzaima and Ammar, Khaled and \"{O}zsu, M. Tamer and Wang, Xingfang and Jin, Tianqi},
title = {An Experimental Comparison of Pregel-like Graph Processing Systems},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732977.2732980},
doi = {10.14778/2732977.2732980},
abstract = {The introduction of Google's Pregel generated much interest in the field of large-scale graph data processing, inspiring the development of Pregel-like systems such as Apache Giraph, GPS, Mizan, and GraphLab, all of which have appeared in the past two years. To gain an understanding of how Pregel-like systems perform, we conduct a study to experimentally compare Giraph, GPS, Mizan, and GraphLab on equal ground by considering graph and algorithm agnostic optimizations and by using several metrics. The systems are compared with four different algorithms (PageRank, single source shortest path, weakly connected components, and distributed minimum spanning tree) on up to 128 Amazon EC2 machines. We find that the system optimizations present in Giraph and GraphLab allow them to perform well. Our evaluation also shows Giraph 1.0.0's considerable improvement since Giraph 0.1 and identifies areas of improvement for all systems.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1047–1058},
numpages = {12}
}

@article{10.14778/2733004.2733012,
author = {Zhang, Zhuo and Li, Chao and Tao, Yangyu and Yang, Renyu and Tang, Hong and Xu, Jie},
title = {Fuxi: A Fault-Tolerant Resource Management and Job Scheduling System at Internet Scale},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733012},
doi = {10.14778/2733004.2733012},
abstract = {Scalability and fault-tolerance are two fundamental challenges for all distributed computing at Internet scale. Despite many recent advances from both academia and industry, these two problems are still far from settled. In this paper, we present Fuxi, a resource management and job scheduling system that is capable of handling the kind of workload at Alibaba where hundreds of terabytes of data are generated and analyzed everyday to help optimize the company's business operations and user experiences. We employ several novel techniques to enable Fuxi to perform efficient scheduling of hundreds of thousands of concurrent tasks over large clusters with thousands of nodes: 1) an incremental resource management protocol that supports multi-dimensional resource allocation and data locality; 2) user-transparent failure recovery where failures of any Fuxi components will not impact the execution of user jobs; and 3) an effective detection mechanism and a multi-level blacklisting scheme that prevents them from affecting job execution. Our evaluation results demonstrate that 95% and 91% scheduled CPU/memory utilization can be fulfilled under synthetic workloads, and Fuxi is capable of achieving 2.36T-B/minute throughput in GraySort. Additionally, the same Fuxi job only experiences approximately 16% slowdown under a 5% fault-injection rate. The slowdown only grows to 20% when we double the fault-injection rate to 10%. Fuxi has been deployed in our production environment since 2009, and it now manages hundreds of thousands of server nodes.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1393–1404},
numpages = {12}
}

@inproceedings{10.1145/2639968.2640066,
author = {Lin, Yu-Ru},
title = {Assessing Sentiment Segregation in Urban Communities},
year = {2014},
isbn = {9781450328883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639968.2640066},
doi = {10.1145/2639968.2640066},
abstract = {In this work, we attempt to examine the relationship between the social antecedents of urban neighborhoods and the citizens' everyday sentiment expression left in social media. Using Twitter users' geocoded messages posted within neighborhoods in the city of Pittsburgh, we first construct sentiment profiles for each neighborhood. We identify neighborhoods with relatively stable sentiment profiles and analyze the correlations between their sentiment orientations and the neighborhoods' demographic attributes including age, public safety, education, and ethnicity. The first order correlations show an interesting association between these neighborhood attributes and particular types of sentiments. We further group neighborhoods with similar demographic characteristics and observe that between two demographic groups, sentiments diverge in several sentiment categories including joy and disgust. This paper presents empirical evidence of sentiment segregation corresponding to the neighborhood contexts, which has implications for monitoring public attitudes and community integration.},
booktitle = {Proceedings of the 2014 International Conference on Social Computing},
pages = {1–8},
numpages = {8},
keywords = {sentiment analysis, social segregation, computational social science, urban communities and neighborhoods},
location = {Beijing, China},
series = {SocialCom '14}
}

