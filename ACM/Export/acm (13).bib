@article{10.1145/2522968.2522972,
author = {Roundy, Kevin A. and Miller, Barton P.},
title = {Binary-Code Obfuscations in Prevalent Packer Tools},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2522968.2522972},
doi = {10.1145/2522968.2522972},
abstract = {The first steps in analyzing defensive malware are understanding what obfuscations are present in real-world malware binaries, how these obfuscations hinder analysis, and how they can be overcome. While some obfuscations have been reported independently, this survey consolidates the discussion while adding substantial depth and breadth to it. This survey also quantifies the relative prevalence of these obfuscations by using the Dyninst binary analysis and instrumentation tool that was recently extended for defensive malware analysis. The goal of this survey is to encourage analysts to focus on resolving the obfuscations that are most prevalent in real-world malware.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {4},
numpages = {32},
keywords = {program binary analysis, Malware, obfuscation}
}

@article{10.1145/2522968.2522971,
author = {Tamburri, Damian A. and Lago, Patricia and Vliet, Hans van},
title = {Organizational Social Structures for Software Engineering},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2522968.2522971},
doi = {10.1145/2522968.2522971},
abstract = {Software engineering evolved from a rigid process to a dynamic interplay of people (e.g., stakeholders or developers). Organizational and social literature call this interplay an Organizational Social Structure (OSS). Software practitioners still lack a systematic way to select, analyze, and support OSSs best fitting their problems (e.g., software development). We provide the state-of-the-art in OSSs, and discuss mechanisms to support OSS-related decisions in software engineering (e.g., choosing the OSS best fitting development scenarios). Our data supports two conclusions. First, software engineering focused on building software using project teams alone, yet these are one of thirteen OSS flavors from literature. Second, an emerging OSS should be further explored for software development: social networks. This article represents a first glimpse at OSS-aware software engineering, that is, to engineer software using OSSs best fit for the problem.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {3},
numpages = {35},
keywords = {software organizations, software practice, cultural implications, user perspective, users, governance, information trust, social structures, knowledge management, organizational decision-making, communities, social adaptivity, social context, social networks, Organizational social structures}
}

@article{10.1145/2483969.2483975,
author = {Nakov, Preslav I. and Hearst, Marti A.},
title = {Semantic Interpretation of Noun Compounds Using Verbal and Other Paraphrases},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1550-4875},
url = {https://doi.org/10.1145/2483969.2483975},
doi = {10.1145/2483969.2483975},
abstract = {We study the problem of semantic interpretation of noun compounds such as bee honey, malaria mosquito, apple cake, and stem cell. In particular, we explore the potential of using predicates that make explicit the hidden relation that holds between the nouns that form the noun compound. For example, mosquito that carries malaria is a paraphrase of the compound malaria mosquito in which the verb explicitly states the semantic relation between the two nouns. We study the utility of using such paraphrasing verbs, with associated weights, to build a representation of the semantics of a noun compound, for example, malaria mosquito can be represented as follows: carry (23), spread (16), cause (12), transmit (9), and so on. We also explore the potential of using multiple paraphrasing verbs as features for predicting abstract semantic relations such as CAUSE, and we demonstrate that using explicit paraphrases can help improve statistical machine translation.},
journal = {ACM Trans. Speech Lang. Process.},
month = jul,
articleno = {13},
numpages = {51},
keywords = {Lexical semantics, machine translation, paraphrases, Web as a corpus, noun compounds, multiword expressions}
}

@inproceedings{10.1145/2483760.2483776,
author = {Tripp, Omer and Weisman, Omri and Guy, Lotem},
title = {Finding Your Way in the Testing Jungle: A Learning Approach to Web Security Testing},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483776},
doi = {10.1145/2483760.2483776},
abstract = { Black-box security testing of web applications is a hard problem. The main complication lies in the black-box assumption: The testing tool has limited insight into the workings of server-side defenses. This has traditionally led commercial as well as research vulnerability scanners toward heuristic approaches, such as testing each input point (e.g. HTTP parameter) with a short, predefined list of effective test payloads to balance between coverage and performance.  We take a fresh approach to the problem of security testing, casting it into a learning setting. In our approach, the testing algorithm has available a comprehensive database of test payloads, such that if the web application's defenses are broken, then with near certainty one of the candidate payloads is able to demonstrate the vulnerability. The question then becomes how to efficiently search through the payload space to find a good candidate. In our solution, the learning algorithm infers from a failed test---by analyzing the website's response---which other payloads are also likely to fail, thereby pruning substantial portions of the search space.  We have realized our approach in XSS Analyzer, an industry-level cross-site scripting (XSS) scanner featuring 500,000,000 test payloads. Our evaluation on 15,552 benchmarks shows solid results: XSS Analyzer achieves &gt;99% coverage relative to brute-force traversal over all payloads, while trying only 10 payloads on average per input point. XSS Analyzer also outperforms several competing algorithms, including a mature commercial algorithm---featured in IBM Security AppScan Standard V8.5---by a far margin. XSS Analyzer has recently been integrated into the latest version of AppScan (V8.6) instead of that algorithm. },
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {347–357},
numpages = {11},
keywords = {cross-site scripting, online learning, vulnerability scanner},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@article{10.1145/2505395.2505398,
author = {Andrienko, Gennady and Gkoulalas-Divanis, Aris and Gruteser, Marco and Kopp, Christine and Liebig, Thomas and Rechert, Klaus},
title = {Report from Dagstuhl: The Liberation of Mobile Location Data and Its Implications for Privacy Research},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1559-1662},
url = {https://doi.org/10.1145/2505395.2505398},
doi = {10.1145/2505395.2505398},
abstract = {With the emergence of the mobile app ecosystem, user location data has escaped the grip of the tightly regulated telecommunication industry and is now being collected at unprecedented scale and accuracy by mobile advertising, platform, and app providers. This position paper is based on discussions of the authors at the Dagstuhl seminar on Mobility Data Mining and Privacy. It seeks to highlight this shift by providing a tutorial on location data flows and associated privacy risks in this mobile app ecosystem. Moreover, it reflects on the implications of this shift to the mobile privacy research community.},
journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
month = jul,
pages = {7–18},
numpages = {12}
}

@inproceedings{10.1145/2485895.2485900,
author = {Marsella, Stacy and Xu, Yuyu and Lhommet, Margaux and Feng, Andrew and Scherer, Stefan and Shapiro, Ari},
title = {Virtual Character Performance from Speech},
year = {2013},
isbn = {9781450321327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485895.2485900},
doi = {10.1145/2485895.2485900},
abstract = {We demonstrate a method for generating a 3D virtual character performance from the audio signal by inferring the acoustic and semantic properties of the utterance. Through a prosodic analysis of the acoustic signal, we perform an analysis for stress and pitch, relate it to the spoken words and identify the agitation state. Our rule-based system performs a shallow analysis of the utterance text to determine its semantic, pragmatic and rhetorical content. Based on these analyses, the system generates facial expressions and behaviors including head movements, eye saccades, gestures, blinks and gazes. Our technique is able to synthesize the performance and generate novel gesture animations based on coarticulation with other closely scheduled animations. Because our method utilizes semantics in addition to prosody, we are able to generate virtual character performances that are more appropriate than methods that use only prosody. We perform a study that shows that our technique outperforms methods that use prosody alone.},
booktitle = {Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {25–35},
numpages = {11},
keywords = {conversational agent, behavior, animation, gestures},
location = {Anaheim, California},
series = {SCA '13}
}

@article{10.1145/2461912.2461979,
author = {Skouras, M\'{e}lina and Thomaszewski, Bernhard and Coros, Stelian and Bickel, Bernd and Gross, Markus},
title = {Computational Design of Actuated Deformable Characters},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2461912.2461979},
doi = {10.1145/2461912.2461979},
abstract = {We present a method for fabrication-oriented design of actuated deformable characters that allows a user to automatically create physical replicas of digitally designed characters using rapid manufacturing technologies. Given a deformable character and a set of target poses as input, our method computes a small set of actuators along with their locations on the surface and optimizes the internal material distribution such that the resulting character exhibits the desired deformation behavior. We approach this problem with a dedicated algorithm that combines finite-element analysis, sparse regularization, and constrained optimization. We validate our pipeline on a set of two- and three-dimensional example characters and present results in simulation and physically-fabricated prototypes.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {82},
numpages = {10},
keywords = {control, elastic solids, computational materials, physically-based simulation}
}

@inproceedings{10.1145/2484762.2484767,
author = {Farcas, Claudiu and Balac, Natasha and Ohno-Machado, Lucila},
title = {Biomedical CyberInfrastructure Challenges},
year = {2013},
isbn = {9781450321709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484762.2484767},
doi = {10.1145/2484762.2484767},
abstract = {Biomedical research traverses a new era of advancements through the adoption of massive computing and big-data solutions to major scientific problems. However, the road ahead is far from "a walk in a park" -- many obstacles exist in the standardization, adoption, and evolution of methods, practices, algorithms, tools, and ultimately knowledge, that would mature along this road. In this article, we discuss such challenges that we encountered in this field and possible solutions from the iDASH program that closely engages this community.},
booktitle = {Proceedings of the Conference on Extreme Science and Engineering Discovery Environment: Gateway to Discovery},
articleno = {6},
numpages = {4},
keywords = {biomedical, challenges, cyberinfrastructure},
location = {San Diego, California, USA},
series = {XSEDE '13}
}

@inproceedings{10.1145/2484762.2484807,
author = {Akli, Linda and Moore, Samuel L. and Rivera, Lorna I. and Teller, Patricia J.},
title = {Training, Education, and Outreach: Raising the Bar},
year = {2013},
isbn = {9781450321709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484762.2484807},
doi = {10.1145/2484762.2484807},
abstract = {This paper describes our efforts to help expand computational and data-enabled science and engineering by educating faculty, students, and staff at the University of Texas at El Paso (UTEP) to advance scientific discovery through the use of cyberinfrastructure. The best practices used to plan, execute, and evaluate the workshop are discussed, and the results of a professional assessment of the workshop are presented.The two-day regional workshop held at UTEP on February 19 and 20, 2013 attracted 100 registrants. It was the largest, most diverse, and most comprehensive workshop that XSEDE has conducted to date. Since the workshop was located at UTEP, it addressed the inclusion of communities that are traditionally under-represented and under-served in STEM, including women and minorities. Note that UTEP is a Hispanic-serving institution with a student population that mirrors the region, i.e., it serves 22,700 students, 77% of whom are Hispanic. In addition, because of its focus, this effort also is meaningful in terms of sustaining a large and diverse scientific, academic, and industrial workforce.},
booktitle = {Proceedings of the Conference on Extreme Science and Engineering Discovery Environment: Gateway to Discovery},
articleno = {73},
numpages = {5},
keywords = {training, evaluation, education, outreach},
location = {San Diego, California, USA},
series = {XSEDE '13}
}

@inproceedings{10.1145/2467696.2467712,
author = {Faniel, Ixchel and Kansa, Eric and Whitcher Kansa, Sarah and Barrera-Gomez, Julianna and Yakel, Elizabeth},
title = {The Challenges of Digging Data: A Study of Context in Archaeological Data Reuse},
year = {2013},
isbn = {9781450320771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2467696.2467712},
doi = {10.1145/2467696.2467712},
abstract = {Field archaeology only recently developed centralized systems for data curation, management, and reuse. Data documentation guidelines, standards, and ontologies have yet to see wide adoption in this discipline. Moreover, repository practices have focused on supporting data collection, deposit, discovery, and access more than data reuse. In this paper we examine the needs of archaeological data reusers, particularly the context they need to understand, verify, and trust data others collect during field studies. We then apply our findings to the existing work on standards development. We find that archaeologists place the most importance on data collection procedures, but the reputation and scholarly affiliation of the archaeologists who conducted the original field studies, the wording and structure of the documentation created during field work, and the repository where the data are housed also inform reuse. While guidelines, standards, and ontologies address some aspects of the context data reusers need, they provide less guidance on others, especially those related to research design. We argue repositories need to address these missing dimensions of context to better support data reuse in archaeology.},
booktitle = {Proceedings of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {295–304},
numpages = {10},
keywords = {data reuse, data management, archaeology, data standards},
location = {Indianapolis, Indiana, USA},
series = {JCDL '13}
}

@article{10.1145/2489253.2489265,
author = {Bisdikian, Chatschik and Kaplan, Lance M. and Srivastava, Mani B.},
title = {On the Quality and Value of Information in Sensor Networks},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/2489253.2489265},
doi = {10.1145/2489253.2489265},
abstract = {The increasing use of sensor-derived information from planned, ad-hoc, and/or opportunistically deployed sensor networks provides enhanced visibility to everyday activities and processes, enabling fast-paced data-to-decision in personal, social, civilian, military, and business contexts. The value that information brings to this visibility and ensuing decisions depends on the quality characteristics of the information gathered. In this article, we highlight, refine, and extend upon our past work in the areas of quality and value of information (QoI and VoI) for sensor networks. Specifically, we present and elaborate on our two-layer QoI/VoI definition, where the former relates to context-independent aspects and the latter to context-dependent aspects of an information product. Then, we refine our taxonomy of pertinent QoI and VoI attributes anchored around a simple ontological relationship between the two. Finally, we introduce a framework for scoring and ranking information products based on their VoI attributes using the analytic hierarchy multicriteria decision process, illustrated via a simple example.},
journal = {ACM Trans. Sen. Netw.},
month = jul,
articleno = {48},
numpages = {26},
keywords = {value of information, Internet of things, VoI, IoT, analytic hierarchy process, QoI, information systems, Quality of information, sensor information fusion, metadata, AHP, provenance}
}

@inproceedings{10.1145/2486159.2486177,
author = {Bar-Yehuda, Reuven and Beder, Michael and Rawitz, Dror},
title = {A Constant Factor Approximation Algorithm for the Storage Allocation Problem: Extended Abstract},
year = {2013},
isbn = {9781450315722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486159.2486177},
doi = {10.1145/2486159.2486177},
abstract = {We study the Storage Allocation Problem (SAP) which is a variant of the Unsplittable Flow Problem on Paths (UFPP). A SAP instance consists of a path P = (V,E) and a set J of tasks. Each edge e ∈ E has a capacity ce and each task j ∈ J is associated with a path Ij in P, a demand dj and a weight wj. The goal is to find a maximum weight subset S ⊆ J of tasks and a height function h:S → ℜ+ such that (i) h(j)|+dj ≤ ce, for every e ∈ Ij; and (ii) if j,i ∈ S such that Ij ∩ Ii ≠ ∅ and h(j) ≥ h(i), then h(j) ≥ h(i) + di. SAP can be seen as a rectangle packing problem in which rectangles can be moved vertically, but not horizontally.We present a polynomial time (9+ε)-approximation algorithm for SAP. Our algorithm is based on a variation of the framework for approximating UFPP by Bonsma et al. [FOCS 2011] and on a (4+ε)-approximation algorithm for δ-small SAP instances (in which dj ≤ δ • ce, for every e ∈ Ij for a sufficiently small constant δ&gt;0). In our algorithm for δ-small instances, tasks are packed carefully in strips in a UFPP manner, and then a (1+ε) factor is incurred by a reduction from SAP to UFPP in strips. The strips are stacked to form a SAP solution. Finally, we show that SAP is strongly NP-hard, even with uniform weights and even if assuming the no bottleneck assumption.},
booktitle = {Proceedings of the Twenty-Fifth Annual ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {204–213},
numpages = {10},
keywords = {unsplittable flow, approximation algorithms, bandwidth allocation, rectangle packing, storage allocation},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {SPAA '13}
}

@inproceedings{10.1145/2484838.2484877,
author = {Hussein, Sari Haj and Lu, Hua and Pedersen, Torben Bach},
title = {Reasoning about RFID-Tracked Moving Objects in Symbolic Indoor Spaces},
year = {2013},
isbn = {9781450319218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484838.2484877},
doi = {10.1145/2484838.2484877},
abstract = {In recent years, indoor spatial data management has started to attract attention, partly due to the increasing use of receptor devices (e.g., RFID readers, and wireless sensor networks) in indoor, as well as outdoor spaces. There is thus a great need for a model that captures such spaces, their receptors, and provides powerful reasoning techniques on top. This paper reviews and extends a recent unified model of outdoor and indoor spaces and receptor deployments in these spaces. The extended model enables modelers to capture various information pieces from the physical world. On top of the extended model, this paper proposes and formalizes the route observability concept, and demonstrates its usefulness in enhancing the reading environment. The extended model also enables incorporating receptor data through a probabilistic trajectory-to-route translator. This translator first facilitates the tracking of moving objects enabling the search for them to be optimized, and second supports high-level reasoning about points of potential traffic (over)load, so-called bottleneck points. The functional analysis illustrates the behavior of the route observability function. The experimental evaluation shows the accuracy of the translator, and the quality of the inference and reasoning. The experiments are conducted on both synthetic data and uncleansed, real-world data obtained from RFID-tagged flight baggage.},
booktitle = {Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
articleno = {9},
numpages = {12},
keywords = {reasoning, indoor space, RFID, modeling, moving objects tracking},
location = {Baltimore, Maryland, USA},
series = {SSDBM}
}

@inproceedings{10.1145/2491148.2491151,
author = {Jimenez Garcia, Juan and Romero, Natalia A. and Boerema, Simone T. and Keyson, David and Havinga, Paul},
title = {ESTHER: A Portable Sensor Toolkit to Collect and Monitor Total Hip Replacement Patient Data},
year = {2013},
isbn = {9781450322072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491148.2491151},
doi = {10.1145/2491148.2491151},
abstract = {Due to the increasing cost of medical care, hospitals are looking at post surgery patients' home as the primary place for recovery. Unfortunately, this paradigm shift involves difficulties for patients and physiotherapists to manage the expected outcomes. While patients face physical and emotional problems related to the new hip, clinical teams have limited resources to follow patients' health experiences during their recovery. Mobile technologies for home care provide opportunities to remotely support patients in their rehabilitation process. They are designed to become part of patients' daily activities, which requires a holistic understanding of the dynamics of post-surgery treatment. Therefore, it is foreseen that requirements to design home care technologies should address clinicians' needs related to the functional aspects as well as patients' experiences of home recovery. ESTHER (Experience Sampling for Total Hip Replacement) is a research and design toolkit developed to study Total Hip Replacement (THR) patients' experiences after surgery and to evaluate design interventions to support patients in the complexity of home recovery. The tool is based on the Experience Sampling Method (ESM) to capture patients' self report on their recovery process. In an iterative approach the tool gradually added to patients' psychological reports physical activity using wireless sensor nodes. The first three iterations of ESTHER are described to illustrate the value of situated self-reports and the richness of combining both self-report and sensing techniques as a holistic approach to understand both behavioral and experiential aspects of home recovery. The experience in conducting situated design research has shown to be valuable in understanding the technical as well as social challenges and opportunities for the research and design community of home health technologies.},
booktitle = {Proceedings of the 3rd ACM MobiHoc Workshop on Pervasive Wireless Healthcare},
pages = {7–12},
numpages = {6},
keywords = {field studies, recovery, experience sampling method, total hip replacement, patient data, portable sensor system},
location = {Bangalore, India},
series = {MobileHealth '13}
}

@inproceedings{10.1145/2484838.2484851,
author = {Mousavi, Hamid and Zaniolo, Carlo},
title = {Fast Computation of Approximate Biased Histograms on Sliding Windows over Data Streams},
year = {2013},
isbn = {9781450319218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484838.2484851},
doi = {10.1145/2484838.2484851},
abstract = {Histograms provide effective synopses of large data sets, and are thus used in a wide variety of applications, including query optimization, approximate query answering, distribution fitting, parallel database partitioning, and data mining. Moreover, very fast approximate algorithms are needed to compute accurate histograms on fast-arriving data streams, whereby online queries can be supported within the given memory and computing resources. Many real-life applications require that the data distribution in certain regions must be modeled with greater accuracy, and Biased Histograms are designed to address this need. In this paper, we define biased histograms over data streams and sliding windows on data streams, and propose the Bar Splitting Biased Histogram (BSBH) algorithm to construct them efficiently and accurately. We prove that BSBH generates expected ∈-approximate biased histograms for data streams with stationary distributions, and our experiments show that BSBH also achieves good approximation in the presence of concept shifts, even major ones. Additionally, BSBH employs a new biased sampling technique which outperforms uniform sampling in terms of accuracy, while using about the same amount of time and memory. Therefore, BSBH outperforms previously proposed algorithms for computing biased histograms over the whole data stream, and it is the first algorithm that supports windows.},
booktitle = {Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
articleno = {13},
numpages = {12},
keywords = {biased histograms, quantiles, data streams, sliding windows},
location = {Baltimore, Maryland, USA},
series = {SSDBM}
}

@inproceedings{10.1145/2500727.2500733,
author = {Keng, Joseph Chan Joo and Wee, Tan Kiat and Jiang, Lingxiao and Balan, Rajesh Krishna},
title = {The Case for Mobile Forensics of Private Data Leaks: Towards Large-Scale User-Oriented Privacy Protection},
year = {2013},
isbn = {9781450323161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2500727.2500733},
doi = {10.1145/2500727.2500733},
abstract = {Privacy protection against mobile applications on mobile devices is becoming a serious concern as user sensitive data may be leaked without proper justification. Most current leak detection tools only report leaked private data, but provide inadequate information about the causes of the leaks for end users to take preventive measures. Hence, users often cannot reconcile the way they have used an application to a reported leak --- i.e., they are unable to comprehend the (il)legitimacy of the leak or make a decision on whether to allow the leak. This paper aims to demonstrate the feasibility and benefits of identifying the causes of leaks from a user's point of view, which we call mobile forensics of privacy leaks. Its goal is to correlate user actions to leaks, and report the causes from a user-oriented perspective. To make the case, we have performed a preliminary study that identifies leak causes based on logs of user actions in more than 220 Android applications and corresponding leak reports from a leak detection tool. Our results show that more than 60% of the 105 applications (of the 220 we sampled) that leak private data leak data do so due to user actions on certain in-application GUI widgets. About 44% also leak data right after users launch them, while 32% leak data periodically after launch. We also constructed a database containing leak causes from all tested apps, and demonstrated the use of visual overlays to warn users about potential leaks.},
booktitle = {Proceedings of the 4th Asia-Pacific Workshop on Systems},
articleno = {6},
numpages = {7},
location = {Singapore, Singapore},
series = {APSys '13}
}

@inproceedings{10.1145/2484838.2484880,
author = {Halperin, Daniel and Ribalet, Francois and Weitz, Konstantin and Saito, Mak A. and Howe, Bill and Armbrust, E. Virginia},
title = {Real-Time Collaborative Analysis with (Almost) Pure SQL: A Case Study in Biogeochemical Oceanography},
year = {2013},
isbn = {9781450319218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484838.2484880},
doi = {10.1145/2484838.2484880},
abstract = {We consider a case study using SQL-as-a-Service to support "instant analysis" of weakly structured relational data at a multi-investigator science retreat. Here, "weakly structured" means tabular, rows-and-columns datasets that share some common context, but that have limited a priori agreement on file formats, relationships, types, schemas, metadata, or semantics. In this case study, the data were acquired from hundreds of distinct locations during a multi-day oceanographic cruise using a variety of physical, biological, and chemical sensors and assays. Months after the cruise when preliminary data processing was complete, 40+ researchers from a variety of disciplines participated in a two-day "data synthesis workshop." At this workshop, two computer scientists used a web-based query-as-a-service platform called SQLShare to perform "SQL stenography": capturing the scientific discussion in real time to integrate data, test hypotheses, and populate visualizations to then inform and enhance further discussion. In this "field test" of our technology and approach, we found that it was not only feasible to support interactive science Q&amp;A with essentially pure SQL, but that we significantly increased the value of the "face time" at the meeting: researchers from different fields were able to validate assumptions and resolve ambiguity about each others' fields. As a result, new science emerged from a meeting that was originally just a planning meeting. In this paper, we describe the details of this experiment, discuss our major findings, and lay out a new research agenda for collaborative science database services.},
booktitle = {Proceedings of the 25th International Conference on Scientific and Statistical Database Management},
articleno = {28},
numpages = {12},
location = {Baltimore, Maryland, USA},
series = {SSDBM}
}

@article{10.1145/2499962.2499967,
author = {Derrick, Douglas C. and Meservy, Thomas O. and Jenkins, Jeffrey L. and Burgoon, Judee K. and Nunamaker, Jay F.},
title = {Detecting Deceptive Chat-Based Communication Using Typing Behavior and Message Cues},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2499962.2499967},
doi = {10.1145/2499962.2499967},
abstract = {Computer-mediated deception is prevalent and may have serious consequences for individuals, organizations, and society. This article investigates several metrics as predictors of deception in synchronous chat-based environments, where participants must often spontaneously formulate deceptive responses. Based on cognitive load theory, we hypothesize that deception influences response time, word count, lexical diversity, and the number of times a chat message is edited. Using a custom chatbot to conduct interviews in an experiment, we collected 1,572 deceitful and 1,590 truthful chat-based responses. The results of the experiment confirm that deception is positively correlated with response time and the number of edits and negatively correlated to word count. Contrary to our prediction, we found that deception is not significantly correlated with lexical diversity. Furthermore, the age of the participant moderates the influence of deception on response time. Our results have implications for understanding deceit in chat-based communication and building deception-detection decision aids in chat-based systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = aug,
articleno = {9},
numpages = {21},
keywords = {chat, deception detection, Decision support system, typing bahavior}
}

@inproceedings{10.1145/2493525.2493535,
author = {Ferreira, Emmanuel and Lef\`{e}vre, Fabrice},
title = {Social Signal and User Adaptation in Reinforcement Learning-Based Dialogue Management},
year = {2013},
isbn = {9781450320191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493525.2493535},
doi = {10.1145/2493525.2493535},
abstract = {This paper investigates the conditions under which cues from social signals can be used for user adaptation (or user tracking) of a learning agent. In this work we consider the case of the Reinforcement Learning (RL) of a dialogue management module. Social signals (gazes, postures, emotions, etc.) have an undeniable importance in human interactions and can be used as an additional and user-dependent (subjective) reinforcement signal during learning. In this paper, the Kalman Temporal Differences (KTD) framework is employed in combination with a potential-based shaping reward method to properly integrate the social information in the optimisation procedure and adapt the policy to user profiles. In a second step the ability of the method to track a new user profile (after self learning of the user or switch to a new user) is shown. Experiments carried out using a state-of-the-art goal-oriented dialogue management framework with simulations support our claims.},
booktitle = {Proceedings of the 2nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication},
pages = {61–69},
numpages = {9},
keywords = {value function approximation, user adaptation, reward shaping, social signals, dialogue management, reinforcement learning},
location = {Beijing, China},
series = {MLIS '13}
}

@article{10.1145/2493175.2493178,
author = {Asadi, Nima and Lin, Jimmy},
title = {Fast Candidate Generation for Real-Time Tweet Search with Bloom Filter Chains},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/2493175.2493178},
doi = {10.1145/2493175.2493178},
abstract = {The rise of social media and other forms of user-generated content have created the demand for real-time search: against a high-velocity stream of incoming documents, users desire a list of relevant results at the time the query is issued. In the context of real-time search on tweets, this work explores candidate generation in a two-stage retrieval architecture where an initial list of results is processed by a second-stage rescorer to produce the final output. We introduce Bloom filter chains, a novel extension of Bloom filters that can dynamically expand to efficiently represent an arbitrarily long and growing list of monotonically-increasing integers with a constant false positive rate. Using a collection of Bloom filter chains, a novel approximate candidate generation algorithm called BWand is able to perform both conjunctive and disjunctive retrieval. Experiments show that our algorithm is many times faster than competitive baselines and that this increased performance does not require sacrificing end-to-end effectiveness. Our results empirically characterize the trade-off space defined by output quality, query evaluation speed, and memory footprint for this particular search architecture.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {13},
numpages = {36},
keywords = {efficiency, tweet search, bloom filters, top-k retrieval, Scalability}
}

@article{10.1145/2499474.2499478,
author = {Garde-Perik, Evelien van de and Offermans, Serge and Boerdonk, Koen van and Lenssen, Kars-Michiel and Hoven, Elise van den},
title = {An Analysis of Input-Output Relations in Interaction with Smart Tangible Objects},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2499474.2499478},
doi = {10.1145/2499474.2499478},
abstract = {This article focuses on the conceptual relation between the user's input and a system's output in interaction with smart tangible objects. Understanding this input-output relation (IO relation) is a prerequisite for the design of meaningful interaction. A meaningful IO relation allows the user to know what to do with a system to achieve a certain goal and to evaluate the outcome. The work discussed in this article followed a design research process in which four concepts were developed and prototyped. An evaluation was performed using these prototypes to investigate the effect of highly different IO relations on the user's understanding of the interaction. The evaluation revealed two types of IO relations differing in functionality and the number of mappings between the user and system actions. These two types of relations are described by two IO models that provide an overview of these mappings. Furthermore, they illustrate the role of the user and the influence of the system in the process of understanding the interaction. The analysis of the two types of IO models illustrates the value of understanding IO relations for the design of smart tangible objects.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = aug,
articleno = {9},
numpages = {20},
keywords = {human-computer interaction, model, input-output relation, design research, Tangible interaction, meaning}
}

@article{10.1145/2516955.2516957,
author = {Kelley, Helen and Compeau, Deborah and Higgins, Christopher A. and Parent, Michael},
title = {Advancing Theory through the Conceptualization and Development of Causal Attributions for Computer Performance Histories},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/2516955.2516957},
doi = {10.1145/2516955.2516957},
abstract = {Attribution theory, advanced by Bernard Weiner and his colleagues is an important, though sometimes controversial, theory that has demonstrated vitality and longevity. The cross-disciplinary application of attribution theory to, for example, organizational behavior, marketing, and education, has stimulated the interests of researchers and contributed to its theoretical validity and reliability. Compared to other disciplines, the application and theoretical testing of attribution theory are in the "spring" of their existence in the field of Information Systems (IS). This paper proposes that conceptualization and measurement of the causal attributions individuals make for their computer performance and performance histories, positive and negative, are critical to understanding computer adoption and post-adoption behaviors. We first identify the causal attributions that enterprise resource planning (ERP) users make for their computer performance histories. We then describe the conceptualization and development of multi-item scales to capture these causal attributions. This work contributes to theory and practice through (1) the development and psychometric testing of several attributional scales for advancing our understanding of the multi-theoretical stream of research investigating technology adoption at the individual level, and (2) by providing a description of a theoretical multi-method approach for the rigorous scale development of causal attributions. Our work suggests that researchers must consider several fundamental principles of attribution theory when investigating IS artifacts during various adoption phases.},
journal = {SIGMIS Database},
month = aug,
pages = {8–33},
numpages = {26},
keywords = {scale development, and psychometric properties, adoption, computer-related performance, attribution theory, post-adoption, measurement validation}
}

@inproceedings{10.1145/2494621.2494647,
author = {Dautov, Rustem and Paraskakis, Iraklis},
title = {A Vision for Monitoring Cloud Application Platforms as Sensor Networks},
year = {2013},
isbn = {9781450321723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494621.2494647},
doi = {10.1145/2494621.2494647},
abstract = {Autonomic management of clouds has received a lot of attention by both academia and industry putting a lot of efforts into investigation of various solutions, even though the focus has been mainly on the IaaS level, while the PaaS level being less often addressed. However, with ever-expanding software environments of cloud application platforms, the self-management at the PaaS level becomes a major concern. We claim that run-time monitoring and detection of critical situations is a fundamental requirement to achieve autonomic behaviour in service-based cloud platforms. Accordingly, we present our novel vision of cloud application platforms as sensor networks -- computer accessible networks of distributed devices using sensors to monitor conditions at different locations. The vision is based on the similarities between the problem domain of cloud application platform monitoring and such sensor-enabled domains as traffic surveillance, environmental monitoring or home automation. We also discuss potential benefits and shortcomings associated with the presented concepts and ideas.},
booktitle = {Proceedings of the 2013 ACM Cloud and Autonomic Computing Conference},
articleno = {25},
numpages = {8},
keywords = {monitoring, sensor network, sensor web, service-oriented computing, cloud application platforms, autonomic computing},
location = {Miami, Florida, USA},
series = {CAC '13}
}

@inproceedings{10.1145/2494621.2494632,
author = {Gadre, Hrishikesh and Rodero, Ivan and Diaz-Montes, Javier and Parashar, Manish},
title = {A Case for MapReduce over the Internet},
year = {2013},
isbn = {9781450321723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494621.2494632},
doi = {10.1145/2494621.2494632},
abstract = {In recent years, MapReduce programming model and specifically its open source implementation Hadoop has been widely used by organizations to perform large-scale data processing tasks such as web-indexing, data mining as well as scientific simulations. The key benefits of this programming model include its simple programming interface and ability to process massive datasets in a scalable fashion without requiring high-end computing infrastructure. We observe that the current design of Hadoop framework assumes a centralized execution environment involving a single datacenter. This assumption leads to simplified design decisions in the Hadoop architecture regarding efficient network usage, specifically in the replica-selection policy in Hadoop Distributed File System (HDFS) and in the reduce phase scheduling algorithm. In this paper, we investigate real-world scenarios in which MapReduce programming model and specifically Hadoop framework could be used for processing large-scale, geographically scattered datasets. We show that using the Hadoop framework with default policies can cause severe performance degradation in such geographically distributed environment. We propose and evaluate extensions to Hadoop MapReduce framework to improve its performance in such environments. The evaluation demonstrates that the proposed extensions substantially outperform default policies in the Hadoop framework.},
booktitle = {Proceedings of the 2013 ACM Cloud and Autonomic Computing Conference},
articleno = {8},
numpages = {10},
keywords = {data center, data processing, MapReduce, distributed processing},
location = {Miami, Florida, USA},
series = {CAC '13}
}

@inproceedings{10.1145/2487575.2488205,
author = {Chandola, Varun and Sukumar, Sreenivas R. and Schryver, Jack C.},
title = {Knowledge Discovery from Massive Healthcare Claims Data},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488205},
doi = {10.1145/2487575.2488205},
abstract = {he role of big data in addressing the needs of the present healthcare system in US and rest of the world has been echoed by government, private, and academic sectors. There has been a growing emphasis to explore the promise of big data analytics in tapping the potential of the massive healthcare data emanating from private and government health insurance providers. While the domain implications of such collaboration are well known, this type of data has been explored to a limited extent in the data mining community. The objective of this paper is two fold: first, we introduce the emerging domain of "big" healthcare claims data to the KDD community, and second, we describe the success and challenges that we encountered in analyzing this data using state of art analytics for massive data. Specifically, we translate the problem of analyzing healthcare data into some of the most well-known analysis problems in the data mining community, social network analysis, text mining, and temporal analysis and higher order feature construction, and describe how advances within each of these areas can be leveraged to understand the domain of healthcare. Each case study illustrates a unique intersection of data mining and healthcare with a common objective of improving the cost-care ratio by mining for opportunities to improve healthcare operations and reducing what seems to fall under fraud, waste, and abuse.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1312–1320},
numpages = {9},
keywords = {healthcare analytics, fraud detection},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2488200,
author = {McMahan, H. Brendan and Holt, Gary and Sculley, D. and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and Chikkerur, Sharat and Liu, Dan and Wattenberg, Martin and Hrafnkelsson, Arnar Mar and Boulos, Tom and Kubica, Jeremy},
title = {Ad Click Prediction: A View from the Trenches},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488200},
doi = {10.1145/2487575.2488200},
abstract = {Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates.We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1222–1230},
numpages = {9},
keywords = {online advertising, large-scale learning, data mining},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2488210,
author = {Vatsavai, Ranga Raju},
title = {Gaussian Multiple Instance Learning Approach for Mapping the Slums of the World Using Very High Resolution Imagery},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488210},
doi = {10.1145/2487575.2488210},
abstract = {In this paper, we present a computationally efficient algorithm based on multiple instance learning for mapping informal settlements (slums) using very high-resolution remote sensing imagery. From remote sensing perspective, informal settlements share unique spatial characteristics that distinguish them from other urban structures like industrial, commercial, and formal residential settlements. However, regular pattern recognition and machine learning methods, which are predominantly single-instance or per-pixel classifiers, often fail to accurately map the informal settlements as they do not capture the complex spatial patterns. To overcome these limitations we employed a multiple instance based machine learning approach, where groups of contiguous pixels (image patches) are modeled as generated by a Gaussian distribution. We have conducted several experiments on very high-resolution satellite imagery, representing four unique geographic regions across the world. Our method showed consistent improvement in accurately identifying informal settlements.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1419–1426},
numpages = {8},
keywords = {remote sensing, spatial data mining, mil},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2501040.2501982,
author = {Ramesh, Archana and Teredesai, Ankur and Bindra, Ashish and Pokuri, Sreenivasulu and Uppala, Krishna},
title = {Audience Segment Expansion Using Distributed In-Database k-Means Clustering},
year = {2013},
isbn = {9781450323239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2501040.2501982},
doi = {10.1145/2501040.2501982},
abstract = {Online display advertisers extensively use the concept of a user segment to cluster users into targetable groups. When the sizes of such segments are less than the desired value for campaign budgets, there is a need to use probabilistic modeling to expand the size. This process is termed look-alike modeling. Given the multitude of data providers and on-line data sources, there are thousands of segments for each targetable consumer extracted from billions of online (even offline) actions performed by millions of users. The majority of advertisers, marketers and publishers have to use large scale distributed infrastructures to create thousands of user segments on a daily basis. Developing accurate data mining models efficiently within such platforms is a challenging task. The volume and variety of data can be a significant bottleneck for non-disk resident algorithms, since operating time for training and scoring hundreds of segments with millions of targetable users is non-trivial.In this paper, we present a novel k-means based distributed in-database algorithm for look-alike modeling implemented within the nPario database system. We demonstrate the utility of the algorithm: accurate, invariant of size and skew of the targetable audience(very few positive examples), and dependent linearly on the capacity and number of nodes in the distributed environment. To the best of our knowledge this is the first ever commercially deployed distributed look-alike modeling implementation to solve this problem. We compare the performance of our algorithm with other distributed and non-distributed look-alike modeling techniques, and report the results over a multi-core environment.},
booktitle = {Proceedings of the Seventh International Workshop on Data Mining for Online Advertising},
articleno = {5},
numpages = {9},
location = {Chicago, Illinois},
series = {ADKDD '13}
}

@inproceedings{10.1145/2487575.2488192,
author = {Weiss, Sholom M. and Dhurandhar, Amit and Baseman, Robert J.},
title = {Improving Quality Control by Early Prediction of Manufacturing Outcomes},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488192},
doi = {10.1145/2487575.2488192},
abstract = {We describe methods for continual prediction of manufactured product quality prior to final testing. In our most expansive modeling approach, an estimated final characteristic of a product is updated after each manufacturing operation. Our initial application is for the manufacture of microprocessors, and we predict final microprocessor speed. Using these predictions, early corrective manufacturing actions may be taken to increase the speed of expected slow wafers (a collection of microprocessors) or reduce the speed of fast wafers. Such predictions may also be used to initiate corrective supply chain management actions. Developing statistical learning models for this task has many complicating factors: (a) a temporally unstable population (b) missing data that is a result of sparsely sampled measurements and (c) relatively few available measurements prior to corrective action opportunities. In a real manufacturing pilot application, our automated models selected 125 fast wafers in real-time. As predicted, those wafers were significantly faster than average. During manufacture, downstream corrective processing restored 25 nominally unacceptable wafers to normal operation.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1258–1266},
numpages = {9},
keywords = {prediction, manufacturing, quality control},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2488217,
author = {Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils},
title = {Online Controlled Experiments at Large Scale},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488217},
doi = {10.1145/2487575.2488217},
abstract = {Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1168–1176},
numpages = {9},
keywords = {controlled experiments, randomized experiments, a/b testing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2488206,
author = {Wu, Huayu and Ng, Wee Siong and Tan, Kian-Lee and Wu, Wei and Xiang, Shili and Xue, Mingqiang},
title = {A Privacy Preserving Framework for Managing Vehicle Data in Road Pricing Systems},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488206},
doi = {10.1145/2487575.2488206},
abstract = {The Electronic Road Pricing (ERP) system was implemented by the Land Transport Authority of Singapore to control traffic by road pricing since 1998. To better understand the traffic condition and improve the pricing scheme, the government initiated the next generation ERP (ERP 2) project, which aims to use the Global Navigation Satellite System (GNSS) collecting positional data from vehicles for analysis. However, most drivers fear of being monitored once the government installs the devices in their vehicles to collect GPS data. The existing data stream management systems (DSMS) centralize both data management and privacy control at server site. This framework assumes DSMS server is secure and trustable, and protects providers' data from illegal access by data users. In ERP 2, the DSMS server is maintained by the government, i.e., data user. Thus, the existing framework is not adoptable. We propose a novel framework in which privacy protection is pushed to data provider site. By doing this, the system could be safer and more efficient. Our framework can be used for the situations such as ERP 2, i.e., data providers would like to control their own privacy policies and/or the workload of DSMS server needs to be reduced.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1427–1435},
numpages = {9},
keywords = {privacy preservation, hippocratic data stream systems, decentralized framework, road pricing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2488221,
author = {Asadi, Nima and Lin, Jimmy and Busch, Michael},
title = {Dynamic Memory Allocation Policies for Postings in Real-Time Twitter Search},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488221},
doi = {10.1145/2487575.2488221},
abstract = {We explore a real-time Twitter search application where tweets are arriving at a rate of several thousands per second. Real-time search demands that they be indexed and searchable immediately, which leads to a number of implementation challenges. In this paper, we focus on one aspect: dynamic postings allocation policies for index structures that are completely held in main memory. The core issue can be characterized as a "Goldilocks Problem". Because memory remains today a scare resource, an allocation policy that is too aggressive leads to inefficient utilization, while a policy that is too conservative is slow and leads to fragmented postings lists. We present a dynamic postings allocation policy that allocates memory in increasingly-larger "slices" from a small number of large, fixed pools of memory. With an analytical model and experiments, we explore different settings that balance time (query evaluation speed) and space (memory utilization).},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1186–1194},
numpages = {9},
keywords = {memory allocation, inverted indexing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2488212,
author = {Montgomery, Thomas A. and Stieg, Paul M. and Cavaretta, Michael J. and Moraal, Paul E.},
title = {Experience from Hosting a Corporate Prediction Market: Benefits beyond the Forecasts},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488212},
doi = {10.1145/2487575.2488212},
abstract = {Prediction markets are virtual stock markets used to gain insight and forecast events by leveraging the wisdom of crowds. Popularly applied in the public to cultural questions (election results, box-office returns), they have recently been applied by corporations to leverage employee knowledge and forecast answers to business questions (sales volumes, products and features, release timing). Determining whether to run a prediction market requires practical experience that is rarely described. Over the last few years, Ford Motor Company obtained practical experience by deploying one of the largest corporate prediction markets known. Business partners in the US, Europe, and South America provided questions on new vehicle features, sales volumes, take rates, pricing, and macroeconomic trends.We describe our experience, including both the strong and weak correlations found between predictions and real world results. Evaluating this methodology goes beyond prediction accuracy, however, since there are many side benefits. In addition to the predictions, we discuss the value of comments, stock price changes over time, the ability to overcome bureaucratic limits, and flexibly filling holes in corporate knowledge, enabling better decision making. We conclude with advice on running prediction markets, including writing good questions, market duration, motivating traders and protecting confidential information.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1384–1392},
numpages = {9},
keywords = {artificial markets, forecasting, social media, organizational knowledge, prediction markets},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2487678,
author = {Jha, Madhav and Seshadhri, C. and Pinar, Ali},
title = {A Space Efficient Streaming Algorithm for Triangle Counting Using the Birthday Paradox},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487678},
doi = {10.1145/2487575.2487678},
abstract = {We design a space efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox. When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O(√n) space (n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 60,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph, by storing a miniscule fraction of edges.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {589–597},
numpages = {9},
keywords = {triangle counting, streaming algorithms},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2487575.2487592,
author = {Lacoste-Julien, Simon and Palla, Konstantina and Davies, Alex and Kasneci, Gjergji and Graepel, Thore and Ghahramani, Zoubin},
title = {SIGMa: Simple Greedy Matching for Aligning Large Knowledge Bases},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487592},
doi = {10.1145/2487575.2487592},
abstract = {The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm that leverages both the structural information from the relationship graph and flexible similarity measures between entity properties in a greedy local search, which makes it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high accuracy. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {572–580},
numpages = {9},
keywords = {entity, greedy algorithm, knowledge base, relationship, large-scale, alignment},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2501025.2501027,
author = {Jin, Fang and Dougherty, Edward and Saraf, Parang and Cao, Yang and Ramakrishnan, Naren},
title = {Epidemiological Modeling of News and Rumors on Twitter},
year = {2013},
isbn = {9781450323307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2501025.2501027},
doi = {10.1145/2501025.2501027},
abstract = {Characterizing information diffusion on social platforms like Twitter enables us to understand the properties of underlying media and model communication patterns. As Twitter gains in popularity, it has also become a venue to broadcast rumors and misinformation. We use epidemiological models to characterize information cascades in twitter resulting from both news and rumors. Specifically, we use the SEIZ enhanced epidemic model that explicitly recognizes skeptics to characterize eight events across the world and spanning a range of event types. We demonstrate that our approach is accurate at capturing diffusion in these events. Our approach can be fruitfully combined with other strategies that use content modeling and graph theoretic features to detect (and possibly disrupt) rumors.},
booktitle = {Proceedings of the 7th Workshop on Social Network Mining and Analysis},
articleno = {8},
numpages = {9},
keywords = {epidemiological modeling, SIS, rumor detection, SEIZ},
location = {Chicago, Illinois},
series = {SNAKDD '13}
}

@inproceedings{10.1145/2505821.2505827,
author = {Cao, Zechun and Wang, Sujing and Forestier, Germain and Puissant, Anne and Eick, Christoph F.},
title = {Analyzing the Composition of Cities Using Spatial Clustering},
year = {2013},
isbn = {9781450323314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505821.2505827},
doi = {10.1145/2505821.2505827},
abstract = {Cities all around the world are in constant evolution due to numerous factors, such as fast urbanization and new ways of communication and transportation. Since understanding the composition of cities is the key to intelligent urbanization, there is a growing need to develop urban computing and analysis tools to guide the orderly development of cities, as well as to enhance their smooth and beneficiary evolution. This paper presents a spatial clustering approach to discover interesting regions and regions which serve different functions in cities. Spatial clustering groups the objects in a spatial dataset and identifies contiguous regions in the space of the spatial attributes. We formally define the task of finding uniform regions in spatial data as a maximization problem of a plug-in measure of uniformity and introduce a prototype-based clustering algorithm named CLEVER to find such regions. Moreover, polygon models which capture the scope of a spatial cluster and histogram-style distribution signatures are used to annotate the content of a spatial cluster in the proposed methodology; they play a key role in summarizing the composition of a spatial dataset. Furthermore, algorithms for identifying popular distribution signatures and approaches for identifying regions which express a particular distribution signature will be presented. The proposed methodology is demonstrated and evaluated in a challenging real-world case study centering on analyzing the composition of the city of Strasbourg in France.},
booktitle = {Proceedings of the 2nd ACM SIGKDD International Workshop on Urban Computing},
articleno = {14},
numpages = {8},
keywords = {urban computing, spatial data mining, finding uniform regions in spatial datasets, algorithms to discover the spatial structure of a city, spatial clustering, region discovery},
location = {Chicago, Illinois},
series = {UrbComp '13}
}

@inproceedings{10.1145/2502069.2502072,
author = {Monti, Corrado and Rozza, Alessandro and Zappella, Giovanni and Zignani, Matteo and Arvidsson, Adam and Colleoni, Elanor},
title = {Modelling Political Disaffection from Twitter Data},
year = {2013},
isbn = {9781450323321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502069.2502072},
doi = {10.1145/2502069.2502072},
abstract = {Twitter is one of the most popular micro-blogging services in the world, often studied in the context of political opinion mining for its peculiar nature of online public discussion platform. In our work we analyse the phenomenon of political disaffection defined as the "lack of confidence in the political process, politicians, and democratic institutions, but with no questioning of the political regime". Disaffection for organised political parties and institutions has been object of studies and media attention in several Western countries. Especially the Italian case has shown a wide diffusion of this attitude. For this reason, we collect a massive database of Italian Twitter data (about 35 millions of tweets from April 2012 to October 2012) and we exploit scalable state-of-the-art machine learning techniques to generate time-series concerning the political disaffection discourse.In order to validate the quality of the time-series generated, we compare them with indicators of political disaffection from public opinion surveys. We find political disaffection on Twitter to be highly correlated with the indicators of political disaffection in the public opinion surveys. Moreover, we show the peaks in the time-series are often generated by external political events reported on the main newspapers.},
booktitle = {Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining},
articleno = {3},
numpages = {9},
keywords = {sentiment analysis, political disaffection, classification, Twitter},
location = {Chicago, Illinois},
series = {WISDOM '13}
}

@inproceedings{10.1145/2517288.2517294,
author = {Li, Jiefei and Liang, Xiaocong and Ding, Weijie and Yang, Weidong and Pan, Rong},
title = {Feature Engineering and Tree Modeling for Author-Paper Identification Challenge},
year = {2013},
isbn = {9781450324953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517288.2517294},
doi = {10.1145/2517288.2517294},
abstract = {The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom. Microsoft Academic Search is an open platform, which provides a variety of metrics and experiences for the research community, in addition to literature search. As the covering data came from many sources, the profile of an author with an ambiguous name tends to contain noise, resulting in papers that are incorrectly assigned to others. KDD Cup 2013 Track 1 challenges participants to determine which papers in an author profile were truly written by the given author.In this work, we present how to use tree-base models to accurately predict the paper author. We incorporate feature engineering into the models with the advantages of them. This paper introduces two kinds of tree-base models (GB-DT [4], RGF [5]) and presents in detail the learning algorithm and how features can be generated for the task. The experimental results show the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 2013 KDD Cup 2013 Workshop},
articleno = {5},
numpages = {8},
keywords = {ensemble tree model, feature engineering},
location = {Chicago, Illinois},
series = {KDD Cup '13}
}

@inproceedings{10.1145/2491159.2491162,
author = {Moore, Robert S. and Firner, Bernhard and Xu, Chenren and Howard, Richard and Martin, Richard P. and Zhang, Yanyong},
title = {It's Tea Time: Do You Know Where Your Mug Is?},
year = {2013},
isbn = {9781450321778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491159.2491162},
doi = {10.1145/2491159.2491162},
abstract = {The transition to Internet of Things depends on the ability to create small, simple applications that are easily written and can be flexibly combined into larger, more powerful systems. We have designed an infrastructure to meet this need and report on a year's experience expanding and using it in an open-plan academic office space with up to a hundred sensors enabling nearly a dozen applications ranging from announcing tea time in the break room, notifying users that the conference room is in use, to printing documents from a web-based map. Applications are simple to write, modular, easily reused, and can incorporate diverse data inputs in a heterogeneous sensing environment. We discuss our efforts to incrementally improve user interfaces and system management.},
booktitle = {Proceedings of the 5th ACM Workshop on HotPlanet},
pages = {63–68},
numpages = {6},
keywords = {modularity, smart building, reusability},
location = {Hong Kong, China},
series = {HotPlanet '13}
}

@inproceedings{10.1145/2499788.2499826,
author = {Deng, Lei and Xu, Bingying and Zhang, Lumin and Han, Yi and Zhou, Bin and Zou, Peng},
title = {Tracking the Evolution of Public Concerns in Social Media},
year = {2013},
isbn = {9781450322522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499788.2499826},
doi = {10.1145/2499788.2499826},
abstract = {Microblogging is becoming a popular social media in recent years. Observations show that a large part of posts in microblogging were talking about public events occurred in the real world. Public concerns reflect interests and expectations of the mass for an event. Therefore, to understand and analyze of public concerns will help us to grasp an event, and predict its trend.This paper presents an evolution analysis method of public concerns for a special kind of post in microblogging, which can provides sufficient background information about an event by its attachments, e.g. a URL for details, a picture, or a video, etc. we called it expandable post. We use expandable posts to reconstruct the topic space. Their reposts are regarded as public concerns, and are located on the space. Thus, the task of tracking public concerns is transformed into tracking the movement of those reposts, and analyzing the relationships between them and their corresponding expandable posts on the topic space. The preliminary experiments on our dataset about H7N9 bird flu collected from Weibo, shows the effectiveness of our method.},
booktitle = {Proceedings of the Fifth International Conference on Internet Multimedia Computing and Service},
pages = {353–357},
numpages = {5},
keywords = {evolution, public concern, microblogging},
location = {Huangshan, China},
series = {ICIMCS '13}
}

@inproceedings{10.1145/2491411.2491412,
author = {Grechanik, Mark and Hossain, B. M. Mainul and Buy, Ugo and Wang, Haisheng},
title = {Preventing Database Deadlocks in Applications},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491412},
doi = {10.1145/2491411.2491412},
abstract = { Many organizations deploy applications that use databases by sending Structured Query Language (SQL) statements to them and obtaining data that result from the execution of these statements. Since applications often share the same databases concurrently, database deadlocks routinely occur in these databases resulting in major performance degradation in these applications. Database engines do not prevent database deadlocks for the same reason that the schedulers of operating system kernels do not preempt processes in a way to avoid race conditions and deadlocks - it is not feasible to find an optimal context switching schedule quickly for multiple processes (and SQL statements), and the overhead of doing it is prohibitive.  We created a novel approach that combines run-time monitoring, which automatically prevents database deadlocks, with static analysis, which detects hold-and-wait cycles that specify how resources (e.g., database tables) are held in contention during executions of SQL statements. We rigorously evaluated our approach. For a realistic case of over 1,200 SQL statements, our algorithm detects all hold-and-wait cycles in less than two seconds. We built a toolset and experimented with three applications. Our tool prevented all existing database deadlocks in these applications and increased their throughputs by up to three orders of magnitude. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {356–366},
numpages = {11},
keywords = {Petri net, database deadlock, hold-and-wait cycle},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/2491411.2491455,
author = {Davril, Jean-Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
title = {Feature Model Extraction from Large Collections of Informal Product Descriptions},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491455},
doi = {10.1145/2491411.2491455},
abstract = { Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {290–300},
numpages = {11},
keywords = {Domain Analysis, Feature Models, Product Lines},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/2522548.2523138,
author = {Sinha, Tanmay and Srikanth, Vrns and Sain, Mangal and Lee, Hoon Jae},
title = {Trends and Research Directions for Privacy Preserving Approaches on the Cloud},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523138},
doi = {10.1145/2522548.2523138},
abstract = {With advancements in hardware and software capabilities, cloud computing has evolved into a widely utilized paradigm for pay per use and robust computation. An unparalleled repository of user sensitive data that resides on the cloud poses severe threat to the privacy of individuals. Users authenticate, store and perform computations on their data using cloud services. From the cloud's perspective, it gathers additional user data via ubiquitous devices, mines this information to offer personalized services like recommendations and disseminates the results. However, the interactions between the cloud and user at each stage of this pipeline development is limited by privacy concerns. In recent years, much work has been done on designing privacy preserving approaches for improving cloud security and the trust network. A wide array of data mining, cryptography and information hiding techniques have been applied to cater to different aspects of providing risk free work environment in the cloud. Given the lack of management of this information, a systematic investigation is required to structurally organize the topics studied. This paper aims to clearly portray the stringent and urgent need for applying privacy preserving approaches to the cloud and highlight the relevant work that has been done along these lines. The key objective is to identify important areas of user-cloud interaction and demonstrate a survey on the state of the art algorithms that have led to improved cloud privacy in these areas The focus is on exploring criteria for the impact of such approaches on user cloud interaction. An understanding of the research issues associated with these sensitive areas of cloud computing may enable us to better leverage benefits of the cloud and reflect on future possibilities of exploration.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {21},
numpages = {12},
keywords = {data mining, personalization, privacy preservation, storage, cloud computing},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2492517.2500301,
author = {Farzanyar, Zahra and Cercone, Nick},
title = {Efficient Mining of Frequent Itemsets in Social Network Data Based on MapReduce Framework},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500301},
doi = {10.1145/2492517.2500301},
abstract = {Social Networks promote information sharing between people everywhere and at all times. Mining data produced in this data-rich environment can be extremely useful. Frequent itemset mining plays an important role in mining associations, correlations, sequential patterns, causality, episodes, multidimensional patterns, max-patterns, partial periodicity, emerging patterns, and many other significant data mining tasks in social networks. With the exponential growth of social network data towards a terabyte or more, most of the traditional frequent itemset mining algorithms become ineffective due to either huge resource requirements or large communications overhead. Cloud computing has proved that processing very large datasets over commodity clusters can be done by providing the right programming model. As a parallel programming model, MapReduce, one of most important techniques for cloud computing, has emerged in the mining of datasets of terabyte scale or larger on clusters of computers. In this paper, we propose an efficient frequent itemset mining algorithm, called IMRApriori, based on MapReduce framework which deals with Hadoop cloud, a parallel store and computing platform. The paper demonstrates experimental results to corroborate the theoretical claims.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1183–1188},
numpages = {6},
keywords = {mapreduce, cloud computing, social networks, frequent itemset mining},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2492542,
author = {Tagarelli, Andrea and Interdonato, Roberto},
title = {"Who's out There?": Identifying and Ranking Lurkers in Social Networks},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492542},
doi = {10.1145/2492517.2492542},
abstract = {The massive presence of silent members in online communities, the so-called lurkers, has long attracted the attention of researchers in social science, cognitive psychology, and computer-human interaction. However, the study of lurking phenomena represents an unexplored opportunity of research in data mining, information retrieval and related fields. In this paper, we take a first step towards the formal specification and analysis of lurking in social networks. Particularly, focusing on the network topology, we address the new problem of lurker ranking and propose the first centrality methods specifically conceived for ranking lurkers in social networks. Using Twitter and FriendFeed as cases in point, our methods' performance was evaluated against data-driven rankings as well as existing centrality methods, including the classic PageRank and alpha-centrality. Empirical evidence has shown the significance of our lurker ranking approach, which substantially differs from other methods in effectively identifying and ranking lurkers.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {215–222},
numpages = {8},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2500328,
author = {Zunino, Rodolfo and Bisio, Federica and Peretti, Chiara and Surlinelli, Roberto and Scillia, Eugenio and Ottaviano, Augusto and Sangiacomo, Fabio},
title = {An Analyst-Adaptive Approach to Focused Crawlers},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500328},
doi = {10.1145/2492517.2500328},
abstract = {The paper presents a general methodology to implement a flexible Focused Crawler for investigation purposes, monitoring, and Open Source Intelligence (OSINT). The resulting tool is specifically aimed to fit the operational requirements of law-enforcement agencies and intelligence analyst. The architecture of the semantic Focused Crawler features static flexibility in the definition of desired concepts, used metrics, and crawling strategy; in addition, the method is capable to learn (and adapt to) the analyst's expectations at runtime. The user may instruct the crawler with a binary feedback (yes/no) about the current performance of the surfing process, and the crawling engine progressively refines the expected targets accordingly. The method implementation is based on an existing text-mining environment, integrated with semantic networks and ontologies. Experimental results witness the effectiveness of the adaptive mechanism.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1073–1077},
numpages = {5},
keywords = {focused crawler, OSINT, analyst-adaptation},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2500261,
author = {Johansson, Fredrik and Kaati, Lisa and Shrestha, Amendra},
title = {Detecting Multiple Aliases in Social Media},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500261},
doi = {10.1145/2492517.2500261},
abstract = {Monitoring and analysis of web forums is becoming important for intelligence analysts around the globe since terrorists and extremists are using forums for spreading propaganda and communicating with each other. Various tools for analyzing the content of forum postings and identifying aliases that need further inspection by analysts have been proposed throughout literature, but a problem related to this is that individuals can make use of several aliases. In this paper we propose a number of matching techniques for detecting forum users who make use of multiple aliases. By combining different techniques such as time profiling and stylometric analysis of messages the accuracy of recognizing users with multiple aliases increases, as shown in experiments conducted on the ICWSM dataset boards.ie.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1004–1011},
numpages = {8},
keywords = {time profiling, multiple aliases, alias matching},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2492536,
author = {Chakrabort, Tanmoy and Sikdar, Sandipan and Tammana, Vihar and Ganguly, Niloy and Mukherjee, Animesh},
title = {Computer Science Fields as Ground-Truth Communities: Their Impact, Rise and Fall},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492536},
doi = {10.1145/2492517.2492536},
abstract = {Study of community in time-varying graphs has been limited to its detection and identification across time. However, presence of time provides us with the opportunity to analyze the interaction patterns of the communities, understand how each individual community grows/shrinks, becomes important over time. This paper, for the first time, systematically studies the temporal interaction patterns of communities using a large scale citation network (directed and unweighted) of computer science. Each individual community in a citation network is naturally defined by a research field -- i.e., acting as ground-truth -- and their interactions through citations in real time can unfold the landscape of dynamic research trends in the computer science domain over the last fifty years. These interactions are quantified in terms of a metric called inwardness that captures the effect of local citations to express the degree of authoritativeness of a community (research field) at a particular time instance. Several arguments to unveil the reasons behind the temporal changes of inwardness of different communities are put forward using exhaustive statistical analysis. The measurements (importance of field) are compared with the project funding statistics of NSF and it is found that the two are in sync. We believe that this measurement study with a large real-world data is an important initial step towards understanding the dynamics of cluster-interactions in a temporal environment. Note that this paper, for the first time, systematically outlines a new avenue of research that one can practice post community detection.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {426–433},
numpages = {8},
keywords = {temporal network, ground-truth communities, computer science, community analysis, citation network},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2500258,
author = {Musial, Katarzyna and Gabrys, Bogdan and Buczko, Marcin},
title = {What Kind of Network Are You? Using Local and Global Characteristics in Network Categorisation Tasks},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500258},
doi = {10.1145/2492517.2500258},
abstract = {The amount of research done in the area of real--world networked systems is rapidly growing. Everybody knows what six degrees of separation or small--world phenomenon are. Scientists very easily give labels to the networks they analyse. If it has power law node degree distribution then it has to be scale--free network or if there is high clustering coefficient then it must be small--world network. These simplifications, although convenient, are not always very useful from the perspective of understanding phenomena existing within the network. In this paper we decided to go back to the basics and investigate whether analysis of one single measure is enough to describe a network. We analyse both local and global characteristics in order to discover the "true" nature of a network. Not only using local and/or global measures can lead to different classification of a network but we also show how significantly different interpretation can result from analysing the same data by building network models as directed/undirected and/or weighted/binary graphs.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1366–1373},
numpages = {8},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2500237,
author = {Huang, Liang-Cheng and Liu, Wei-Chung and Chou, Seng-Cho T.},
title = {Howcare: A Personal Health Cloud Archive and Care-Partners' Community},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500237},
doi = {10.1145/2492517.2500237},
abstract = {Nowadays, most people care about their personal health, no matter mental or physical health in their daily life. They sustain and improve their health status with exercise, diet control, adopt good sleep habit, keep natural patterns on sleeping and bowel movement. These people need a tool for monitor and record long-termly their own health status. On the other hand, some people do not see they need to change their health related lifestyle to improve their health status until they are diagnosed with diseases. While he/she is sick, he/she also need to write down their health diary he/herself or the caregiver (most of them are the disadvantaged in their family) for the physician to monitor the illness.In this paper we proposed a social network service named HowCare, a caregiver based social support online community, with a personal health cloud archive and its unique designs with "HealthRank" algorithm to match caregiver's social network with correlated illness situation they face to.The aim of HowCare are, to help people keep their own health data on the cloud and allows patients or caregiver with the same disease to interact with each other, and through the social network and telehealth design, it will influence the patient's willingness to accept healthier life and improve health status},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1237–1241},
numpages = {5},
keywords = {caregiver, personal health information, online health community, health cloud, social support},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2500271,
author = {Oostdijk, Nelleke and van Halteren, Hans},
title = {Shallow Parsing for Recognizing Threats in Dutch Tweets},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500271},
doi = {10.1145/2492517.2500271},
abstract = {In this paper, we investigate the recognition of threats in Dutch tweets. As tweets often display irregular grammatical form and deviant orthography, analysis by standard means is problematic. Therefore, we have implemented a new shallow parsing mechanism which is driven by handcrafted rules. Experimental results are encouraging, with an F-measure of about 40% on a random sample of Dutch tweets. Moreover, the error analysis shows some clear avenues for further improvement.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1034–1041},
numpages = {8},
keywords = {threats, Twitter, handcrafted rules, Dutch, social media},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2492553,
author = {Chen, Cheng and Wu, Kui and Srinivasan, Venkatesh and Bharadwaj, R. Kesav},
title = {The Best Answers? Think Twice: Online Detection of Commercial Campaigns in the CQA Forums},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492553},
doi = {10.1145/2492517.2492553},
abstract = {In an emerging trend, more and more Internet users search for information from Community Question and Answer (CQA) websites, as interactive communication in such websites provides users with a rare feeling of trust. More often than not, end users look for instant help when they browse the CQA websites for the best answers. Hence, it is imperative that they should be warned of any potential commercial campaigns hidden behind the answers. Existing research focuses more on the quality of answers and does not meet the above need. Textual similarities between questions and answers are widely used in previous research. However, this feature will no longer be effective when facing commercial paid posters. More context information, such as writing templates and a user's reputation track need to be combined together to form a new model to detect the potential campaign answers. In this paper, we develop a system that automatically analyzes the hidden patterns of commercial spam and raises alarms instantaneously to end users whenever a potential commercial campaign is detected. Our detection method integrates semantic analysis and posters' track records and utilizes the special features of CQA websites largely different from those in other types of forums such as microblogs or news reports. Our system is adaptive and accommodates new evidence uncovered by the detection algorithms over time. Validated with real-world trace data from a popular Chinese CQA website over a period of three months, our system shows great potential towards adaptive online detection of CQA spams.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {458–465},
numpages = {8},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2492562,
author = {Steurer, Michael and Trattner, Christoph},
title = {Acquaintance or Partner? Predicting Partnership in Online and Location-Based Social Networks},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492562},
doi = {10.1145/2492517.2492562},
abstract = {Existing approaches to predicting tie strength between users involve either online social networks or location-based social networks. To date, few studies combined these networks to investigate the intensity of social relations between users. In this paper we analyzed tie strength defined as partners and acquaintances in two domains: a location-based social network and an online social network (Second Life). We compared user pairs in terms of their partnership and found significant differences between partners and acquaintances. Following these observations, we evaluated the social proximity of users via supervised and unsupervised learning algorithms and established that homophilic features were most valuable for the prediction of partnership.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {372–379},
numpages = {8},
keywords = {online social networks, location-based social networks, partner prediction, virtual worlds},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2500226,
author = {Wu, Peggy and Rye, Jeffrey and Miller, Christopher and Schmer-Galunder, Sonja and Ott, Tammy},
title = {Non-Intrusive Detection of Psycho-Social Dimensions Using Sociolinguistics},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500226},
doi = {10.1145/2492517.2500226},
abstract = {Long duration space flights such as a two and a half year mission to Mars present many unique challenges to the behavioral health of astronauts. Factors such as social monotony, workload, a confined environment, sensory deprivation, and limited access to family and psychosocial support can affect crew welfare and task performance. NASA recognizes a "risk of performance decrements due to inadequate cooperation, coordination, communication, and psychosocial adaptation within a team;" reports from Mir revealed that conflicts between crew members have resulted in early termination of missions. Currently, flight crews and support staff have real time voice and video communications capabilities on the International Space Station to keep astronauts connected, and allow operations staff to monitor the crew's well-being. However, communications for long duration missions will likely be limited and disrupted by time latencies. Crew workload may also prohibit crew members from providing the extensive self-reports that the Earth-bound support team needs to accurately access the crew's psychological health. Further, the metrics of interest are difficult to obtain because some are inherently qualitative, while others may not be amendable to self-reports. We first describe an extensive review of psycho-social dimensions relevant to long duration space flight, their manifestations, and possible detection methods. We then describe a novel method of non-intrusive detection developed initially for application in the Empire Challenge military exercise in 2010. This system, called ADMIRE for Assessment of Discourse Media Indicators of Relative Esteem, leverages prior work in cultural and socio-linguistic theory to develop standardized, non-intrusive methods for data collection and knowledge extraction about factors salient to group psychosocial dynamics. Finally, we describe our approach to follow-up work applying ADMIRE to historical space flight data, as well as in ongoing studies in space analog environments to identify potential changes in individual and team psycho-social factors before they lead to deficits in health and task performance.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1337–1344},
numpages = {8},
keywords = {politeness, long duration space mission, psycho-social measures, communications, socio-linguistics, power and social dynamics, non-intrusive detection},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2492522,
author = {Skillicorn, D. B. and Zheng, Q. and Morselli, C.},
title = {Spectral Embedding for Dynamic Social Networks},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492522},
doi = {10.1145/2492517.2492522},
abstract = {The interactions in real-world social networks change over time. Dynamic social network analysis aims to understand the structures in networks as they evolve, building on static analysis techniques but including variation. Working directly with the graphs that represent social networks is difficult, and it has become common to use spectral techniques that embed graphs in a geometry and then work with the geometry instead. We extend such spectral techniques to dynamically changing data by binding network snapshots at different times into a single directed graph structure in a way that keeps structures aligned. This global network can then be embedded. Pairwise similarity, as well as community and cluster structures can be tracked over time, and the idea of the trajectory of a node across time becomes meaningful. We illustrate the approach using a real-world dataset, the Caviar drug-trafficking network.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {316–323},
numpages = {8},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2492517.2492518,
author = {Ovelg\"{o}nne, Michael},
title = {Distributed Community Detection in Web-Scale Networks},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492518},
doi = {10.1145/2492517.2492518},
abstract = {Partitioning large networks into smaller subnetworks (communities) is an important tool to analyze the structure of complex linked systems. In recent years, many in-memory community detection algorithms have been proposed for graphs with millions of edges. Analyzing massive graphs with billions of edges is impossible for existing algorithms. In this contribution, we show how to find community partitions of networks with billions of edges. Our approach is based on an ensemble learning scheme for community detection that provides a way to identify high quality partitions from an ensemble of partitions with lower quality. We present a pre-processing procedure for community detection algorithms that significantly decreases the problem size. After reducing the problem size, traditional non-distributed community detection algorithms can be applied. We implemented a weak but highly scalable label propagation algorithm on top of the distributed-computing framework Apache Hadoop. The evaluation of our implementation on a 50-node Hadoop cluster and with evaluation datasets up to 3.3 billion edges shows very good results with respect to clustering quality as well as scalability. For a smaller 260 million edge network, we show that our preprocessing can improve the results of the popular Louvain modularity clustering algorithm.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {66–73},
numpages = {8},
keywords = {distributed algorithms, graph clustering, MapReduce, community detection},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2491627.2491641,
author = {Koziolek, Heiko and Goldschmidt, Thomas and de Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan},
title = {Experiences from Identifying Software Reuse Opportunities by Domain Analysis},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491641},
doi = {10.1145/2491627.2491641},
abstract = {In a large corporate organization there are sometimes similar software products in certain subdomains with a perceived functional overlap. This promises to be an opportunity for systematic reuse to reduce software development and maintenance costs. In such situations companies have used different domain analysis approaches (e.g., SEI Technical Probe) that helped to assess technical and organizational potential for a software product line approach. We applied existing domain analysis approaches for software product line engineering and tailored them to include a feature analysis as well as architecture evaluation. In this paper, we report our experiences from applying the approach in two subdomains of industrial automation.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {208–217},
numpages = {10},
keywords = {domain analysis, software product lines, business case},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2491627.2491642,
author = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
title = {Communication Factors for Speed and Reuse in Large-Scale Agile Software Development},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491642},
doi = {10.1145/2491627.2491642},
abstract = {An open issue in industry is the combination of software reuse in the context of large scale Agile Software Development. The speed offered by Agile Software Development is needed for short time to market, while reuse strategies such as Software Product Line Engineering are needed for long-term productivity, efficiency, and profit. The paper investigates, through a survey, communication factors affecting both speed and reuse in 3 large companies developing embedded systems and employing Agile Software Development and Software Product Line Engineering. Our results include a prioritized list of communication related factors obtained by statistical analysis and the recognition and spread of the factors in the companies. We have recognized 5 interfaces with the Agile development team that need to be improved: system engineers (architects), product management, distributed teams, inter-project teams and sales unit. Few factors (involving inter-project communication) depend on the business drivers for the company. We also reveal that Agile teams need strategic and architectural inputs in order to be implanted in a large company employing Software Product Line Engineering. Academic and industrial training as well as different tactics for co-location would improve the communication skills of engineers. There is also a need for solutions, in the reference architecture, for fostering Agile Software Development: the goal is the combination of the focus on customer value of the teams, reusability, system requirements and avoidance of organizational dependencies.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {42–51},
numpages = {10},
keywords = {communication, development speed, embedded systems, factors, software reuse, software process improvement (SPI), speed, agile software development},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2493190.2493245,
author = {Reilly, Derek and MacKay, Bonnie},
title = {Annotating Ecology: Looking to Biological Fieldwork for Mobile Spatial Annotation Workflows},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493245},
doi = {10.1145/2493190.2493245},
abstract = {We present findings from a qualitative study of the spatial practices of biological fieldwork. We argue that these fieldwork practices inform a vision of decentralized spatial annotation in which a variety of motivations, needs, and perspectives coexist, and may support each other synergistically. We contrast this with current and past designs of mobile spatial annotation systems in the literature. From our analysis we identify three guidelines for mobile annotation systems design in biological fieldwork that we argue also extend to other domains: allowing the management of space through user control over annotation processes, promoting structured but flexible annotation through user-defined annotation formats, and providing robust and comprehensive integration of disparate data sources to allow ad hoc, exploratory queries.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {35–44},
numpages = {10},
keywords = {fieldwork, mobile spatial annotation},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493223,
author = {Muslukhov, Ildar and Boshmaf, Yazan and Kuo, Cynthia and Lester, Jonathan and Beznosov, Konstantin},
title = {Know Your Enemy: The Risk of Unauthorized Access in Smartphones by Insiders},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493223},
doi = {10.1145/2493190.2493223},
abstract = {Smartphones store large amounts of sensitive data, such as SMS messages, photos, or email. In this paper, we report the results of a study investigating users' concerns about unauthorized data access on their smartphones (22 interviewed and 724 surveyed subjects). We found that users are generally concerned about insiders (e.g., friends) accessing their data on smartphones. Furthermore, we present the first evidence that the insider threat is a real problem impacting smartphone users. In particular, 12% of subjects reported a negative experience with unauthorized access. We also found that younger users are at higher risk of experiencing unauthorized access. Based on our results, we propose a stronger adversarial model that incorporates the insider threat. To better reflect users' concerns and risks, a stronger adversarial model must be considered during the design and evaluation of data protection systems and authentication methods for smartphones.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {271–280},
numpages = {10},
keywords = {loss, insider, user study, theft, stranger, smartphone, physical threats},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2493190.2493209,
author = {Guha, Shion and Birnholtz, Jeremy},
title = {Can You See Me Now? Location, Visibility and the Management of Impressions on Foursquare},
year = {2013},
isbn = {9781450322737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493190.2493209},
doi = {10.1145/2493190.2493209},
abstract = {Location based social networking applications enable people to share their location with friends for social purposes by "checking in" to places they visit. Prior research suggests that both privacy and impression management motivate location disclosure concerns. In this interview study of foursquare users, we explore the ways people think about location sharing and its effects on impression management and formation. Results indicate that location-sharing decisions depend on the perceived visibility of the check-in, blur boundaries between public and private venues, and can initiate tensions within the foursquare friend network. We introduce the concept of "check-in transience" to explain factors contributing to impression management and argue that sharing location is often used as a signaling strategy to achieve social objectives.},
booktitle = {Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {183–192},
numpages = {10},
keywords = {impression management, visibility, check-in, foursquare},
location = {Munich, Germany},
series = {MobileHCI '13}
}

@inproceedings{10.1145/2509315.2509317,
author = {Rakoczi, Gergely and Duchowski, Andrew and Casas-Tost, Helena and Pohl, Margit},
title = {Visual Perception of International Traffic Signs: Influence of e-Learning and Culture on Eye Movements},
year = {2013},
isbn = {9781450321105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509315.2509317},
doi = {10.1145/2509315.2509317},
abstract = {Various eye movement metrics were recorded during the visual perception of international traffic signs embedded within an e-learning course designed to familiarize participants with foreign signage. Goals of the were to gauge differences in task types, sign origin, and ethnicity (American, Chinese, and Austrian) as well as effectiveness of the e-learning teaching materials in terms of prior preparation. Results, in contrast to other studies, suggest that teaching materials had no overall effect on either eye movement metrics nor on task success rates. Instead, sign origin had the strongest effect on gaze, as foreign signs in mixed presentation with domestic signs, elicited a larger number of fixations with longer mean fixation durations, highest regression rates, and lower performance scores. Possible effects of ethnicity were also noted: Americans showed lower mean fixation durations over the entire experiment, independent of test conditions, with Chinese participants fixating faster on (correct) road signs than the other ethnic groups.},
booktitle = {Proceedings of the 2013 Conference on Eye Tracking South Africa},
pages = {8–16},
numpages = {9},
keywords = {visual perception, e-learning, signage and culture, eye tracking, visual behaviour, learning instructions},
location = {Cape Town, South Africa},
series = {ETSA '13}
}

@article{10.1145/2501654.2501661,
author = {Sherchan, Wanita and Nepal, Surya and Paris, Cecile},
title = {A Survey of Trust in Social Networks},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2501654.2501661},
doi = {10.1145/2501654.2501661},
abstract = {Web-based social networks have become popular as a medium for disseminating information and connecting like-minded people. The public accessibility of such networks with the ability to share opinions, thoughts, information, and experience offers great promise to enterprises and governments. In addition to individuals using such networks to connect to their friends and families, governments and enterprises have started exploiting these platforms for delivering their services to citizens and customers. However, the success of such attempts relies on the level of trust that members have with each other as well as with the service provider. Therefore, trust becomes an essential and important element of a successful social network. In this article, we present the first comprehensive review of social and computer science literature on trust in social networks. We first review the existing definitions of trust and define social trust in the context of social networks. We then discuss recent works addressing three aspects of social trust: trust information collection, trust evaluation, and trust dissemination. Finally, we compare and contrast the literature and identify areas for further research in social trust.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {47},
numpages = {33},
keywords = {social networks, Trust management, trust models, social trust}
}

@article{10.1145/2518017.2518019,
author = {Zhang, Zhuoyao and Cherkasova, Ludmila and Verma, Abhishek and Loo, Boon Thau},
title = {Performance Modeling and Optimization of Deadline-Driven Pig Programs},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {1556-4665},
url = {https://doi.org/10.1145/2518017.2518019},
doi = {10.1145/2518017.2518019},
abstract = {Many applications associated with live business intelligence are written as complex data analysis programs defined by directed acyclic graphs of MapReduce jobs, for example, using Pig, Hive, or Scope frameworks. An increasing number of these applications have additional requirements for completion time guarantees. In this article, we consider the popular Pig framework that provides a high-level SQL-like abstraction on top of MapReduce engine for processing large data sets. There is a lack of performance models and analysis tools for automated performance management of such MapReduce jobs. We offer a performance modeling environment for Pig programs that automatically profiles jobs from the past runs and aims to solve the following inter-related problems: (i) estimating the completion time of a Pig program as a function of allocated resources; (ii) estimating the amount of resources (a number of map and reduce slots) required for completing a Pig program with a given (soft) deadline. First, we design a basic performance model that accurately predicts completion time and required resource allocation for a Pig program that is defined as a sequence of MapReduce jobs: predicted completion times are within 10% of the measured ones. Second, we optimize a Pig program execution by enforcing the optimal schedule of its concurrent jobs. For DAGs with concurrent jobs, this optimization helps reducing the program completion time: 10%--27% in our experiments. Moreover, it eliminates possible nondeterminism of concurrent jobs’ execution in the Pig program, and therefore, enables a more accurate performance model for Pig programs. Third, based on these optimizations, we propose a refined performance model for Pig programs with concurrent jobs. The proposed approach leads to significant resource savings (20%--60% in our experiments) compared with the original, unoptimized solution. We validate our solution using a 66-node Hadoop cluster and a diverse set of workloads: PigMix benchmark, TPC-H queries, and customized queries mining a collection of HP Labs’ web proxy logs.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = sep,
articleno = {14},
numpages = {28},
keywords = {Hadoop, Pig, scheduling, resource allocation}
}

@article{10.1145/2508970,
author = {Oswald, Marion},
title = {Something Bad Might Happen: Lawyers, Anonymization and Risk},
year = {2013},
issue_date = {Fall 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2508970},
doi = {10.1145/2508970},
abstract = {The line between personal and anonymous information is often unclear. Increasingly it falls to lawyers to understand and manage the risks associated with the sharing of "anonymized" data sets.},
journal = {XRDS},
month = sep,
pages = {22–26},
numpages = {5}
}

@article{10.1145/2510123,
author = {Lewis, Clayton and Treviranus, Jutta},
title = {Public Policy and the Global Public Inclusive Infrastructure Project},
year = {2013},
issue_date = {September + October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/2510123},
doi = {10.1145/2510123},
abstract = {Public policy increasingly plays a role in influencing the work that we do as HCI researchers, interaction designers, and practitioners. "Public policy" is a broad term that includes both government policy and policy within non-governmental organizations, such as standards bodies. The Interacting with Public Policy forum focuses on topics at the intersection of human-computer interaction and public policy. Jonathan Lazar, Editor},
journal = {Interactions},
month = sep,
pages = {62–66},
numpages = {5}
}

@article{10.1145/2517998,
author = {Birch, David},
title = {Talking 'Bout Your Reputation},
year = {2013},
issue_date = {Fall 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2517998},
doi = {10.1145/2517998},
abstract = {People think they want anonymity, but actually desire privacy. But how do we reframe the debate surrounding privacy and security? Perhaps technology is the answer.},
journal = {XRDS},
month = sep,
pages = {32–35},
numpages = {4}
}

@article{10.1145/2516960,
author = {Serwadda, Abdul and Phoha, Vir V.},
title = {Examining a Large Keystroke Biometrics Dataset for Statistical-Attack Openings},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1094-9224},
url = {https://doi.org/10.1145/2516960},
doi = {10.1145/2516960},
abstract = {Research on keystroke-based authentication has traditionally assumed human impostors who generate forgeries by physically typing on the keyboard. With bots now well understood to have the capacity to originate precisely timed keystroke sequences, this model of attack is likely to underestimate the threat facing a keystroke-based system in practice. In this work, we investigate how a keystroke-based authentication system would perform if it were subjected to synthetic attacks designed to mimic the typical user. To implement the attacks, we perform a rigorous statistical analysis on keystroke biometrics data collected over a 2-year period from more than 3000 users, and then use the observed statistical traits to design and launch algorithmic attacks against three state-of-the-art password-based keystroke verification systems.Relative to the zero-effort attacks typically used to test the performance of keystroke biometric systems, we show that our algorithmic attack increases the mean Equal Error Rates (EERs) of three high performance keystroke verifiers by between 28.6% and 84.4%. We also find that the impact of the attack is more pronounced when the keystroke profiles subjected to the attack are based on shorter strings, and that some users see considerably greater performance degradation under the attack than others. This article calls for a shift from the traditional zero-effort approach of testing the performance of password-based keystroke verifiers, to a more rigorous algorithmic approach that captures the threat posed by today’s bots.},
journal = {ACM Trans. Inf. Syst. Secur.},
month = sep,
articleno = {8},
numpages = {30},
keywords = {keystroke dynamics, spoofing attacks, Biometrics}
}

@article{10.14778/2732219.2732227,
author = {Balkesen, Cagri and Alonso, Gustavo and Teubner, Jens and \"{O}zsu, M. Tamer},
title = {Multi-Core, Main-Memory Joins: Sort <i>vs.</i> Hash Revisited},
year = {2013},
issue_date = {September 2013},
publisher = {VLDB Endowment},
volume = {7},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732219.2732227},
doi = {10.14778/2732219.2732227},
abstract = {In this paper we experimentally study the performance of main-memory, parallel, multi-core join algorithms, focusing on sort-merge and (radix-)hash join. The relative performance of these two join approaches have been a topic of discussion for a long time. With the advent of modern multi-core architectures, it has been argued that sort-merge join is now a better choice than radix-hash join. This claim is justified based on the width of SIMD instructions (sort-merge outperforms radix-hash join once SIMD is sufficiently wide), and NUMA awareness (sort-merge is superior to hash join in NUMA architectures). We conduct extensive experiments on the original and optimized versions of these algorithms. The experiments show that, contrary to these claims, radix-hash join is still clearly superior, and sort-merge approaches to performance of radix only when very large amounts of data are involved. The paper also provides the fastest implementations of these algorithms, and covers many aspects of modern hardware architectures relevant not only for joins but for any parallel data processing operator.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {85–96},
numpages = {12}
}

@inproceedings{10.1145/2513534.2513547,
author = {Babar, Muhammad Ali},
title = {Perspectives and Reflections on Cloud Computing and Internet Technologies from NordiCloud 2012},
year = {2013},
isbn = {9781450323079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513534.2513547},
doi = {10.1145/2513534.2513547},
abstract = {The Nordic Symposium on Cloud Computing and Internet Technologies (NordiCloud) has been found with the key aim of promoting and supporting Cloud computing and Internet Technologies research and practice in Nordic and Baltic regions. It would provide a bridge between researchers and practitioners from Nordic/Baltic countries and Cloud computing communities in other parts of the World. The first symposium was organized as a collocated event with WICSA/ECSA 2012 in Helsinki, Finland. This report summarizes the key points from the symposium based on the presented talks and discussions. It highlights some of the areas that the participants considered worth pursuing in order to help organizations to not only exploit the opportunities that cloud computing offers but also to deal with the challenges when adopting cloud computing. This report points out some of the key challenges and potential solutions that are likely to interest researcher and practitioners in Nordic/Baltic regions and beyond.},
booktitle = {Proceedings of the Second Nordic Symposium on Cloud Computing &amp; Internet Technologies},
pages = {72–79},
numpages = {8},
keywords = {PaaS, tools as a service (TaaS), utility computing, software services, SaaS, empirical software engineering, cloud computing, IaaS, internet of things (IoT), software engineering},
location = {Oslo, Norway},
series = {NordiCloud '13}
}

@inproceedings{10.1145/2506182.2506187,
author = {Gunaratna, Kalpa and Thirunarayan, Krishnaprasad and Jain, Prateek and Sheth, Amit and Wijeratne, Sanjaya},
title = {A Statistical and Schema Independent Approach to Identify Equivalent Properties on Linked Data},
year = {2013},
isbn = {9781450319720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506182.2506187},
doi = {10.1145/2506182.2506187},
abstract = {Linked Open Data (LOD) cloud has gained significant attention in the Semantic Web community recently. Currently it consists of approximately 295 interlinked datasets with over 50 billion triples including 500 million links, and continues to expand in size. This vast source of structured information has the potential to have a significant impact on knowledge-based applications. However, a key impediment to the use of LOD cloud is limited support for data integration tasks over concepts, instances, and properties. Efforts to address this limitation over properties have focused on matching data-type properties across datasets; however, matching of object-type properties has not received similar attention. We present an approach that can automatically match object-type properties across linked datasets, primarily exploiting and bootstrapping from entity co-reference links such as owl:sameAs. Our evaluation, using sample instance sets taken from Freebase, DBpedia, LinkedMDB, and DBLP datasets covering multiple domains shows that our approach matches properties with high precision and recall (on average, F measure gain of 57% - 78%).},
booktitle = {Proceedings of the 9th International Conference on Semantic Systems},
pages = {33–40},
numpages = {8},
keywords = {relationship identification, linked open data, statistical equivalence, property alignment},
location = {Graz, Austria},
series = {I-SEMANTICS '13}
}

@inproceedings{10.1145/2494188.2494208,
author = {Dennerlein, Sebastian and Gutounig, Robert and Kraker, Peter and Kaiser, Rene and Rauter, Romana and Ausserhofer, Julian},
title = {Assessing Barcamps: Incentives for Participation in Ad-Hoc Conferences and the Role of Social Media},
year = {2013},
isbn = {9781450323000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494188.2494208},
doi = {10.1145/2494188.2494208},
abstract = {Barcamps are informal conferences whose content is not defined in advance, often referred to as ad-hoc conferences or un-conferences. Therefore, the outcomes of a barcamp are largely unknown before the event. This raises the question of the participants' motivations to attend and contribute. To answer this question, we conducted an exploratory empirical study at Barcamp Graz 2012. We applied a mixed-method approach: first we used a socio-demographic questionnaire (n=99) which allowed us to characterize the 'typical barcamper'. Second, we conducted qualitative interviews (n=10) to get a deeper understanding of the participants' motivations to attend, expectations, and the use of social media in that context. We identified three concepts, which could be deducted from the interviews: people, format and topics. We found that the motivation to attend and even a common identity is quite strongly based on these three factors. Furthermore, the results indicate that participants share a set of activities and methods by following the barcamp's inherent rules and make extensive use of social media.},
booktitle = {Proceedings of the 13th International Conference on Knowledge Management and Knowledge Technologies},
articleno = {15},
numpages = {8},
keywords = {Motivation, Communities of Practice, Barcamp, Self Organization, Knowledge Exchange, Social Media},
location = {Graz, Austria},
series = {i-Know '13}
}

@inproceedings{10.5555/2648668.2648693,
author = {Li, Chao and Wang, Rui and Goswami, Nilanjan and Li, Xian and Li, Tao and Qian, Depei},
title = {Chameleon: Adapting Throughput Server to Time-Varying Green Power Budget Using Online Learning},
year = {2013},
isbn = {9781479912353},
publisher = {IEEE Press},
abstract = {Eco-friendly energy sources (i.e. green power) attract great attention as lowering computer carbon footprint has become a necessity. Existing proposals on managing green energy powered systems show sub-optimal results since they either use rigid load power capping or heavily rely on backup power. We propose Chameleon, a novel adaptive green throughput server. Chameleon comprises of multiple flexible power management policies and leverages learning algorithm to select the optimal operating mode during runtime. The proposed design outperforms the state-of-the-art approach by 13% on performance, improves system MTBF by 42%, and still maintains up to 95% green energy utilization.},
booktitle = {Proceedings of the 2013 International Symposium on Low Power Electronics and Design},
pages = {100–105},
numpages = {6},
keywords = {throughput server, green power, learning, adaptation},
location = {Beijing, China},
series = {ISLPED '13}
}

@article{10.1145/2501626.2501636,
author = {Ghasemzadeh, Hassan and Jafari, Roozbeh},
title = {Ultra Low-Power Signal Processing in Wearable Monitoring Systems: A Tiered Screening Architecture with Optimal Bit Resolution},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/2501626.2501636},
doi = {10.1145/2501626.2501636},
abstract = {Advances in technology have led to the development of wearable sensing, computing, and communication devices that can be woven into the physical environment of our daily lives, enabling a large variety of new applications in several domains, including wellness and health care. Despite their tremendous potential to impact our lives, wearable health monitoring systems face a number of hurdles to become a reality. The enabling processors and architectures demand a large amount of energy, requiring sizable batteries. In this article, we propose a granular decision-making architecture for physical movement monitoring applications. The module can be viewed as a tiered wake-up circuitry. This decision-making module, in combination with a low-power microcontroller, allows for significant power saving through an ultra low-power processing architecture. The significant power saving is achieved by performing a preliminary ultra low-power signal processing, and hence, keeping the microcontroller off when the incoming signal is not of interest. The preliminary signal processing is performed by a set of special-purpose functional units, also called screening blocks, that implement template matching functions. We formulate and solve an optimization problem for selecting screening blocks such that the accuracy requirements of the signal processing are accommodated while the total power is minimized. Our experimental results on real data from wearable motion sensors show that the proposed algorithm achieves 63.2% energy saving while maintaining a sensitivity of 94.3% in recognizing transitional actions.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {9},
numpages = {23},
keywords = {body sensor networks, power optimization, Medical embedded systems, signal processing, wearable monitoring systems}
}

@inproceedings{10.1145/2493432.2493507,
author = {Frost, Mads and Doryab, Afsaneh and Faurholt-Jepsen, Maria and Kessing, Lars Vedel and Bardram, Jakob E.},
title = {Supporting Disease Insight through Data Analysis: Refinements of the Monarca Self-Assessment System},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493507},
doi = {10.1145/2493432.2493507},
abstract = {There is a growing interest in personal health technologies that sample behavioral data from a patient and visualize this data back to the patient for increased health awareness. However, a core challenge for patients is often to understand the connection between specific behaviors and health, i.e. to go beyond health awareness to disease insight. This paper presents MONARCA 2.0, which records subjective and objective data from patients suffering from bipolar disorder, processes this, and informs both the patient and clinicians on the importance of the different data items according to the patient's mood. The goal is to provide patients with a increased insight into the parameters influencing the nature of their disease. The paper describes the user-centered design and the technical implementation of the system, as well as findings from an initial field deployment.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {133–142},
numpages = {10},
keywords = {mental illness management, bipolar disorder, pervasive healthcare, smartphone, data analysis, personal health monitoring},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493443,
author = {Shin, Choonsung and Dey, Anind K.},
title = {Automatically Detecting Problematic Use of Smartphones},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493443},
doi = {10.1145/2493432.2493443},
abstract = {Smartphone adoption has increased significantly and, with the increase in smartphone capabilities, this means that users can access the Internet, communicate, and entertain themselves anywhere and anytime. However, there is growing evidence of problematic use of smartphones that impacts both social and heath aspects of users' lives. Currently, assessment of overuse or problematic use depends on one-time, self-reported behavioral information about phone use. Due to the known issues with self-reports in such types of assessments, we explore an automated, objective and repeatable approach for assessing problematic usage. We collect a wide range of phone usage data from smartphones, identify a number of usage features that are relevant to this assessment, and build detection models based on Adaboost with machine learning algorithms automatically detecting problematic use. We found that the number of apps used per day, the ratio of SMSs to calls, the number of event-initiated sessions, the number of apps used per event initiated session, and the length of non-event-initiated sessions are useful for detecting problematic usage. With these, a detection model can identify users with problematic usage with 89.6% accuracy (F-score of .707).},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {335–344},
numpages = {10},
keywords = {machine learning, health, detection},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493495,
author = {Gr\"{o}nvall, Erik and Verdezoto, Nervo},
title = {Beyond Self-Monitoring: Understanding Non-Functional Aspects of Home-Based Healthcare Technology},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493495},
doi = {10.1145/2493432.2493495},
abstract = {Monitoring of health parameters in non-clinical settings is one strategy to address the increasingly aging population and age-related disabilities and diseases. However, challenges exist when introducing self-monitoring activities in people's everyday life. An active lifestyle can challenge the appropriation of healthcare technologies and people with comorbidity may have diverse but co-existing monitoring needs. In this paper, we seek to understand home-based health monitoring practices to better design and integrate them into people's everyday life. We perform an analysis of socio-technical complexities in home-based healthcare technologies through three case studies of self-monitoring: 1) pre-eclampsia (i.e. pregnancy poisoning), 2) heart conditions, and 3) preventive care. Through the analysis seven themes emerged (people, resources, places, routines, knowledge, control and motivation) that can facilitate the understanding of home-based healthcare activities. We present three modes of self-monitoring use and provide a set of design recommendations for future Ubicomp designs of home-based healthcare technology.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {587–596},
numpages = {10},
keywords = {personal health management., self-monitoring, case studies, home-based healthcare technology},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493465,
author = {Ferreira, Denzil and Ferreira, Eija and Goncalves, Jorge and Kostakos, Vassilis and Dey, Anind K.},
title = {Revisiting Human-Battery Interaction with an Interactive Battery Interface},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493465},
doi = {10.1145/2493432.2493465},
abstract = {Mobile phone user interfaces typically show an icon to indicate remaining battery, but not the amount of time the device can be used for, often forcing users to make faulty estimates and predictions about battery life. Here we report on two studies that capture users' experiences with a user-centered battery interface design. In Study 1, we analyze 12 participants' use of mobile phones, demonstrating that mobile phone users do not know how or what to do to extend their mobile's battery life. We further identify the information they rely on to assess battery life. In Study 2, we use this information to design, prototype and evaluate an interactive battery interface (IBI) with another 22 participants. Our findings describe how users perceive battery life and how we used their mental models of mobile phone batteries to create IBI. Lastly, we report on the users' experiences and IBI's effect on battery lifetime, showing gains of approximately 27% over the course of a day.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {563–572},
numpages = {10},
keywords = {battery monitoring, range anxiety, mobile computing, battery explanations, context-awareness},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493492,
author = {Ladha, Cassim and Hammerla, Nils Y. and Olivier, Patrick and Pl\"{o}tz, Thomas},
title = {ClimbAX: Skill Assessment for Climbing Enthusiasts},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493492},
doi = {10.1145/2493432.2493492},
abstract = {In recent years the sport of climbing has seen consistent increase in popularity. Climbing requires a complex skill set for successful and safe exercising. While elite climbers receive intensive expert coaching to refine this skill set, this progression approach is not viable for the amateur population. We have developed ClimbAX - a climbing performance analysis system that aims for replicating expert assessments and thus represents a first step towards an automatic coaching system for climbing enthusiasts. Through an accelerometer based wearable sensing platform, climber's movements are captured. An automatic analysis procedure detects climbing sessions and moves, which form the basis for subsequent performance assessment. The assessment parameters are derived from sports science literature and include: power, control, stability, speed. ClimbAX was evaluated in a large case study with 53 climbers under competition settings. We report a strong correlation between predicted scores and official competition results, which demonstrate the effectiveness of our automatic skill assessment system.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {235–244},
numpages = {10},
keywords = {climbing, skill assessment, activity recognition, sports analysis},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493433,
author = {Epstein, Daniel A. and Borning, Alan and Fogarty, James},
title = {Fine-Grained Sharing of Sensed Physical Activity: A Value Sensitive Approach},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493433},
doi = {10.1145/2493432.2493433},
abstract = {Personal informatics applications in a variety of domains are increasingly enabled by low cost personal sensing. Although applications capture fine-grained activity for self reflection, sharing is generally limited to high level summaries. There are potential advantages to fine-grained sharing, but also potential harms. To help investigate this complex design space, we employ Value Sensitive Design to consider whether and how to share fine grained step activity. We identify key values and value tensions, and we develop scenarios to highlight these. We then design a set of data transformations that seek to maximize the benefits while minimizing the harms of detailed sharing. These include a novel approach to interactive modification of fine grained step data, allowing people to remove private data and using motif discovery to generate realistic replacement data. Finally, we conduct semi structured interviews with 12 participants examining these scenarios and transformations. We distill results into a set of design considerations for fine-grained physical activity sharing.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {489–498},
numpages = {10},
keywords = {personal informatics, social sharing, physical activity sensing, value sensitive design},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493454,
author = {Spina, Gabriele and Huang, Guannan and Vaes, Anouk and Spruit, Martijn and Amft, Oliver},
title = {COPDTrainer: A Smartphone-Based Motion Rehabilitation Training System with Real-Time Acoustic Feedback},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493454},
doi = {10.1145/2493432.2493454},
abstract = {Patient motion training requires adaptive, personalized exercise models and systems that are easy to handle. In this paper, we evaluate a training system based on a smartphone that integrates in clinical routines and serves as a tool for therapist and patient. Only the smartphone's build-in inertial sensors were used to monitor exercise execution and providing acoustic feedback on exercise performance and exercise errors. We used a sinusoidal motion model to exploit the typical repetitive structure of motion exercises. A Teach-mode was used to personalize the system by training under the guidance of a therapist and deriving exercise model parameters. Subsequently, in a Train-mode, the system provides exercise feedback. We validate our approach in a validation with healthy volunteers and in an intervention study with COPD patients. System performance, trainee performance, and feedback efficacy were analysed. We further compare the therapist and training system performances and demonstrate that our approach is viable.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {597–606},
numpages = {10},
keywords = {exercise quality., patient monitoring, rehabilitation training, wearable computing},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493496,
author = {Natarajan, Annamalai and Parate, Abhinav and Gaiser, Edward and Angarita, Gustavo and Malison, Robert and Marlin, Benjamin and Ganesan, Deepak},
title = {Detecting Cocaine Use with Wearable Electrocardiogram Sensors},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493496},
doi = {10.1145/2493432.2493496},
abstract = {Ubiquitous physiological sensing has the potential to profoundly improve our understanding of human behavior, leading to more targeted treatments for a variety of disorders. The long term goal of this work is development of novel computational tools to support the study of addiction in the context of cocaine use. The current paper takes the first step in this important direction by posing a simple, but crucial question: Can cocaine use be reliably detected using wearable electrocardiogram (ECG) sensors? The main contributions in this paper include the presentation of a novel clinical study of cocaine use, the development of a computational pipeline for inferring morphological features from noisy ECG waveforms, and the evaluation of feature sets for cocaine use detection. Our results show that 32mg/70kg doses of cocaine can be detected with the area under the receiver operating characteristic curve levels above 0.9 both within and between-subjects.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {123–132},
numpages = {10},
keywords = {medicine and science},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493508,
author = {Silveira, Fernando and Eriksson, Brian and Sheth, Anmol and Sheppard, Adam},
title = {Predicting Audience Responses to Movie Content from Electro-Dermal Activity Signals},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493508},
doi = {10.1145/2493432.2493508},
abstract = {The ability to assess fine-scale user responses has applications in advertising, content creation, recommendation, and psychology research. Unfortunately, current approaches, such as focus groups and audience surveys, are limited in size and scope. In this paper, we propose a combined biometric sensing and analysis methodology to leverage audience-scale electro-dermal activity (EDA) data for the purpose of evaluating user responses to video. We provide detailed characterization of how temporal physiological responses to video stimulus can be modeled, along with first-of-its-kind audience-scale EDA group experiments in uncontrolled real-world environments. Our study provides insights into the techniques used to analyze EDA, the effectiveness of the different temporal features, and group dynamics of audiences. Our experiments demonstrate the ability to classify movie ratings with accuracy of over 70% on specific films. Results of this study suggest the ability to assess emotional reactions of groups using minimally invasive sensing modalities in uncontrolled environments.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {707–716},
numpages = {10},
keywords = {affective computing, signal processing, gsr, user experiments, biometrics},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493502,
author = {Hoque, Mohammed (Ehsan) and Courgeon, Matthieu and Martin, Jean-Claude and Mutlu, Bilge and Picard, Rosalind W.},
title = {MACH: My Automated Conversation Coach},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493502},
doi = {10.1145/2493432.2493502},
abstract = {MACH--My Automated Conversation coacH--is a novel system that provides ubiquitous access to social skills training. The system includes a virtual agent that reads facial expressions, speech, and prosody and responds with verbal and nonverbal behaviors in real time. This paper presents an application of MACH in the context of training for job interviews. During the training, MACH asks interview questions, automatically mimics certain behavior issued by the user, and exhibit appropriate nonverbal behaviors. Following the interaction, MACH provides visual feedback on the user's performance. The development of this application draws on data from 28 interview sessions, involving employment-seeking students and career counselors. The effectiveness of MACH was assessed through a weeklong trial with 90 MIT undergraduates. Students who interacted with MACH were rated by human experts to have improved in overall interview performance, while the ratings of students in control groups did not improve. Post-experiment interviews indicate that participants found the interview experience informative about their behaviors and expressed interest in using MACH in the future.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {697–706},
numpages = {10},
keywords = {social training, embodied conversational agents, automated multimodal affect sensing and behavior recognition, automated feedback},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493451,
author = {Clear, Adrian K. and Morley, Janine and Hazas, Mike and Friday, Adrian and Bates, Oliver},
title = {Understanding Adaptive Thermal Comfort: New Directions for UbiComp},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493451},
doi = {10.1145/2493432.2493451},
abstract = {In many parts of the world, mechanical heating and cooling is used to regulate indoor climates, with the aim of maintaining a uniform temperature. Achieving this is energy-intensive, since large indoor spaces must be constantly heated or cooled, and the difference to the outdoor temperature is large. This paper starts from the premise that comfort is not delivered to us by the indoor environment, but is instead something that is pursued as a normal part of daily life, through a variety of means. Based on a detailed study of four university students over several months, we explore how Ubicomp technologies can help create a more sustainable reality where people are more active in pursuing and maintaining their thermal comfort, and environments are less tightly controlled and less energy-intensive, and we outline areas for future research in this domain.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {113–122},
numpages = {10},
keywords = {sustainability, heating, cooling, thermal comfort},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493432.2493458,
author = {Han, Seungyeop and Philipose, Matthai and Ju, Yun-Cheng},
title = {NLify: Lightweight Spoken Natural Language Interfaces via Exhaustive Paraphrasing},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493458},
doi = {10.1145/2493432.2493458},
abstract = {This paper presents the design and implementation of a programming system that enables third-party developers to add spoken natural language (SNL) interfaces to standalone mobile applications. The central challenge is to create statistical recognition models that are accurate and resource-efficient in the face of the variety of natural language, while requiring little specialized knowledge from developers. We show that given a few examples from the developer, it is possible to elicit comprehensive sets of paraphrases of the examples using internet crowds. The exhaustive nature of these paraphrases allows us to use relatively simple, automatically derived statistical models for speech and language understanding that perform well without per-application tuning. We have realized our design fully as an extension to the Visual Studio IDE. Based on a new benchmark dataset with 3500 spoken instances of 27 commands from 20 subjects and a small developer study, we establish the promise of our approach and the impact of various design choices.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {429–438},
numpages = {10},
keywords = {crowdsourcing, paraphrasing, spoken natural language interface, third-party mobile applications},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2493988.2494334,
author = {Jackson, Melody Moore and Zeagler, Clint and Valentin, Giancarlo and Martin, Alex and Martin, Vincent and Delawalla, Adil and Blount, Wendy and Eiring, Sarah and Hollis, Ryan and Kshirsagar, Yash and Starner, Thad},
title = {FIDO - Facilitating Interactions for Dogs with Occupations: Wearable Dog-Activated Interfaces},
year = {2013},
isbn = {9781450321273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493988.2494334},
doi = {10.1145/2493988.2494334},
abstract = {Working dogs have improved the lives of thousands of people. However, communication between human and canine partners is currently limited. The main goal of the FIDO project is to research fundamental aspects of wearable technologies to support communication between working dogs and their handlers. In this pilot study, the FIDO team investigated on-body interfaces for assistance dogs in the form of wearable technology integrated into assistance dog vests. We created four different sensors that dogs could activate (based on biting, tugging, and nose gestures) and tested them on-body with three assistance-trained dogs. We were able to demonstrate that it is possible to create wearable sensors that dogs can reliably activate on command.},
booktitle = {Proceedings of the 2013 International Symposium on Wearable Computers},
pages = {81–88},
numpages = {8},
keywords = {wearable technology, animal-computer interaction},
location = {Zurich, Switzerland},
series = {ISWC '13}
}

@inproceedings{10.5555/2578048.2578060,
author = {Krehl, Claudia and Sharples, Sarah and Flintham, Martin},
title = {Less is More: Classifying Mobile Interactions to Support Context Sensing in Journeys},
year = {2013},
publisher = {BCS Learning &amp; Development Ltd.},
address = {Swindon, GBR},
abstract = {This paper develops a classification of mobile interactions based on contextual information relevant to the mobile device user in journeys. Context-aware systems can be used to reduce the stress involved, support users in their activities and increase utility of travel time. But context is often portrayed as real, stable and structured, which can limit the value of applications as they lack dynamics and relevancy. This paper aims to classify mobile interactions in journeys by adopting an alternative view of context. It is argued that sensing less contextual information can be more valuable providing the most relevant information to the user can be identified. Context is explored using qualitative approaches that investigate user interactions during end-to-end journeys. The resulting classification serves as a basis for understanding mobile interactions and it assists designers and HCI practitioners to develop improved context-aware application.},
booktitle = {Proceedings of the 27th International BCS Human Computer Interaction Conference},
articleno = {8},
numpages = {10},
keywords = {context, task, social setting, journeys, mobile interactions},
location = {London, UK},
series = {BCS-HCI '13}
}

@inproceedings{10.5555/2578048.2578078,
author = {Brown, Nela and Stockman, Tony},
title = {Examining the Use of Thematic Analysis as a Tool for Informing Design of New Family Communication Technologies},
year = {2013},
publisher = {BCS Learning &amp; Development Ltd.},
address = {Swindon, GBR},
abstract = {Frequently family members are geographically separated for large parts of the day. This separation, allied to a busy schedule, can make it difficult to share daily experiences and maintain the feeling of connectedness. This paper describes an exploratory study to investigate family dynamics and the use of technology in families with primary school children. We interviewed five families about their daily communication and use of technology. We examined the use of thematic analysis, a method for qualitative data analysis used in social science, as a tool for systematically identifying and describing features of qualitative data and informing the design of new family technologies. The results of the investigation showed that the first 3 phases of the 6-phase thematic analysis approach were the most fruitful in yielding information about the families' use of technology and information at a level that could be of value for designing new communications technology. The use of the full 6 phases of the approach however, is more appropriate where it is required to produce a summary of the data in a form of a high level thematic map accompanied by the analytic narrative.},
booktitle = {Proceedings of the 27th International BCS Human Computer Interaction Conference},
articleno = {21},
numpages = {6},
keywords = {thematic analysis, communication, family dynamics, semi-structured interviews, technology},
location = {London, UK},
series = {BCS-HCI '13}
}

@inproceedings{10.1145/2535813.2535824,
author = {Crane, Stephen and Larsen, Per and Brunthaler, Stefan and Franz, Michael},
title = {Booby Trapping Software},
year = {2013},
isbn = {9781450325820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2535813.2535824},
doi = {10.1145/2535813.2535824},
abstract = {Cyber warfare is asymmetric in the current paradigm, with attackers having the high ground over defenders. This asymmetry stems from the situation that attackers have the initiative, while defenders concentrate on passive fortifications. Defenders are constantly patching the newest hole in their defenses and creating taller and thicker walls, without placing guards on those walls to watch for the enemy and react to attacks. Current passive cyber security defenses such as intrusion detection, anti-virus, and hardened software are not sufficient to repel attackers. In fact, in conventional warfare this passivity would be entirely nonsensical, given the available active strategies, such as counterattacks and deception.Based on this observation, we have identified the technique of booby trapping software. This extends the arsenal of weaponry available to defenders with an active technique for directly reacting to attacks. Ultimately, we believe this approach will restore some of the much sought after equilibrium between attackers and defenders in the digital domain.},
booktitle = {Proceedings of the 2013 New Security Paradigms Workshop},
pages = {95–106},
numpages = {12},
keywords = {intrusion detection, compilers, active defense, booby traps},
location = {Banff, Alberta, Canada},
series = {NSPW '13}
}

@inproceedings{10.5555/2578048.2578057,
author = {Dezuli, Niloofar and Huber, Jochen and Churchill, Elizabeth F. and M\"{u}hlh\"{a}user, Max},
title = {CoStream: Co-Construction of Shared Experiences through Mobile Live Video Sharing},
year = {2013},
publisher = {BCS Learning &amp; Development Ltd.},
address = {Swindon, GBR},
abstract = {Mobile media sharing is an increasingly popular form of social media interaction. Research has shown that asynchronous sharing fosters and maintains social connections and serves as a memory aid. More recently, researchers have investigated the potential for mobile media sharing as a mechanism for providing additional event-related information to spectators in a stadium. In this paper, we describe CoStream, a novel system for mobile live sharing of user-generated video in-situ during events. Developed iteratively with users, CoStream goes beyond prior work by providing a strong real-time coupling to the event, leveraging users' social connections to provide multiple perspectives on the ongoing action. Field trials demonstrate that real time sharing of different perspectives on the same event has the potential to provide fundamentally new experiences of same-place events, such as concerts or stadium sports. We discuss how CoStream enriches social interactions, increases context, social and spatial awareness, and thus encourages active spectatorship. We further contribute key requirements for the design of future interfaces supporting the co-construction of shared experiences during events, in-situ.},
booktitle = {Proceedings of the 27th International BCS Human Computer Interaction Conference},
articleno = {6},
numpages = {10},
keywords = {field study, iterative design, event, multimedia sharing, mobile live video sharing},
location = {London, UK},
series = {BCS-HCI '13}
}

@inproceedings{10.1145/2499149.2499171,
author = {di Mascio, Tania and Tarantino, Laura and Vittorini, Pierpaolo and Caputo, Mattia},
title = {Design Choices: Affected by User Feedback? Affected by System Performances? Lessons Learned from the TERENCE Project},
year = {2013},
isbn = {9781450320610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499149.2499171},
doi = {10.1145/2499149.2499171},
abstract = {Iterative design is nowadays indicated as the approach that most likely produces a successful system, and the application of usability evaluation methods is identified as the primary key that allow designers to reveal and fix problems early or, to better say, at the right moment during the design of the system. These approaches often overlooks issues that get revealed only when system "goes live", namely system performances and a different attitude of the users towards the system. With respect to these issues, in this paper we discuss the lessons learned from the TERENCE project -- a technology enhanced learning project for improving the text comprehension in children 7-11 years old -- and in particular from a large-scale evaluation. Our experience suggests modifications to the classical UCD lifecycle: on the one hand post implementation activities should be foreseen since the beginning (and assigned reasonable time and appropriate budget), and on the other hand system performance evaluation should be anticipated and integrated in the lifecycle, to be able to predict system behavior and variables that affect it, and consequently produce "performance informed" design iterations (with performances considered at interaction, architecture and coding levels). We also sketch a possible prediction approach.},
booktitle = {Proceedings of the Biannual Conference of the Italian Chapter of SIGCHI},
articleno = {24},
numpages = {9},
keywords = {UCD, system performance, usability evaluation},
location = {Trento, Italy},
series = {CHItaly '13}
}

@inproceedings{10.1145/2499149.2499176,
author = {Talamo, Alessandra and Ventura, Stefano and Giorgi, Sabina and Ceriani, Miguel and Bottoni, Paolo and Mellini, Barbara},
title = {"<i>Do the Gestures You Think of</i>": Creating Affordances in Codesign},
year = {2013},
isbn = {9781450320610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499149.2499176},
doi = {10.1145/2499149.2499176},
abstract = {The paper discusses the psychological implications of codesign techniques. From conceiving innovation in design as the result of the meeting of users' and designers' representations around the future concept, the authors argue that this meeting is only possible when interobjectivity is developed among actors through the use of concrete design tools. Excerpts from a codesign project devoted to the development of sharing services for older people are shown to describe how visualization techniques and tools make it possible to create mutual understanding among participants and agreement on the creation of relevant affordances.},
booktitle = {Proceedings of the Biannual Conference of the Italian Chapter of SIGCHI},
articleno = {22},
numpages = {9},
keywords = {codesign, innovation, interobjectivity, affordances},
location = {Trento, Italy},
series = {CHItaly '13}
}

@inproceedings{10.1145/2544114.2544129,
author = {Garner, Tom},
title = {Identifying Habitual Statistical Features of EEG in Response to Fear-Related Stimuli in an Audio-Only Computer Video Game},
year = {2013},
isbn = {9781450326599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2544114.2544129},
doi = {10.1145/2544114.2544129},
abstract = {A better understanding of observable and quantifiable psychophysiological outputs such as electroencephalography (EEG) during computer video gameplay has significant potential to support the development of an automated, emotionally intelligent system. Integrated into a game engine, such a system could facilitate an effective biofeedback loop, accurately interpreting player emotions and adjusting gameplay parameters to respond to players' emotional states in a way that moves towards exciting ventures in affective interactivity. This paper presents a crucial step to reaching this objective by way of examining the statistical features of EEG that may relate to user experience during audio-centric gameplay. An audio-only test game ensures that game sound is the exclusive stimulus modality with gameplay contextualisation and qualitative data collection enabling the study to focus specifically upon fear. Though requiring of an unambiguous horror-game context, the results documented within this paper identify several statistical features of EEG data that could differentiate fear from calm.},
booktitle = {Proceedings of the 8th Audio Mostly Conference},
articleno = {14},
numpages = {6},
keywords = {affect, electroencephalography, biometrics, fear, emotion},
location = {Pite\r{a}, Sweden},
series = {AM '13}
}

@inproceedings{10.5555/2555754.2555773,
author = {Hoffmann, Henry and Maggio, Martina and Santambrogio, Marco D. and Leva, Alberto and Agarwal, Anant},
title = {A Generalized Software Framework for Accurate and Efficient Management of Performance Goals},
year = {2013},
isbn = {9781479914432},
publisher = {IEEE Press},
abstract = {A number of techniques have been proposed to provide run-time performance guarantees while minimizing power consumption. One drawback of existing approaches is that they work only on a fixed set of components (or actuators) that must be specified at design time. If new components become available, these management systems must be redesigned and reimplemented. In this paper, we propose PTRADE, a novel performance management framework that is general with respect to the components it manages. PTRADE can be deployed to work on a new system with different components without redesign and reimplementation. PTRADE's generality is demonstrated through the management of performance goals for a variety of benchmarks on two different Linux/x86 systems and a simulated 128-core system, each with different components governing power and performance tradeoffs. Our experimental results show that PTRADE provides generality while meeting performance goals with low error and close to optimal power consumption.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Embedded Software},
articleno = {19},
numpages = {10},
keywords = {adaptive systems, power-aware computing, self-aware computing},
location = {Montreal, Quebec, Canada},
series = {EMSOFT '13}
}

@inproceedings{10.5555/2555729.2555750,
author = {Chakraborty, Prasenjit and Panda, Preeti Ranjan},
title = {SPM-Sieve: A Framework for Assisting Data Partitioning in Scratch Pad Memory Based Systems},
year = {2013},
isbn = {9781479914005},
publisher = {IEEE Press},
abstract = {Modern system architectures sometimes include scratch pad memories (SPM) in their memory hierarchy to take advantage of their simpler design, in an attempt to meet the system area, performance, and power budget. These systems employing SPM can be broadly categorized as: (a) cacheless systems with only SPM, (b) hybrid systems with both cache and SPM, and (c) reconfigurable systems with the provision to reconfigure local memory as either cache, SPM, or a combination of the two. However SPM based systems have needed larger efforts spent on their programming, mainly due to allocating data and orchestrating data transfers explicitly by software. Tight product development cycles require faster development and porting of diverse applications to multiple SPM based architectures. In this paper we present SPM-Sieve, a profile-based tool and framework targeted for SPM based architectures that generates partitioning decisions of the first level memory in the system hierarchy, and suggests object mapping amongst the memory partitions without resorting to detailed simulation of all configurations. This is done by natively executing an application and using minimal target architecture specification, which not only provides early information influencing data organization in the application, but also provides a foundation for other more sophisticated algorithms to produce optimized allocations. We demonstrate the utility and generality of SPM-Sieve by evaluating it on a large number of SPEC2000 benchmarks targeted for a 128KB first level memory. We evaluate its effectiveness by performing simulation studies comparing the partition suggested by the tool against varying partition sizes, and observe that its suggestions are very competitive for SPM based architectures with and without caches.},
booktitle = {Proceedings of the 2013 International Conference on Compilers, Architectures and Synthesis for Embedded Systems},
articleno = {21},
numpages = {10},
keywords = {scratch pad memory, memory allocation, software cache},
location = {Montreal, Quebec, Canada},
series = {CASES '13}
}

@inproceedings{10.4108/icst.bodynets.2013.253676,
author = {Lee, Eun Kyung and Pandey, Parul and Pompili, Dario},
title = {Real-Time Tracking of Stress Propagation Using Distributed Granger Causality},
year = {2013},
isbn = {9781936968893},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.bodynets.2013.253676},
doi = {10.4108/icst.bodynets.2013.253676},
abstract = {Stress is one of the key factors that impacts the quality of our daily life. It is known that stress can propagate from one individual to others working in close proximity or towards a common goal, e.g., in a military operation or workforce, thus affecting productivity, efficiency, and the ability to make rational decisions. Real-time assessment of the stress of individuals alone is, however, not sufficient as understanding its source and direction in which it propagates in a group is equally if not more important. In this paper, the direction of propagation and magnitude of influence of stress in a group of individuals are studied by applying real-time, in-situ analysis of Granger Causality. G-causality has established itself as one of the promising non-invasive approaches in operational neuroscience to reveal the direction of influence between brain areas by analyzing temporal precedence. Extending G-causality analysis on real-time group data faces, however, communication and computation challenges, to address which a distributed mobile computational framework is employed and workflows defining how data and tasks are divided among the entities of the framework are designed.},
booktitle = {Proceedings of the 8th International Conference on Body Area Networks},
pages = {370–376},
numpages = {7},
location = {Boston, Massachusetts},
series = {BodyNets '13}
}

@inproceedings{10.4108/icst.bodynets.2013.253685,
author = {Muaremi, Amir and Seiter, Julia and Gravenhorst, Franz and Tr\"{o}ster, Gerhard and Bexheti, Agon and Arnrich, Bert},
title = {Monitor Pilgrims: Prayer Activity Recognition Using Wearable Sensors},
year = {2013},
isbn = {9781936968893},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.bodynets.2013.253685},
doi = {10.4108/icst.bodynets.2013.253685},
abstract = {Each year, millions of Muslims visit the holy sites in Makkah and Madinah. The so-called Hajj pilgrimage is one of the biggest annual events in the world. The individual impact on participating pilgrims is significant, with many of the pilgrims reporting it as a life-changing event. However, quite a little is done to objectively monitor the pilgrims and to understand, from the individual point of view, the characteristics of each of the pilgrimage stages. In this work, through observing differences in bio-physiological responses of the subjects during prayers, we are able to differentiate, in the first case, between congregational prayers and individual prayers, and, in the second case, between silent prayers and loud prayers. We collected data from 10 participants in an 8-day pilgrimage using two wearable sensors, namely chest belts and wrist-worn devices. We derive features from ECG, respiration and GSR data, and use the ANOVA model to analyze feature groups. Based on that, we build classifiers to differentiate between types of prayers. The SVM classifier shows the best performance with a mean accuracy rate of 78 % for the first case, and 84 % for the second case.},
booktitle = {Proceedings of the 8th International Conference on Body Area Networks},
pages = {161–164},
numpages = {4},
keywords = {pilgrims, HRV, wearable sensors, GSR, respiratory, prayer},
location = {Boston, Massachusetts},
series = {BodyNets '13}
}

@inproceedings{10.1145/2513002.2513572,
author = {Keogh, Brendan},
title = {When Game over Means Game over: Using Permanent Death to Craft Living Stories in <i>Minecraft</i>},
year = {2013},
isbn = {9781450322546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513002.2513572},
doi = {10.1145/2513002.2513572},
abstract = {The play style of 'perma-death' (permanent death) alters the videogame player's experience by adding harsh consequences to the usually trivial event of character death. While perma-death has a long history as a fixed constraint in certain games and genres, there are numerous cases of players self-imposing the rules of perma-death play in a broader variety of games through voluntary acts such as opting to delete a save file if their character dies. Such self-imposed cases of perma-death radically alter how the player engages with the game. In a collision of fixed affordances and player-imposed rules, the tone of the game's conventional gameplay shifts from one of experimentation to one of vulnerability. To explore how perma-death functions and how it alters the player's experience of a game, this paper looks at a perma-death experiment conducted by the author in the game Minecraft. As the project progressed, its online diary gathered a committed readership. The fear of permanent death did not drastically alter the base game of Minecraft but, as will be explored, imbued the performance of playing Minecraft with a narrative weight.},
booktitle = {Proceedings of The 9th Australasian Conference on Interactive Entertainment: Matters of Life and Death},
articleno = {20},
numpages = {6},
keywords = {performance, rules, gaming, expansive gameplay, Minecraft, perma-death, game studies, emergent gameplay, videogames},
location = {Melbourne, Australia},
series = {IE '13}
}

